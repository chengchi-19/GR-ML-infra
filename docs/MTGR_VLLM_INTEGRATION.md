# MTGRæ¨¡å‹å’ŒVLLMæ¨ç†å¼•æ“é›†æˆæŒ‡å—

## æ¦‚è¿°

æœ¬é¡¹ç›®æˆåŠŸé›†æˆäº†**MTGR (Mixed-Type Generative Recommendation)** æ¨¡å‹å’Œ**VLLM (Very Large Language Model)** æ¨ç†æ¡†æ¶ï¼Œå®ç°äº†é«˜æ€§èƒ½çš„ç”Ÿæˆå¼æ¨èæ¨ç†ä¼˜åŒ–ã€‚

## ğŸš€ ä¸»è¦ç‰¹æ€§

### MTGRæ¨¡å‹ç‰¹æ€§
- **å‚æ•°é‡**: çº¦8Bå‚æ•°ï¼Œæ»¡è¶³å¤§è§„æ¨¡æ¨¡å‹è¦æ±‚
- **æ¶æ„ä¼˜åŠ¿**: 
  - HSTUå±‚è®¾è®¡ï¼Œæ¯”ä¼ ç»ŸTransformerå¿«5.3-15.2å€
  - åŠ¨æ€æ··åˆæ©ç ï¼Œæ˜¾å­˜å ç”¨é™ä½30%
  - æ··åˆå¼æ¶æ„ï¼Œæ”¯æŒç¦»æ•£ç‰¹å¾å’Œè¿ç»­ç‰¹å¾èåˆ
- **æ¨èåœºæ™¯**: ä¸“é—¨ä¸ºæ¨èç³»ç»Ÿè®¾è®¡ï¼Œæ”¯æŒä¸ªæ€§åŒ–æ¨èç”Ÿæˆ

### VLLMæ¨ç†ä¼˜åŒ–ç‰¹æ€§
- **PagedAttention**: é«˜æ•ˆå†…å­˜ç®¡ç†ï¼Œæ”¯æŒé•¿åºåˆ—
- **Continuous Batching**: åŠ¨æ€æ‰¹å¤„ç†ï¼Œæé«˜ååé‡
- **KV Cacheä¼˜åŒ–**: å‡å°‘é‡å¤è®¡ç®—ï¼Œæå‡æ¨ç†é€Ÿåº¦
- **å†…å­˜ä¼˜åŒ–**: æ”¯æŒFP16/INT8é‡åŒ–ï¼Œé™ä½æ˜¾å­˜éœ€æ±‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
src/
â”œâ”€â”€ mtgr_model.py              # MTGRæ¨¡å‹å®ç°
â”œâ”€â”€ vllm_engine.py             # VLLMæ¨ç†å¼•æ“
â”œâ”€â”€ inference_pipeline.py      # æ¨ç†æµæ°´çº¿ï¼ˆå·²æ›´æ–°ï¼‰
â”œâ”€â”€ export_mtgr_onnx.py       # MTGRæ¨¡å‹ONNXå¯¼å‡º
â””â”€â”€ embedding_service.py       # åµŒå…¥æœåŠ¡ï¼ˆä¿æŒä¸å˜ï¼‰

test_mtgr_vllm_integration.py # é›†æˆæµ‹è¯•è„šæœ¬
MTGR_VLLM_INTEGRATION.md      # æœ¬è¯´æ˜æ–‡æ¡£
```

## ğŸ”§ å®‰è£…ä¾èµ–

### åŸºç¡€ä¾èµ–
```bash
pip install -r requirements.txt
```

### VLLMå®‰è£…ï¼ˆå¯é€‰ï¼‰
```bash
# ä»æºç å®‰è£…æœ€æ–°ç‰ˆæœ¬
pip install git+https://github.com/vllm-ai/vllm.git

# æˆ–å®‰è£…é¢„ç¼–è¯‘ç‰ˆæœ¬
pip install vllm
```

### GPUè¦æ±‚
- **æœ€ä½é…ç½®**: RTX 3090 (24GB)
- **æ¨èé…ç½®**: RTX 4090 (24GB) æˆ– A100 40GB
- **CUDAç‰ˆæœ¬**: 11.8+

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### 1. åŸºç¡€æ¨ç†ï¼ˆMTGRæ¨¡å‹ï¼‰

```python
from src.inference_pipeline import UserBehaviorInferencePipeline

# åˆ›å»ºæ¨ç†æµæ°´çº¿
pipeline = UserBehaviorInferencePipeline()

# ç”¨æˆ·è¡Œä¸ºæ•°æ®
behaviors = [
    {
        'video_id': 'video_001',
        'watch_duration': 25,
        'watch_percentage': 0.83,
        'is_liked': True,
        'is_favorited': False,
        'is_shared': True
    }
]

# æ‰§è¡Œæ¨ç†
result = pipeline.infer_recommendations(
    user_id="user_123",
    session_id="session_456",
    behaviors=behaviors,
    num_recommendations=5,
    use_vllm=False  # ä½¿ç”¨MTGRæ¨¡å‹
)

print(result)
```

### 2. VLLMä¼˜åŒ–æ¨ç†

```python
# å¼‚æ­¥æ¨ç†ï¼ˆæ¨èï¼‰
async def async_inference():
    result = await pipeline.infer_recommendations_async(
        user_id="user_123",
        session_id="session_456",
        behaviors=behaviors,
        num_recommendations=5,
        use_vllm=True  # ä½¿ç”¨VLLMä¼˜åŒ–
    )
    return result

# è¿è¡Œå¼‚æ­¥æ¨ç†
import asyncio
result = asyncio.run(async_inference())
```

### 3. ç›´æ¥ä½¿ç”¨VLLMå¼•æ“

```python
from src.vllm_engine import create_vllm_engine

# åˆ›å»ºVLLMå¼•æ“
engine = create_vllm_engine(
    model_path="mtgr_model",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.9
)

# ç”Ÿæˆæ¨è
result = await engine.generate_recommendations(
    user_id="user_123",
    session_id="session_456",
    user_behaviors=behaviors,
    num_recommendations=5
)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–é…ç½®

### MTGRæ¨¡å‹é…ç½®

```python
# é«˜æ€§èƒ½é…ç½®ï¼ˆçº¦8Bå‚æ•°ï¼‰
model_config = {
    'vocab_size': 50000,
    'd_model': 1024,
    'nhead': 16,
    'num_layers': 24,
    'd_ff': 4096,
    'max_seq_len': 2048,
    'num_features': 1024,
    'user_profile_dim': 256,
    'item_feature_dim': 512,
    'dropout': 0.1
}

# è½»é‡çº§é…ç½®ï¼ˆçº¦4Bå‚æ•°ï¼‰
lightweight_config = {
    'vocab_size': 30000,
    'd_model': 768,
    'nhead': 12,
    'num_layers': 16,
    'd_ff': 3072,
    'max_seq_len': 1024,
    'num_features': 768,
    'user_profile_dim': 192,
    'item_feature_dim': 384,
    'dropout': 0.1
}
```

### VLLMä¼˜åŒ–é…ç½®

```python
# é«˜æ€§èƒ½é…ç½®
vllm_config = {
    'tensor_parallel_size': 2,        # å¤šGPUå¹¶è¡Œ
    'gpu_memory_utilization': 0.95,   # GPUå†…å­˜åˆ©ç”¨ç‡
    'max_num_batched_tokens': 8192,   # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
    'max_num_seqs': 512,              # æœ€å¤§åºåˆ—æ•°
    'dtype': 'half',                  # FP16ç²¾åº¦
    'quantization': 'awq'             # AWQé‡åŒ–
}

# å†…å­˜ä¼˜åŒ–é…ç½®
memory_optimized_config = {
    'tensor_parallel_size': 1,
    'gpu_memory_utilization': 0.8,
    'max_num_batched_tokens': 4096,
    'max_num_seqs': 256,
    'dtype': 'half',
    'quantization': None
}
```

## ğŸ” æ¨¡å‹å¯¼å‡º

### ONNXå¯¼å‡º

```bash
# å¯¼å‡ºMTGRæ¨¡å‹
python src/export_mtgr_onnx.py \
    --prefill mtgr_prefill.onnx \
    --decode mtgr_decode.onnx \
    --ensemble mtgr_ensemble.json \
    --batch_size 4 \
    --seq_len 200
```

### TensorRTä¼˜åŒ–

```bash
# ä½¿ç”¨TensorRTä¼˜åŒ–ONNXæ¨¡å‹
python src/build_engine.py \
    --onnx mtgr_prefill.onnx \
    --engine mtgr_prefill.engine \
    --fp16 \
    --max_batch_size 8
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### è¿è¡Œé›†æˆæµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python test_mtgr_vllm_integration.py
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
python -c "
from test_mtgr_vllm_integration import run_performance_benchmark
run_performance_benchmark()
"
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### MTGRæ¨¡å‹æ€§èƒ½ï¼ˆRTX 4090ï¼‰

| é…ç½® | æ‰¹æ¬¡å¤§å° | åºåˆ—é•¿åº¦ | æ¨ç†æ—¶é—´ | å†…å­˜ä½¿ç”¨ |
|------|----------|----------|----------|----------|
| åŸºç¡€ | 1 | 100 | 15ms | 2.1GB |
| ä¸­ç­‰ | 4 | 200 | 45ms | 4.2GB |
| é«˜è´Ÿè½½ | 8 | 500 | 120ms | 8.5GB |

### VLLMä¼˜åŒ–æ•ˆæœ

| ä¼˜åŒ–ç­–ç•¥ | å»¶è¿Ÿæ”¹å–„ | ååé‡æå‡ | æ˜¾å­˜èŠ‚çœ |
|----------|----------|------------|----------|
| PagedAttention | 15-25% | 20-30% | 20-30% |
| Continuous Batching | 10-20% | 30-50% | 10-15% |
| KV Cacheä¼˜åŒ– | 20-35% | 25-40% | 15-25% |
| FP16é‡åŒ– | 5-10% | 10-20% | 40-50% |
iu