#!/usr/bin/env python3
"""
MTGRæ¨¡å‹ONNXå¯¼å‡ºè„šæœ¬
æ”¯æŒprefillå’Œdecodeä¸¤é˜¶æ®µå¯¼å‡ºï¼Œå…¼å®¹TensorRTä¼˜åŒ–
"""

import argparse
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Tuple
import logging

from src.mtgr_model import create_mtgr_model, MTGRWrapper

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MTGRONNXExporter:
    """MTGRæ¨¡å‹ONNXå¯¼å‡ºå™¨"""
    
    def __init__(self, model_config: Dict[str, Any]):
        self.model_config = model_config
        self.model = create_mtgr_model(model_config)
        self.model.eval()
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"MTGRæ¨¡å‹åŠ è½½å®Œæˆï¼Œæ€»å‚æ•°é‡: {total_params:,} (çº¦{total_params/1e9:.1f}B)")
    
    def create_dummy_data(self, batch_size: int = 1, seq_len: int = 100) -> Tuple[torch.Tensor, ...]:
        """åˆ›å»ºç”¨äºONNXå¯¼å‡ºçš„è™šæ‹Ÿæ•°æ®"""
        vocab_size = self.model_config.get('vocab_size', 50000)
        num_features = self.model_config.get('num_features', 1024)
        user_profile_dim = self.model_config.get('user_profile_dim', 256)
        item_feature_dim = self.model_config.get('item_feature_dim', 512)
        
        # è¾“å…¥æ•°æ®
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), dtype=torch.long)
        dense_features = torch.randn(batch_size, num_features, dtype=torch.float32)
        user_profile = torch.randn(batch_size, user_profile_dim, dtype=torch.float32)
        item_features = torch.randn(batch_size, item_feature_dim, dtype=torch.float32)
        attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
        
        return input_ids, dense_features, user_profile, item_features, attention_mask
    
    def export_prefill_model(self, output_path: str, batch_size: int = 1, seq_len: int = 100):
        """å¯¼å‡ºPrefillé˜¶æ®µæ¨¡å‹"""
        logger.info(f"å¯¼å‡ºPrefillæ¨¡å‹åˆ°: {output_path}")
        
        # åˆ›å»ºè™šæ‹Ÿæ•°æ®
        dummy_data = self.create_dummy_data(batch_size, seq_len)
        
        # å¯¼å‡ºONNX
        torch.onnx.export(
            self.model,
            dummy_data,
            output_path,
            input_names=[
                'input_ids', 'dense_features', 'user_profile', 
                'item_features', 'attention_mask'
            ],
            output_names=[
                'logits', 'recommendation_score', 'engagement_score',
                'retention_score', 'monetization_score', 'hidden_states'
            ],
            opset_version=14,
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'seq_len'},
                'dense_features': {0: 'batch_size'},
                'user_profile': {0: 'batch_size'},
                'item_features': {0: 'batch_size'},
                'attention_mask': {0: 'batch_size', 1: 'seq_len'},
                'logits': {0: 'batch_size', 1: 'seq_len'},
                'recommendation_score': {0: 'batch_size'},
                'engagement_score': {0: 'batch_size'},
                'retention_score': {0: 'batch_size'},
                'monetization_score': {0: 'batch_size'},
                'hidden_states': {0: 'batch_size', 1: 'seq_len'}
            },
            do_constant_folding=True,
            export_params=True,
            verbose=False
        )
        
        logger.info(f"Prefillæ¨¡å‹å¯¼å‡ºå®Œæˆ: {output_path}")
    
    def export_decode_model(self, output_path: str, batch_size: int = 1, past_len: int = 500):
        """å¯¼å‡ºDecodeé˜¶æ®µæ¨¡å‹"""
        logger.info(f"å¯¼å‡ºDecodeæ¨¡å‹åˆ°: {output_path}")
        
        # åˆ›å»ºè™šæ‹Ÿæ•°æ®
        vocab_size = self.model_config.get('vocab_size', 50000)
        num_features = self.model_config.get('num_features', 1024)
        user_profile_dim = self.model_config.get('user_profile_dim', 256)
        item_feature_dim = self.model_config.get('item_feature_dim', 512)
        d_model = self.model_config.get('d_model', 1024)
        
        # Decodeé˜¶æ®µçš„è¾“å…¥
        token_id = torch.randint(0, vocab_size, (batch_size, 1), dtype=torch.long)
        past_key_value_states = torch.zeros(batch_size, past_len, d_model, dtype=torch.float32)
        dense_features = torch.randn(batch_size, num_features, dtype=torch.float32)
        user_profile = torch.randn(batch_size, user_profile_dim, dtype=torch.float32)
        item_features = torch.randn(batch_size, item_feature_dim, dtype=torch.float32)
        
        # åˆ›å»ºDecodeåŒ…è£…å™¨
        decode_wrapper = DecodeWrapper(self.model)
        
        # å¯¼å‡ºONNX
        torch.onnx.export(
            decode_wrapper,
            (token_id, past_key_value_states, dense_features, user_profile, item_features),
            output_path,
            input_names=[
                'token_id', 'past_key_value_states', 'dense_features',
                'user_profile', 'item_features'
            ],
            output_names=[
                'logits', 'recommendation_score', 'engagement_score',
                'retention_score', 'monetization_score', 'present_key_value_states'
            ],
            opset_version=14,
            dynamic_axes={
                'token_id': {0: 'batch_size'},
                'past_key_value_states': {0: 'batch_size', 1: 'past_len'},
                'dense_features': {0: 'batch_size'},
                'user_profile': {0: 'batch_size'},
                'item_features': {0: 'batch_size'},
                'logits': {0: 'batch_size'},
                'recommendation_score': {0: 'batch_size'},
                'engagement_score': {0: 'batch_size'},
                'retention_score': {0: 'batch_size'},
                'monetization_score': {0: 'batch_size'},
                'present_key_value_states': {0: 'batch_size', 1: 'present_len'}
            },
            do_constant_folding=True,
            export_params=True,
            verbose=False
        )
        
        logger.info(f"Decodeæ¨¡å‹å¯¼å‡ºå®Œæˆ: {output_path}")
    
    def export_ensemble_model(self, prefill_path: str, decode_path: str, output_path: str):
        """å¯¼å‡ºé›†æˆæ¨¡å‹é…ç½®"""
        logger.info(f"åˆ›å»ºé›†æˆæ¨¡å‹é…ç½®: {output_path}")
        
        # è¿™é‡Œå¯ä»¥åˆ›å»ºTritonçš„é›†æˆæ¨¡å‹é…ç½®
        # ç”±äºONNXæœ¬èº«ä¸æ”¯æŒé›†æˆï¼Œè¿™é‡Œåªæ˜¯åˆ›å»ºé…ç½®æ–‡ä»¶
        config = {
            "prefill_model": prefill_path,
            "decode_model": decode_path,
            "ensemble_config": {
                "input_mapping": {
                    "input_ids": "prefill.input_ids",
                    "dense_features": "prefill.dense_features",
                    "user_profile": "prefill.user_profile",
                    "item_features": "prefill.item_features",
                    "attention_mask": "prefill.attention_mask"
                },
                "output_mapping": {
                    "logits": "prefill.logits",
                    "hidden_states": "prefill.hidden_states"
                }
            }
        }
        
        # ä¿å­˜é…ç½®
        import json
        with open(output_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        logger.info(f"é›†æˆæ¨¡å‹é…ç½®å·²ä¿å­˜: {output_path}")

class DecodeWrapper(nn.Module):
    """Decodeé˜¶æ®µçš„åŒ…è£…å™¨"""
    
    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, token_id, past_key_value_states, dense_features, user_profile, item_features):
        return self.model.forward_decode(
            token_id, past_key_value_states, dense_features, user_profile, item_features
        )

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Export MTGR Model to ONNX')
    parser.add_argument('--prefill', default='mtgr_prefill.onnx', help='Output path for prefill ONNX')
    parser.add_argument('--decode', default='mtgr_decode.onnx', help='Output path for decode ONNX')
    parser.add_argument('--ensemble', default='mtgr_ensemble.json', help='Output path for ensemble config')
    parser.add_argument('--batch_size', type=int, default=1, help='Batch size for export')
    parser.add_argument('--seq_len', type=int, default=100, help='Sequence length for export')
    parser.add_argument('--past_len', type=int, default=500, help='Past sequence length for decode')
    
    # MTGRæ¨¡å‹é…ç½®å‚æ•°
    parser.add_argument('--vocab_size', type=int, default=50000, help='Vocabulary size')
    parser.add_argument('--d_model', type=int, default=1024, help='Model dimension')
    parser.add_argument('--nhead', type=int, default=16, help='Number of attention heads')
    parser.add_argument('--num_layers', type=int, default=24, help='Number of transformer layers')
    parser.add_argument('--d_ff', type=int, default=4096, help='Feedforward dimension')
    parser.add_argument('--max_seq_len', type=int, default=2048, help='Maximum sequence length')
    parser.add_argument('--num_features', type=int, default=1024, help='Number of dense features')
    parser.add_argument('--user_profile_dim', type=int, default=256, help='User profile dimension')
    parser.add_argument('--item_feature_dim', type=int, default=512, help='Item feature dimension')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout rate')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = {
        'vocab_size': args.vocab_size,
        'd_model': args.d_model,
        'nhead': args.nhead,
        'num_layers': args.num_layers,
        'd_ff': args.d_ff,
        'max_seq_len': args.max_seq_len,
        'num_features': args.num_features,
        'user_profile_dim': args.user_profile_dim,
        'item_feature_dim': args.item_feature_dim,
        'dropout': args.dropout
    }
    
    print("=" * 60)
    print("MTGRæ¨¡å‹ONNXå¯¼å‡º")
    print("=" * 60)
    print(f"æ¨¡å‹é…ç½®: {model_config}")
    print(f"å¯¼å‡ºé…ç½®: batch_size={args.batch_size}, seq_len={args.seq_len}")
    
    try:
        # åˆ›å»ºå¯¼å‡ºå™¨
        exporter = MTGRONNXExporter(model_config)
        
        # å¯¼å‡ºPrefillæ¨¡å‹
        exporter.export_prefill_model(args.prefill, args.batch_size, args.seq_len)
        
        # å¯¼å‡ºDecodeæ¨¡å‹
        exporter.export_decode_model(args.decode, args.batch_size, args.past_len)
        
        # åˆ›å»ºé›†æˆé…ç½®
        exporter.export_ensemble_model(args.prefill, args.decode, args.ensemble)
        
        print("\nâœ… æ‰€æœ‰æ¨¡å‹å¯¼å‡ºå®Œæˆ!")
        print(f"  Prefillæ¨¡å‹: {args.prefill}")
        print(f"  Decodeæ¨¡å‹: {args.decode}")
        print(f"  é›†æˆé…ç½®: {args.ensemble}")
        
        # éªŒè¯æ¨¡å‹
        print("\nğŸ” éªŒè¯å¯¼å‡ºçš„æ¨¡å‹...")
        try:
            import onnx
            prefill_model = onnx.load(args.prefill)
            decode_model = onnx.load(args.decode)
            print(f"  Prefillæ¨¡å‹: {prefill_model.graph.input[0].type.tensor_type.shape.dim}")
            print(f"  Decodeæ¨¡å‹: {decode_model.graph.input[0].type.tensor_type.shape.dim}")
            print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡")
        except ImportError:
            print("âš ï¸  ONNXæœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹éªŒè¯")
        except Exception as e:
            print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
