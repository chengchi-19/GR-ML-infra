#!/usr/bin/env python3
"""
KV Cacheæ ¼å¼è½¬æ¢å·¥å…·

å®ç°TensorRT/HSTUæ ¼å¼ä¸vLLMæ ¼å¼ä¹‹é—´çš„KV Cacheè½¬æ¢
"""

import logging
import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union

logger = logging.getLogger(__name__)


class KVCacheConverter:
    """KV Cacheæ ¼å¼è½¬æ¢å™¨"""

    def __init__(self):
        self.supported_formats = ['hstu_tensorrt', 'vllm_paged', 'standard']
        logger.info("âœ… KV Cacheè½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆ")

    def convert_tensorrt_to_vllm(
        self,
        tensorrt_kv_cache: Dict[str, torch.Tensor],
        block_size: int = 16,
        max_seq_len: Optional[int] = None
    ) -> Dict[str, Any]:
        """å°†TensorRT/HSTUæ ¼å¼çš„KV Cacheè½¬æ¢ä¸ºvLLMæ ¼å¼

        Args:
            tensorrt_kv_cache: TensorRTæ ¼å¼çš„KV Cache
            block_size: vLLMçš„blockå¤§å°
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦

        Returns:
            vLLMæ ¼å¼çš„KV Cacheå­—å…¸
        """

        logger.info("ğŸ”„ å¼€å§‹è½¬æ¢KV Cache: TensorRT -> vLLM")

        if not tensorrt_kv_cache:
            logger.warning("è¾“å…¥KV Cacheä¸ºç©º")
            return {}

        try:
            vllm_kv_cache = {}
            conversion_info = {
                'source_format': 'hstu_tensorrt',
                'target_format': 'vllm_paged',
                'block_size': block_size,
                'layers_converted': 0,
                'total_blocks': 0
            }

            # éå†æ¯ä¸€å±‚
            for layer_name, layer_cache in tensorrt_kv_cache.items():
                if not isinstance(layer_cache, dict) or 'key' not in layer_cache or 'value' not in layer_cache:
                    logger.warning(f"è·³è¿‡æ— æ•ˆå±‚: {layer_name}")
                    continue

                key_tensor = layer_cache['key']
                value_tensor = layer_cache['value']

                # éªŒè¯è¾“å…¥æ ¼å¼ [batch_size, seq_len, num_heads, head_dim]
                if len(key_tensor.shape) != 4:
                    logger.warning(f"å±‚ {layer_name} Keyå¼ é‡å½¢çŠ¶ä¸æ­£ç¡®: {key_tensor.shape}")
                    continue

                batch_size, seq_len, num_heads, head_dim = key_tensor.shape

                # è½¬æ¢ä¸ºvLLMçš„blockæ ¼å¼
                vllm_layer_cache = self._convert_layer_to_vllm_blocks(
                    key_tensor, value_tensor, block_size, layer_name
                )

                if vllm_layer_cache:
                    vllm_kv_cache[layer_name] = vllm_layer_cache
                    conversion_info['layers_converted'] += 1
                    conversion_info['total_blocks'] += vllm_layer_cache.get('num_blocks', 0)

            conversion_info['success'] = len(vllm_kv_cache) > 0
            vllm_kv_cache['_conversion_info'] = conversion_info

            logger.info(f"âœ… KV Cacheè½¬æ¢å®Œæˆ: {conversion_info['layers_converted']}å±‚, {conversion_info['total_blocks']}ä¸ªblocks")

            return vllm_kv_cache

        except Exception as e:
            logger.error(f"âŒ KV Cacheè½¬æ¢å¤±è´¥: {e}")
            return {}

    def _convert_layer_to_vllm_blocks(
        self,
        key_tensor: torch.Tensor,
        value_tensor: torch.Tensor,
        block_size: int,
        layer_name: str
    ) -> Dict[str, Any]:
        """å°†å•å±‚çš„K, Vå¼ é‡è½¬æ¢ä¸ºvLLM blockæ ¼å¼"""

        try:
            batch_size, seq_len, num_heads, head_dim = key_tensor.shape

            # è®¡ç®—éœ€è¦çš„blockæ•°é‡
            num_blocks = (seq_len + block_size - 1) // block_size
            padded_seq_len = num_blocks * block_size

            # Paddingåˆ°blockè¾¹ç•Œ
            if seq_len < padded_seq_len:
                padding_size = padded_seq_len - seq_len

                # åˆ›å»ºpaddingå¼ é‡
                key_padding = torch.zeros(
                    batch_size, padding_size, num_heads, head_dim,
                    dtype=key_tensor.dtype, device=key_tensor.device
                )
                value_padding = torch.zeros(
                    batch_size, padding_size, num_heads, head_dim,
                    dtype=value_tensor.dtype, device=value_tensor.device
                )

                # æ‹¼æ¥padding
                key_padded = torch.cat([key_tensor, key_padding], dim=1)
                value_padded = torch.cat([value_tensor, value_padding], dim=1)
            else:
                key_padded = key_tensor
                value_padded = value_tensor

            # é‡å¡‘ä¸ºblockæ ¼å¼: [batch_size, num_blocks, block_size, num_heads, head_dim]
            key_blocks = key_padded.view(batch_size, num_blocks, block_size, num_heads, head_dim)
            value_blocks = value_padded.view(batch_size, num_blocks, block_size, num_heads, head_dim)

            # vLLMé€šå¸¸ä½¿ç”¨ç¬¬ä¸€ä¸ªbatchï¼ˆç®€åŒ–å¤„ç†ï¼‰
            # å®é™…ç”Ÿäº§ç¯å¢ƒå¯èƒ½éœ€è¦å¤„ç†æ•´ä¸ªbatch
            key_vllm = key_blocks[0]  # [num_blocks, block_size, num_heads, head_dim]
            value_vllm = value_blocks[0]

            # è½¬æ¢ä¸ºvLLMæœŸæœ›çš„ç»´åº¦é¡ºåº: [num_blocks, num_heads, block_size, head_dim]
            key_vllm = key_vllm.transpose(1, 2)  # [num_blocks, num_heads, block_size, head_dim]
            value_vllm = value_vllm.transpose(1, 2)

            return {
                'key_blocks': key_vllm,
                'value_blocks': value_vllm,
                'block_info': {
                    'num_blocks': num_blocks,
                    'block_size': block_size,
                    'num_heads': num_heads,
                    'head_dim': head_dim,
                    'original_seq_len': seq_len,
                    'padded_seq_len': padded_seq_len,
                    'padding_size': padded_seq_len - seq_len
                },
                'metadata': {
                    'layer_name': layer_name,
                    'original_shape': list(key_tensor.shape),
                    'block_shape': list(key_vllm.shape),
                    'dtype': str(key_tensor.dtype),
                    'device': str(key_tensor.device)
                }
            }

        except Exception as e:
            logger.error(f"âŒ å±‚ {layer_name} è½¬æ¢å¤±è´¥: {e}")
            return {}

    def convert_vllm_to_tensorrt(
        self,
        vllm_kv_cache: Dict[str, Any]
    ) -> Dict[str, torch.Tensor]:
        """å°†vLLMæ ¼å¼çš„KV Cacheè½¬æ¢å›TensorRTæ ¼å¼"""

        logger.info("ğŸ”„ å¼€å§‹è½¬æ¢KV Cache: vLLM -> TensorRT")

        if not vllm_kv_cache:
            logger.warning("è¾“å…¥vLLM KV Cacheä¸ºç©º")
            return {}

        try:
            tensorrt_kv_cache = {}

            for layer_name, layer_cache in vllm_kv_cache.items():
                if layer_name.startswith('_'):  # è·³è¿‡å…ƒæ•°æ®å­—æ®µ
                    continue

                if not isinstance(layer_cache, dict):
                    continue

                # æå–blockæ•°æ®
                key_blocks = layer_cache.get('key_blocks')
                value_blocks = layer_cache.get('value_blocks')
                block_info = layer_cache.get('block_info', {})

                if key_blocks is None or value_blocks is None:
                    logger.warning(f"è·³è¿‡æ— æ•ˆå±‚: {layer_name}")
                    continue

                # è½¬æ¢å›åŸå§‹æ ¼å¼
                original_key, original_value = self._convert_vllm_blocks_to_tensor(
                    key_blocks, value_blocks, block_info, layer_name
                )

                if original_key is not None and original_value is not None:
                    tensorrt_kv_cache[layer_name] = {
                        'key': original_key,
                        'value': original_value
                    }

            logger.info(f"âœ… vLLM -> TensorRTè½¬æ¢å®Œæˆ: {len(tensorrt_kv_cache)}å±‚")
            return tensorrt_kv_cache

        except Exception as e:
            logger.error(f"âŒ vLLM -> TensorRTè½¬æ¢å¤±è´¥: {e}")
            return {}

    def _convert_vllm_blocks_to_tensor(
        self,
        key_blocks: torch.Tensor,
        value_blocks: torch.Tensor,
        block_info: Dict[str, int],
        layer_name: str
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:
        """å°†vLLM blocksè½¬æ¢å›åŸå§‹å¼ é‡æ ¼å¼"""

        try:
            # è·å–blockä¿¡æ¯
            num_blocks = block_info.get('num_blocks', key_blocks.shape[0])
            block_size = block_info.get('block_size', key_blocks.shape[2])
            num_heads = block_info.get('num_heads', key_blocks.shape[1])
            head_dim = block_info.get('head_dim', key_blocks.shape[3])
            original_seq_len = block_info.get('original_seq_len')

            # vLLMæ ¼å¼: [num_blocks, num_heads, block_size, head_dim]
            # è½¬æ¢ä¸º: [num_blocks, block_size, num_heads, head_dim]
            key_reshaped = key_blocks.transpose(1, 2)  # [num_blocks, block_size, num_heads, head_dim]
            value_reshaped = value_blocks.transpose(1, 2)

            # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼: [1, padded_seq_len, num_heads, head_dim]
            padded_seq_len = num_blocks * block_size
            key_sequence = key_reshaped.view(1, padded_seq_len, num_heads, head_dim)
            value_sequence = value_reshaped.view(1, padded_seq_len, num_heads, head_dim)

            # å¦‚æœæœ‰åŸå§‹åºåˆ—é•¿åº¦ä¿¡æ¯ï¼Œå»é™¤padding
            if original_seq_len and original_seq_len < padded_seq_len:
                key_sequence = key_sequence[:, :original_seq_len, :, :]
                value_sequence = value_sequence[:, :original_seq_len, :, :]

            return key_sequence, value_sequence

        except Exception as e:
            logger.error(f"âŒ å±‚ {layer_name} blocksè½¬æ¢å¤±è´¥: {e}")
            return None, None

    def validate_kv_cache_format(
        self,
        kv_cache: Dict[str, Any],
        expected_format: str
    ) -> bool:
        """éªŒè¯KV Cacheæ ¼å¼æ˜¯å¦æ­£ç¡®"""

        if expected_format not in self.supported_formats:
            logger.error(f"ä¸æ”¯æŒçš„æ ¼å¼: {expected_format}")
            return False

        if not kv_cache:
            logger.warning("KV Cacheä¸ºç©º")
            return False

        try:
            if expected_format == 'hstu_tensorrt':
                return self._validate_tensorrt_format(kv_cache)
            elif expected_format == 'vllm_paged':
                return self._validate_vllm_format(kv_cache)
            else:
                logger.warning(f"æ ¼å¼éªŒè¯æœªå®ç°: {expected_format}")
                return True

        except Exception as e:
            logger.error(f"âŒ KV Cacheæ ¼å¼éªŒè¯å¤±è´¥: {e}")
            return False

    def _validate_tensorrt_format(self, kv_cache: Dict[str, Any]) -> bool:
        """éªŒè¯TensorRTæ ¼å¼çš„KV Cache"""

        for layer_name, layer_cache in kv_cache.items():
            if not isinstance(layer_cache, dict):
                logger.error(f"å±‚ {layer_name} ä¸æ˜¯å­—å…¸æ ¼å¼")
                return False

            if 'key' not in layer_cache or 'value' not in layer_cache:
                logger.error(f"å±‚ {layer_name} ç¼ºå°‘keyæˆ–value")
                return False

            key_tensor = layer_cache['key']
            value_tensor = layer_cache['value']

            if not isinstance(key_tensor, torch.Tensor) or not isinstance(value_tensor, torch.Tensor):
                logger.error(f"å±‚ {layer_name} key/valueä¸æ˜¯torch.Tensor")
                return False

            if len(key_tensor.shape) != 4 or len(value_tensor.shape) != 4:
                logger.error(f"å±‚ {layer_name} å¼ é‡ç»´åº¦ä¸æ­£ç¡®")
                return False

            if key_tensor.shape != value_tensor.shape:
                logger.error(f"å±‚ {layer_name} keyå’Œvalueå½¢çŠ¶ä¸åŒ¹é…")
                return False

        return True

    def _validate_vllm_format(self, kv_cache: Dict[str, Any]) -> bool:
        """éªŒè¯vLLMæ ¼å¼çš„KV Cache"""

        for layer_name, layer_cache in kv_cache.items():
            if layer_name.startswith('_'):  # è·³è¿‡å…ƒæ•°æ®
                continue

            if not isinstance(layer_cache, dict):
                logger.error(f"vLLMå±‚ {layer_name} ä¸æ˜¯å­—å…¸æ ¼å¼")
                return False

            required_fields = ['key_blocks', 'value_blocks', 'block_info']
            for field in required_fields:
                if field not in layer_cache:
                    logger.error(f"vLLMå±‚ {layer_name} ç¼ºå°‘å­—æ®µ: {field}")
                    return False

            key_blocks = layer_cache['key_blocks']
            value_blocks = layer_cache['value_blocks']

            if not isinstance(key_blocks, torch.Tensor) or not isinstance(value_blocks, torch.Tensor):
                logger.error(f"vLLMå±‚ {layer_name} blocksä¸æ˜¯torch.Tensor")
                return False

            if len(key_blocks.shape) != 4 or len(value_blocks.shape) != 4:
                logger.error(f"vLLMå±‚ {layer_name} blocksç»´åº¦ä¸æ­£ç¡®")
                return False

        return True

    def get_kv_cache_stats(self, kv_cache: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–KV Cacheç»Ÿè®¡ä¿¡æ¯"""

        if not kv_cache:
            return {'empty': True}

        stats = {
            'total_layers': 0,
            'total_parameters': 0,
            'memory_usage_mb': 0,
            'format_type': 'unknown',
            'layers': {}
        }

        try:
            # æ£€æµ‹æ ¼å¼ç±»å‹
            if self.validate_kv_cache_format(kv_cache, 'hstu_tensorrt'):
                stats['format_type'] = 'hstu_tensorrt'
            elif self.validate_kv_cache_format(kv_cache, 'vllm_paged'):
                stats['format_type'] = 'vllm_paged'

            # ç»Ÿè®¡æ¯å±‚ä¿¡æ¯
            for layer_name, layer_cache in kv_cache.items():
                if layer_name.startswith('_'):  # è·³è¿‡å…ƒæ•°æ®
                    continue

                layer_stats = self._get_layer_stats(layer_cache, stats['format_type'])
                if layer_stats:
                    stats['layers'][layer_name] = layer_stats
                    stats['total_layers'] += 1
                    stats['total_parameters'] += layer_stats.get('parameters', 0)
                    stats['memory_usage_mb'] += layer_stats.get('memory_mb', 0)

            return stats

        except Exception as e:
            logger.error(f"âŒ è·å–KV Cacheç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

    def _get_layer_stats(self, layer_cache: Dict[str, Any], format_type: str) -> Dict[str, Any]:
        """è·å–å•å±‚ç»Ÿè®¡ä¿¡æ¯"""

        try:
            if format_type == 'hstu_tensorrt':
                key_tensor = layer_cache.get('key')
                value_tensor = layer_cache.get('value')

                if key_tensor is not None and value_tensor is not None:
                    key_params = key_tensor.numel()
                    value_params = value_tensor.numel()
                    total_params = key_params + value_params

                    # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆå­—èŠ‚ï¼‰
                    element_size = key_tensor.element_size()
                    memory_bytes = total_params * element_size
                    memory_mb = memory_bytes / (1024 * 1024)

                    return {
                        'shape': list(key_tensor.shape),
                        'parameters': total_params,
                        'memory_mb': memory_mb,
                        'dtype': str(key_tensor.dtype),
                        'device': str(key_tensor.device)
                    }

            elif format_type == 'vllm_paged':
                key_blocks = layer_cache.get('key_blocks')
                value_blocks = layer_cache.get('value_blocks')

                if key_blocks is not None and value_blocks is not None:
                    key_params = key_blocks.numel()
                    value_params = value_blocks.numel()
                    total_params = key_params + value_params

                    element_size = key_blocks.element_size()
                    memory_bytes = total_params * element_size
                    memory_mb = memory_bytes / (1024 * 1024)

                    block_info = layer_cache.get('block_info', {})

                    return {
                        'block_shape': list(key_blocks.shape),
                        'parameters': total_params,
                        'memory_mb': memory_mb,
                        'dtype': str(key_blocks.dtype),
                        'device': str(key_blocks.device),
                        'num_blocks': block_info.get('num_blocks', 0),
                        'block_size': block_info.get('block_size', 0)
                    }

            return {}

        except Exception as e:
            logger.error(f"âŒ è·å–å±‚ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# å…¨å±€è½¬æ¢å™¨å®ä¾‹
_kv_cache_converter = None


def get_kv_cache_converter() -> KVCacheConverter:
    """è·å–å…¨å±€KV Cacheè½¬æ¢å™¨å®ä¾‹"""
    global _kv_cache_converter
    if _kv_cache_converter is None:
        _kv_cache_converter = KVCacheConverter()
    return _kv_cache_converter


def convert_tensorrt_to_vllm(
    tensorrt_kv_cache: Dict[str, torch.Tensor],
    block_size: int = 16
) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šTensorRT -> vLLMè½¬æ¢"""
    converter = get_kv_cache_converter()
    return converter.convert_tensorrt_to_vllm(tensorrt_kv_cache, block_size)


def convert_vllm_to_tensorrt(vllm_kv_cache: Dict[str, Any]) -> Dict[str, torch.Tensor]:
    """ä¾¿æ·å‡½æ•°ï¼švLLM -> TensorRTè½¬æ¢"""
    converter = get_kv_cache_converter()
    return converter.convert_vllm_to_tensorrt(vllm_kv_cache)


if __name__ == "__main__":
    # æµ‹è¯•KV Cacheè½¬æ¢å™¨
    logger.info("ğŸ§ª æµ‹è¯•KV Cacheè½¬æ¢å™¨...")

    # åˆ›å»ºæµ‹è¯•ç”¨çš„TensorRTæ ¼å¼KV Cache
    batch_size, seq_len, num_heads, head_dim = 1, 64, 12, 64
    num_layers = 3

    test_tensorrt_cache = {}
    for i in range(num_layers):
        test_tensorrt_cache[f'layer_{i}'] = {
            'key': torch.randn(batch_size, seq_len, num_heads, head_dim),
            'value': torch.randn(batch_size, seq_len, num_heads, head_dim)
        }

    converter = KVCacheConverter()

    # æµ‹è¯•TensorRT -> vLLMè½¬æ¢
    vllm_cache = converter.convert_tensorrt_to_vllm(test_tensorrt_cache, block_size=16)
    print(f"TensorRT -> vLLMè½¬æ¢ç»“æœ: {len(vllm_cache)}å±‚")

    # æµ‹è¯•vLLM -> TensorRTè½¬æ¢
    converted_back = converter.convert_vllm_to_tensorrt(vllm_cache)
    print(f"vLLM -> TensorRTè½¬æ¢ç»“æœ: {len(converted_back)}å±‚")

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = converter.get_kv_cache_stats(test_tensorrt_cache)
    print(f"TensorRT Cacheç»Ÿè®¡: {stats}")

    vllm_stats = converter.get_kv_cache_stats(vllm_cache)
    print(f"vLLM Cacheç»Ÿè®¡: {vllm_stats}")

    print("âœ… KV Cacheè½¬æ¢å™¨æµ‹è¯•å®Œæˆ")