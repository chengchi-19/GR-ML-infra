#!/usr/bin/env python3
"""
GR-ML-infra TensorRTæ’ä»¶Pythonç»‘å®š

æä¾›Pythonæ¥å£æ¥åŠ è½½å’Œä½¿ç”¨TensorRTæ’ä»¶
"""

import os
import sys
import ctypes
import logging
from typing import Optional, List, Dict, Any
import numpy as np

logger = logging.getLogger(__name__)

try:
    import tensorrt as trt
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    logger.warning("TensorRT Python APIä¸å¯ç”¨")

class GRMLInfraTensorRTPlugins:
    """GR-ML-infra TensorRTæ’ä»¶ç®¡ç†å™¨"""

    def __init__(self, plugin_lib_path: Optional[str] = None):
        self.plugin_lib = None
        self.plugins_initialized = False

        if not TENSORRT_AVAILABLE:
            logger.warning("TensorRTä¸å¯ç”¨ï¼Œæ’ä»¶åŠŸèƒ½å°†è¢«ç¦ç”¨")
            return

        # æŸ¥æ‰¾æ’ä»¶åº“
        if plugin_lib_path is None:
            plugin_lib_path = self._find_plugin_library()

        if plugin_lib_path and os.path.exists(plugin_lib_path):
            self._load_plugin_library(plugin_lib_path)
            self._initialize_plugins()
        else:
            logger.warning(f"TensorRTæ’ä»¶åº“æœªæ‰¾åˆ°: {plugin_lib_path}")

    def _find_plugin_library(self) -> Optional[str]:
        """æŸ¥æ‰¾æ’ä»¶åº“æ–‡ä»¶"""
        possible_paths = [
            # ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„
            os.path.join(os.path.dirname(__file__), "libgr_ml_infra_tensorrt_plugins.so"),
            os.path.join(os.path.dirname(__file__), "cpp", "libgr_ml_infra_tensorrt_plugins.so"),

            # ç³»ç»Ÿè·¯å¾„
            "/usr/local/lib/libgr_ml_infra_tensorrt_plugins.so",
            "/usr/lib/libgr_ml_infra_tensorrt_plugins.so",

            # é¡¹ç›®æ ¹ç›®å½•
            os.path.join(os.path.dirname(__file__), "..", "..", "..", "lib", "libgr_ml_infra_tensorrt_plugins.so"),
        ]

        for path in possible_paths:
            if os.path.exists(path):
                logger.info(f"æ‰¾åˆ°TensorRTæ’ä»¶åº“: {path}")
                return path

        return None

    def _load_plugin_library(self, lib_path: str):
        """åŠ è½½æ’ä»¶åº“"""
        try:
            self.plugin_lib = ctypes.CDLL(lib_path)

            # å®šä¹‰Cå‡½æ•°ç­¾å
            self.plugin_lib.initialize_gr_ml_infra_plugins.argtypes = []
            self.plugin_lib.initialize_gr_ml_infra_plugins.restype = None

            self.plugin_lib.get_num_registered_plugins.argtypes = []
            self.plugin_lib.get_num_registered_plugins.restype = ctypes.c_int

            logger.info(f"âœ… TensorRTæ’ä»¶åº“åŠ è½½æˆåŠŸ: {lib_path}")

        except Exception as e:
            logger.error(f"âŒ åŠ è½½TensorRTæ’ä»¶åº“å¤±è´¥: {e}")
            self.plugin_lib = None

    def _initialize_plugins(self):
        """åˆå§‹åŒ–æ’ä»¶"""
        if self.plugin_lib is None:
            return

        try:
            # åˆå§‹åŒ–æ’ä»¶
            self.plugin_lib.initialize_gr_ml_infra_plugins()

            # è·å–æ’ä»¶æ•°é‡
            num_plugins = self.plugin_lib.get_num_registered_plugins()

            self.plugins_initialized = True
            logger.info(f"âœ… TensorRTæ’ä»¶åˆå§‹åŒ–æˆåŠŸï¼Œæ³¨å†Œäº† {num_plugins} ä¸ªæ’ä»¶")

        except Exception as e:
            logger.error(f"âŒ TensorRTæ’ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")

    def is_available(self) -> bool:
        """æ£€æŸ¥æ’ä»¶æ˜¯å¦å¯ç”¨"""
        return TENSORRT_AVAILABLE and self.plugins_initialized

    def get_num_registered_plugins(self) -> int:
        """è·å–å·²æ³¨å†Œæ’ä»¶æ•°é‡"""
        if not self.is_available():
            return 0

        try:
            return self.plugin_lib.get_num_registered_plugins()
        except Exception as e:
            logger.error(f"è·å–æ’ä»¶æ•°é‡å¤±è´¥: {e}")
            return 0

    def create_onnx_custom_node(
        self,
        plugin_name: str,
        inputs: List[str],
        outputs: List[str],
        attributes: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºONNXè‡ªå®šä¹‰èŠ‚ç‚¹é…ç½®"""
        if attributes is None:
            attributes = {}

        return {
            'op_type': plugin_name,
            'inputs': inputs,
            'outputs': outputs,
            'attributes': attributes,
            'domain': 'gr.ml.infra'
        }

    def get_supported_plugins(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ’ä»¶åˆ—è¡¨"""
        return [
            'FusedAttentionLayerNorm',
            'HierarchicalSequenceFusion',
            'HSTUHierarchicalAttention',
            'InteractionTritonFast',
            'SequenceRecommendationInteraction'
        ]

# å…¨å±€æ’ä»¶ç®¡ç†å™¨å®ä¾‹
_plugin_manager = None

def get_plugin_manager() -> GRMLInfraTensorRTPlugins:
    """è·å–å…¨å±€æ’ä»¶ç®¡ç†å™¨å®ä¾‹"""
    global _plugin_manager
    if _plugin_manager is None:
        _plugin_manager = GRMLInfraTensorRTPlugins()
    return _plugin_manager

def initialize_plugins(plugin_lib_path: Optional[str] = None) -> bool:
    """åˆå§‹åŒ–TensorRTæ’ä»¶"""
    global _plugin_manager
    _plugin_manager = GRMLInfraTensorRTPlugins(plugin_lib_path)
    return _plugin_manager.is_available()

def is_plugins_available() -> bool:
    """æ£€æŸ¥æ’ä»¶æ˜¯å¦å¯ç”¨"""
    return get_plugin_manager().is_available()

def get_num_registered_plugins() -> int:
    """è·å–å·²æ³¨å†Œæ’ä»¶æ•°é‡"""
    return get_plugin_manager().get_num_registered_plugins()

def create_fused_attention_layernorm_node(
    input_name: str,
    output_name: str,
    hidden_dim: int,
    num_heads: int,
    dropout_rate: float = 0.1,
    layer_norm_eps: float = 1e-5
) -> Dict[str, Any]:
    """åˆ›å»ºèåˆæ³¨æ„åŠ›LayerNormèŠ‚ç‚¹"""
    return get_plugin_manager().create_onnx_custom_node(
        plugin_name='FusedAttentionLayerNorm',
        inputs=[input_name],
        outputs=[output_name],
        attributes={
            'hidden_dim': hidden_dim,
            'num_heads': num_heads,
            'dropout_rate': dropout_rate,
            'layer_norm_eps': layer_norm_eps
        }
    )

def create_hierarchical_sequence_fusion_node(
    input_name: str,
    output_name: str,
    hidden_dim: int,
    num_levels: int = 3
) -> Dict[str, Any]:
    """åˆ›å»ºå±‚æ¬¡åŒ–åºåˆ—èåˆèŠ‚ç‚¹"""
    return get_plugin_manager().create_onnx_custom_node(
        plugin_name='HierarchicalSequenceFusion',
        inputs=[input_name],
        outputs=[output_name],
        attributes={
            'hidden_dim': hidden_dim,
            'num_levels': num_levels
        }
    )

def create_interaction_triton_fast_node(
    input_name: str,
    output_name: str,
    num_features: int,
    embedding_dim: int
) -> Dict[str, Any]:
    """åˆ›å»ºå¿«é€Ÿäº¤äº’ç®—å­èŠ‚ç‚¹"""
    return get_plugin_manager().create_onnx_custom_node(
        plugin_name='InteractionTritonFast',
        inputs=[input_name],
        outputs=[output_name],
        attributes={
            'num_features': num_features,
            'embedding_dim': embedding_dim
        }
    )

if __name__ == "__main__":
    # æµ‹è¯•æ’ä»¶åŠ è½½
    print("ğŸ”§ æµ‹è¯•GR-ML-infra TensorRTæ’ä»¶åŠ è½½...")

    if initialize_plugins():
        print(f"âœ… æ’ä»¶åŠ è½½æˆåŠŸï¼Œæ³¨å†Œäº† {get_num_registered_plugins()} ä¸ªæ’ä»¶")

        # æµ‹è¯•åˆ›å»ºè‡ªå®šä¹‰èŠ‚ç‚¹
        node = create_fused_attention_layernorm_node(
            input_name="input",
            output_name="output",
            hidden_dim=1024,
            num_heads=16
        )
        print(f"ğŸ¯ èåˆæ³¨æ„åŠ›èŠ‚ç‚¹é…ç½®: {node}")

    else:
        print("âŒ æ’ä»¶åŠ è½½å¤±è´¥")