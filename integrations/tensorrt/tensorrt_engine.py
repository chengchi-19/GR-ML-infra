#!/usr/bin/env python3
"""
TensorRTæ¨ç†ä¼˜åŒ–æ¡†æ¶é›†æˆé€‚é…å™¨

åŸºäºNVIDIA TensorRT Python APIï¼Œæä¾›é«˜æ€§èƒ½GPUæ¨ç†åŠ é€Ÿï¼Œ
æ”¯æŒFP16/INT8é‡åŒ–å’ŒåŠ¨æ€shapeä¼˜åŒ–ã€‚
"""

import os
import sys
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
import torch

logger = logging.getLogger(__name__)

try:
    # å¯¼å…¥TensorRTæ ¸å¿ƒç»„ä»¶
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    
    TENSORRT_AVAILABLE = True
    logger.info("âœ… TensorRTæ¡†æ¶å¯¼å…¥æˆåŠŸ")
    
except ImportError as e:
    TENSORRT_AVAILABLE = False
    logger.warning(f"âš ï¸ TensorRTæ¡†æ¶å¯¼å…¥å¤±è´¥: {e}")


class TensorRTConfig:
    """TensorRTæ¨ç†é…ç½®"""
    def __init__(
        self,
        model_name: str = "hstu-tensorrt",
        onnx_path: Optional[str] = None,
        engine_path: Optional[str] = None,
        precision: str = "fp16",  # fp32, fp16, int8
        max_batch_size: int = 8,
        max_workspace_size: int = 1 << 30,  # 1GB
        optimization_level: int = 5,
        enable_dynamic_shapes: bool = True,
        min_shapes: Optional[Dict[str, Tuple]] = None,
        opt_shapes: Optional[Dict[str, Tuple]] = None,
        max_shapes: Optional[Dict[str, Tuple]] = None,
        enable_tensor_parallelism: bool = False,
        dla_core: Optional[int] = None,
        enable_strict_types: bool = False,
        enable_fp16_io: bool = False,
        calibration_cache_path: Optional[str] = None,
        **kwargs
    ):
        self.model_name = model_name
        self.onnx_path = onnx_path
        self.engine_path = engine_path
        self.precision = precision
        self.max_batch_size = max_batch_size
        self.max_workspace_size = max_workspace_size
        self.optimization_level = optimization_level
        self.enable_dynamic_shapes = enable_dynamic_shapes
        
        # åŠ¨æ€shapeé…ç½®
        self.min_shapes = min_shapes or {}
        self.opt_shapes = opt_shapes or {}
        self.max_shapes = max_shapes or {}
        
        # é«˜çº§é…ç½®
        self.enable_tensor_parallelism = enable_tensor_parallelism
        self.dla_core = dla_core
        self.enable_strict_types = enable_strict_types
        self.enable_fp16_io = enable_fp16_io
        self.calibration_cache_path = calibration_cache_path
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        for key, value in kwargs.items():
            setattr(self, key, value)


class TensorRTOptimizedEngine:
    """
    TensorRTä¼˜åŒ–æ¨ç†å¼•æ“
    
    æä¾›é«˜æ€§èƒ½GPUæ¨ç†åŠ é€Ÿå’Œå†…å­˜ä¼˜åŒ–
    """
    
    def __init__(self, config: TensorRTConfig, hstu_model=None):
        self.config = config
        self.hstu_model = hstu_model
        
        if not TENSORRT_AVAILABLE:
            logger.warning("TensorRTä¸å¯ç”¨ï¼Œä½¿ç”¨HSTUæ¨¡å‹å›é€€")
            self.tensorrt_available = False
            return
        
        self.tensorrt_available = True
        self.logger = trt.Logger(trt.Logger.WARNING)
        self.runtime = None
        self.engine = None
        self.context = None
        
        # ç¼“å†²åŒºç®¡ç†
        self.input_bindings = {}
        self.output_bindings = {}
        self.host_inputs = {}
        self.host_outputs = {}
        self.device_inputs = {}
        self.device_outputs = {}
        self.bindings = []
        self.stream = None
        
        # åˆå§‹åŒ–å¼•æ“
        self._initialize_tensorrt_engine()
    
    def _initialize_tensorrt_engine(self):
        """åˆå§‹åŒ–TensorRTå¼•æ“"""
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„æ„å»ºçš„å¼•æ“
            if self.config.engine_path and os.path.exists(self.config.engine_path):
                logger.info(f"åŠ è½½é¢„æ„å»ºçš„TensorRTå¼•æ“: {self.config.engine_path}")
                self._load_engine(self.config.engine_path)
            elif self.config.onnx_path and os.path.exists(self.config.onnx_path):
                logger.info(f"ä»ONNXæ¨¡å‹æ„å»ºTensorRTå¼•æ“: {self.config.onnx_path}")
                engine_path = self._build_engine_from_onnx(self.config.onnx_path)
                if engine_path:
                    self._load_engine(engine_path)
                else:
                    raise RuntimeError("ä»ONNXæ„å»ºå¼•æ“å¤±è´¥")
            elif self.hstu_model is not None:
                logger.info("ä»HSTUæ¨¡å‹æ„å»ºTensorRTå¼•æ“")
                self._build_engine_from_hstu_model()
            else:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹æºï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.tensorrt_available = False
                return
            
            # åˆå§‹åŒ–CUDA stream
            self.stream = cuda.Stream()
            
            logger.info("âœ… TensorRTå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ TensorRTå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.tensorrt_available = False
    
    def _load_engine(self, engine_path: str):
        """åŠ è½½TensorRTå¼•æ“"""
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        
        self.runtime = trt.Runtime(self.logger)
        self.engine = self.runtime.deserialize_cuda_engine(engine_data)
        
        if self.engine is None:
            raise RuntimeError("å¼•æ“ååºåˆ—åŒ–å¤±è´¥")
        
        self.context = self.engine.create_execution_context()
        self._setup_bindings()
    
    def _build_engine_from_onnx(self, onnx_path: str) -> Optional[str]:
        """ä»ONNXæ¨¡å‹æ„å»ºTensorRTå¼•æ“"""
        try:
            logger.info(f"ä»ONNXæ¨¡å‹æ„å»ºTensorRTå¼•æ“: {onnx_path}")
            
            # éªŒè¯ONNXæ¨¡å‹
            if not self._validate_onnx_model(onnx_path):
                logger.error("ONNXæ¨¡å‹éªŒè¯å¤±è´¥")
                return None
            
            # åˆ›å»ºæ„å»ºå™¨å’Œç½‘ç»œ
            builder = trt.Builder(self.logger)
            config = builder.create_builder_config()
            
            # è®¾ç½®å†…å­˜æ± å¤§å° (TensorRT 8.5+)
            if hasattr(config, 'set_memory_pool_limit'):
                config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, self.config.max_workspace_size)
            else:
                config.max_workspace_size = self.config.max_workspace_size
            
            # è®¾ç½®ç²¾åº¦å’Œä¼˜åŒ–æ ‡å¿—
            self._configure_precision_and_optimization(config, builder)
            
            # åˆ›å»ºç½‘ç»œ
            network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
            network = builder.create_network(network_flags)
            parser = trt.OnnxParser(network, self.logger)
            
            # è§£æONNX
            logger.info("è§£æONNXæ¨¡å‹...")
            with open(onnx_path, 'rb') as f:
                onnx_data = f.read()
                if not parser.parse(onnx_data):
                    logger.error("ONNXè§£æå¤±è´¥:")
                    for i in range(parser.num_errors):
                        error = parser.get_error(i)
                        logger.error(f"  é”™è¯¯ {i}: {error}")
                    return None
            
            logger.info(f"âœ… ONNXè§£ææˆåŠŸï¼Œç½‘ç»œæœ‰ {network.num_layers} å±‚")
            
            # æ‰“å°ç½‘ç»œä¿¡æ¯
            self._print_network_info(network)
            
            # è®¾ç½®ä¼˜åŒ–é…ç½®
            if self.config.enable_dynamic_shapes:
                self._setup_optimization_profiles(builder, config, network)
            
            # æ„å»ºå¼•æ“
            logger.info("ğŸ”§ å¼€å§‹æ„å»ºTensorRTå¼•æ“ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
            serialized_engine = builder.build_serialized_network(network, config)
            
            if serialized_engine is None:
                logger.error("âŒ å¼•æ“æ„å»ºå¤±è´¥")
                return None
            
            # ä¿å­˜å¼•æ“
            engine_path = self.config.engine_path or f"{self.config.model_name}.trt"
            os.makedirs(os.path.dirname(engine_path), exist_ok=True)
            
            with open(engine_path, 'wb') as f:
                f.write(serialized_engine)
            
            logger.info(f"âœ… TensorRTå¼•æ“ä¿å­˜åˆ°: {engine_path}")
            
            # éªŒè¯ç”Ÿæˆçš„å¼•æ“
            if self._validate_engine(engine_path):
                logger.info("âœ… TensorRTå¼•æ“éªŒè¯æˆåŠŸ")
                return engine_path
            else:
                logger.error("âŒ TensorRTå¼•æ“éªŒè¯å¤±è´¥")
                return None
            
        except Exception as e:
            logger.error(f"âŒ ä»ONNXæ„å»ºå¼•æ“å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def _validate_onnx_model(self, onnx_path: str) -> bool:
        """éªŒè¯ONNXæ¨¡å‹"""
        try:
            import onnx
            # åŠ è½½å¹¶æ£€æŸ¥ONNXæ¨¡å‹
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            # æ‰“å°æ¨¡å‹åŸºæœ¬ä¿¡æ¯
            logger.info(f"ONNXæ¨¡å‹ä¿¡æ¯:")
            logger.info(f"  - Opsetç‰ˆæœ¬: {onnx_model.opset_import[0].version}")
            logger.info(f"  - è¾“å…¥æ•°é‡: {len(onnx_model.graph.input)}")
            logger.info(f"  - è¾“å‡ºæ•°é‡: {len(onnx_model.graph.output)}")
            
            for i, input_info in enumerate(onnx_model.graph.input):
                logger.info(f"  - è¾“å…¥{i}: {input_info.name}")
            
            for i, output_info in enumerate(onnx_model.graph.output):
                logger.info(f"  - è¾“å‡º{i}: {output_info.name}")
            
            return True
            
        except Exception as e:
            logger.error(f"ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _configure_precision_and_optimization(self, config, builder):
        """é…ç½®ç²¾åº¦å’Œä¼˜åŒ–è®¾ç½®"""
        # è®¾ç½®ç²¾åº¦
        if self.config.precision == "fp16" and builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)
            logger.info("âœ… å¯ç”¨FP16ç²¾åº¦")
            
            # å¯ç”¨FP16 I/Oï¼ˆå¦‚æœæ”¯æŒï¼‰
            if self.config.enable_fp16_io:
                config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)
                
        elif self.config.precision == "int8" and builder.platform_has_fast_int8:
            config.set_flag(trt.BuilderFlag.INT8)
            logger.info("âœ… å¯ç”¨INT8ç²¾åº¦")
            
            # INT8æ ¡å‡†ï¼ˆå¦‚æœæä¾›ï¼‰
            if self.config.calibration_cache_path and os.path.exists(self.config.calibration_cache_path):
                # è¿™é‡Œéœ€è¦å®ç°INT8æ ¡å‡†å™¨
                logger.info(f"ä½¿ç”¨INT8æ ¡å‡†ç¼“å­˜: {self.config.calibration_cache_path}")
        
        else:
            logger.info("ä½¿ç”¨FP32ç²¾åº¦")
        
        # è®¾ç½®ä¼˜åŒ–çº§åˆ«
        if hasattr(config, 'builder_optimization_level'):
            config.builder_optimization_level = self.config.optimization_level
            logger.info(f"ä¼˜åŒ–çº§åˆ«: {self.config.optimization_level}")
        
        # å…¶ä»–ä¼˜åŒ–æ ‡å¿—
        if self.config.enable_strict_types:
            config.set_flag(trt.BuilderFlag.STRICT_TYPES)
            logger.info("å¯ç”¨ä¸¥æ ¼ç±»å‹æ£€æŸ¥")
        
        # å¯ç”¨æ›´å¤šä¼˜åŒ–é€‰é¡¹
        config.set_flag(trt.BuilderFlag.GPU_FALLBACK)
        config.set_flag(trt.BuilderFlag.REFIT)
        
        # DLAæ”¯æŒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.dla_core is not None and builder.max_DLA_batch_size > 0:
            config.default_device_type = trt.DeviceType.DLA
            config.DLA_core = self.config.dla_core
            logger.info(f"ä½¿ç”¨DLAæ ¸å¿ƒ: {self.config.dla_core}")
    
    def _print_network_info(self, network):
        """æ‰“å°ç½‘ç»œè¯¦ç»†ä¿¡æ¯"""
        logger.info("ğŸ” ç½‘ç»œç»“æ„ä¿¡æ¯:")
        logger.info(f"  - æ€»å±‚æ•°: {network.num_layers}")
        logger.info(f"  - è¾“å…¥æ•°é‡: {network.num_inputs}")
        logger.info(f"  - è¾“å‡ºæ•°é‡: {network.num_outputs}")
        
        # æ‰“å°è¾“å…¥ä¿¡æ¯
        for i in range(network.num_inputs):
            input_tensor = network.get_input(i)
            logger.info(f"  - è¾“å…¥{i}: {input_tensor.name}, shape={input_tensor.shape}, dtype={input_tensor.dtype}")
        
        # æ‰“å°è¾“å‡ºä¿¡æ¯
        for i in range(network.num_outputs):
            output_tensor = network.get_output(i)
            logger.info(f"  - è¾“å‡º{i}: {output_tensor.name}, shape={output_tensor.shape}, dtype={output_tensor.dtype}")
    
    def _validate_engine(self, engine_path: str) -> bool:
        """éªŒè¯ç”Ÿæˆçš„TensorRTå¼•æ“"""
        try:
            # åŠ è½½å¼•æ“
            with open(engine_path, 'rb') as f:
                engine_data = f.read()
            
            runtime = trt.Runtime(self.logger)
            engine = runtime.deserialize_cuda_engine(engine_data)
            
            if engine is None:
                logger.error("æ— æ³•ååºåˆ—åŒ–å¼•æ“")
                return False
            
            # æ‰“å°å¼•æ“ä¿¡æ¯
            logger.info("ğŸ” å¼•æ“ä¿¡æ¯:")
            logger.info(f"  - æœ€å¤§æ‰¹æ¬¡å¤§å°: {engine.max_batch_size}")
            logger.info(f"  - ç»‘å®šæ•°é‡: {engine.num_bindings}")
            logger.info(f"  - å±‚æ•°é‡: {engine.num_layers}")
            logger.info(f"  - è®¾å¤‡å†…å­˜å¤§å°: {engine.device_memory_size / (1024*1024):.2f} MB")
            
            # æ£€æŸ¥ç»‘å®š
            for i in range(engine.num_bindings):
                binding_name = engine.get_binding_name(i)
                binding_shape = engine.get_binding_shape(i)
                binding_dtype = engine.get_binding_dtype(i)
                is_input = engine.binding_is_input(i)
                logger.info(f"  - ç»‘å®š{i}: {binding_name} ({'è¾“å…¥' if is_input else 'è¾“å‡º'}), shape={binding_shape}, dtype={binding_dtype}")
            
            return True
            
        except Exception as e:
            logger.error(f"å¼•æ“éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _setup_optimization_profiles(self, builder, config, network):
        """è®¾ç½®ä¼˜åŒ–é…ç½®æ–‡ä»¶"""
        profile = builder.create_optimization_profile()
        
        # ä¸ºæ¯ä¸ªè¾“å…¥è®¾ç½®åŠ¨æ€shape
        for i in range(network.num_inputs):
            input_tensor = network.get_input(i)
            input_name = input_tensor.name
            
            # è·å–ç”¨æˆ·é…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼
            min_shape = self.config.min_shapes.get(input_name)
            opt_shape = self.config.opt_shapes.get(input_name)
            max_shape = self.config.max_shapes.get(input_name)
            
            # å¦‚æœæ²¡æœ‰ç”¨æˆ·é…ç½®ï¼Œæ ¹æ®è¾“å…¥åç§°æ¨æ–­
            if min_shape is None or opt_shape is None or max_shape is None:
                min_shape, opt_shape, max_shape = self._infer_shapes_for_input(input_name, input_tensor.shape)
            
            profile.set_shape(input_name, min_shape, opt_shape, max_shape)
            logger.info(f"è¾“å…¥{input_name}: min={min_shape}, opt={opt_shape}, max={max_shape}")
        
        config.add_optimization_profile(profile)
    
    def _infer_shapes_for_input(self, input_name: str, tensor_shape) -> Tuple[Tuple, Tuple, Tuple]:
        """æ ¹æ®è¾“å…¥åç§°æ¨æ–­shapeé…ç½®"""
        
        if 'input_ids' in input_name.lower() or 'token' in input_name.lower():
            # åºåˆ—è¾“å…¥
            return (1, 8), (4, 64), (self.config.max_batch_size, 2048)
        elif 'dense_features' in input_name.lower():
            # å¯†é›†ç‰¹å¾
            feature_dim = tensor_shape[-1] if len(tensor_shape) > 1 and tensor_shape[-1] > 0 else 1024
            return (1, feature_dim), (4, feature_dim), (self.config.max_batch_size, feature_dim)
        elif 'attention_mask' in input_name.lower() or 'mask' in input_name.lower():
            # æ³¨æ„åŠ›æ©ç 
            return (1, 8), (4, 64), (self.config.max_batch_size, 2048)
        else:
            # é€šç”¨é…ç½®
            if len(tensor_shape) == 2:
                dim = tensor_shape[1] if tensor_shape[1] > 0 else 512
                return (1, dim), (4, dim), (self.config.max_batch_size, dim)
            else:
                return (1,), (4,), (self.config.max_batch_size,)
    
    def _build_engine_from_hstu_model(self):
        """ä»HSTUæ¨¡å‹æ„å»ºTensorRTå¼•æ“"""
        try:
            if self.hstu_model is None:
                raise RuntimeError("HSTUæ¨¡å‹ä¸å¯ç”¨")
            
            # é¦–å…ˆå°†HSTUæ¨¡å‹å¯¼å‡ºä¸ºONNX
            logger.info("å°†HSTUæ¨¡å‹å¯¼å‡ºä¸ºONNX...")
            onnx_path = self._export_hstu_to_onnx()
            
            if onnx_path:
                # ä»ONNXæ„å»ºTensorRTå¼•æ“
                engine_path = self._build_engine_from_onnx(onnx_path)
                if engine_path:
                    self._load_engine(engine_path)
                else:
                    raise RuntimeError("ä»HSTUæ¨¡å‹æ„å»ºTensorRTå¼•æ“å¤±è´¥")
            else:
                raise RuntimeError("HSTUæ¨¡å‹å¯¼å‡ºONNXå¤±è´¥")
                
        except Exception as e:
            logger.error(f"ä»HSTUæ¨¡å‹æ„å»ºå¼•æ“å¤±è´¥: {e}")
            self.tensorrt_available = False
    
    def _export_hstu_to_onnx(self) -> Optional[str]:
        """å°†HSTUæ¨¡å‹å¯¼å‡ºä¸ºONNXï¼ˆä½¿ç”¨ä¸“ç”¨å¯¼å‡ºå™¨ï¼‰"""
        try:
            # ä½¿ç”¨ä¸“ç”¨çš„ONNXå¯¼å‡ºå™¨
            from ..hstu.onnx_exporter import export_hstu_model
            
            # è·å–æ¨¡å‹é…ç½®
            model_config = self.hstu_model.config if hasattr(self.hstu_model, 'config') else None
            
            if model_config is None:
                logger.warning("æ— æ³•è·å–æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨ç®€å•å¯¼å‡ºæ–¹æ³•")
                return self._simple_onnx_export()
            
            # ä½¿ç”¨ä¸“ä¸šå¯¼å‡ºå™¨å¯¼å‡º
            export_result = export_hstu_model(
                model=self.hstu_model,
                model_config=model_config,
                export_dir=os.path.join(os.path.dirname(self.config.engine_path or './'), 'onnx_models'),
                batch_sizes=[1, 2, 4, 8],
                sequence_lengths=[64, 128, 256, 512],
                export_inference_only=True,
                optimize=True
            )
            
            if export_result['success']:
                # ä¼˜å…ˆä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å‹
                if 'optimized_model' in export_result['export_paths']:
                    return export_result['export_paths']['optimized_model']
                elif 'onnx_model' in export_result['export_paths']:
                    return export_result['export_paths']['onnx_model']
                else:
                    logger.warning("ä¸“ä¸šå¯¼å‡ºå™¨æœªè¿”å›å¯ç”¨æ¨¡å‹ï¼Œä½¿ç”¨ç®€å•å¯¼å‡º")
                    return self._simple_onnx_export()
            else:
                logger.warning(f"ä¸“ä¸šå¯¼å‡ºå¤±è´¥: {export_result.get('error')}ï¼Œä½¿ç”¨ç®€å•å¯¼å‡º")
                return self._simple_onnx_export()
            
        except Exception as e:
            logger.error(f"ä¸“ä¸šONNXå¯¼å‡ºå¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•å¯¼å‡º")
            return self._simple_onnx_export()
    
    def _simple_onnx_export(self) -> Optional[str]:
        """ç®€å•çš„ONNXå¯¼å‡ºæ–¹æ³•ï¼ˆå›é€€é€‰é¡¹ï¼‰"""
        try:
            onnx_path = f"{self.config.model_name}_hstu_simple.onnx"
            
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            batch_size = 1
            seq_len = 128
            
            dummy_input_ids = torch.randint(0, 50000, (batch_size, seq_len), dtype=torch.long)
            dummy_attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
            dummy_dense_features = torch.randn(batch_size, 1024, dtype=torch.float32)
            dummy_position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)
            
            # å¯¼å‡ºä¸ºONNX
            torch.onnx.export(
                self.hstu_model,
                {
                    'input_ids': dummy_input_ids,
                    'attention_mask': dummy_attention_mask,
                    'dense_features': dummy_dense_features,
                    'position_ids': dummy_position_ids,
                },
                onnx_path,
                export_params=True,
                opset_version=17,  # ä½¿ç”¨æ›´æ–°çš„opsetç‰ˆæœ¬
                do_constant_folding=True,
                input_names=['input_ids', 'attention_mask', 'dense_features', 'position_ids'],
                output_names=['logits', 'hidden_states', 'engagement_scores', 'retention_scores', 'monetization_scores'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence'},
                    'attention_mask': {0: 'batch_size', 1: 'sequence'},
                    'dense_features': {0: 'batch_size'},
                    'position_ids': {0: 'batch_size', 1: 'sequence'},
                    'logits': {0: 'batch_size', 1: 'sequence'},
                    'hidden_states': {0: 'batch_size', 1: 'sequence'},
                    'engagement_scores': {0: 'batch_size'},
                    'retention_scores': {0: 'batch_size'},
                    'monetization_scores': {0: 'batch_size'},
                },
                verbose=False
            )
            
            logger.info(f"ç®€å•ONNXå¯¼å‡ºå®Œæˆ: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            logger.error(f"ç®€å•ONNXå¯¼å‡ºä¹Ÿå¤±è´¥: {e}")
            return None
    
    def _setup_bindings(self):
        """è®¾ç½®è¾“å…¥è¾“å‡ºç»‘å®š"""
        self.bindings = [None] * self.engine.num_bindings
        
        for i in range(self.engine.num_bindings):
            binding_name = self.engine.get_binding_name(i)
            binding_shape = self.engine.get_binding_shape(i)
            binding_dtype = trt.nptype(self.engine.get_binding_dtype(i))
            
            if self.engine.binding_is_input(i):
                self.input_bindings[binding_name] = i
                logger.info(f"è¾“å…¥ç»‘å®š {binding_name}: shape={binding_shape}, dtype={binding_dtype}")
            else:
                self.output_bindings[binding_name] = i
                logger.info(f"è¾“å‡ºç»‘å®š {binding_name}: shape={binding_shape}, dtype={binding_dtype}")
    
    def infer(
        self,
        inputs: Dict[str, torch.Tensor],
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """æ‰§è¡ŒTensorRTæ¨ç†"""
        
        if not self.tensorrt_available:
            return self._fallback_infer(inputs, **kwargs)
        
        try:
            # è®¾ç½®åŠ¨æ€è¾“å…¥å½¢çŠ¶
            for input_name, input_tensor in inputs.items():
                if input_name in self.input_bindings:
                    binding_idx = self.input_bindings[input_name]
                    self.context.set_binding_shape(binding_idx, input_tensor.shape)
            
            # åˆ†é…ç¼“å†²åŒº
            self._allocate_buffers(inputs)
            
            # å¤åˆ¶è¾“å…¥æ•°æ®åˆ°GPU
            for input_name, input_tensor in inputs.items():
                if input_name in self.input_bindings:
                    input_data = input_tensor.detach().cpu().numpy()
                    cuda.memcpy_htod_async(
                        self.device_inputs[input_name],
                        input_data.ravel(),
                        self.stream
                    )
            
            # æ‰§è¡Œæ¨ç†
            success = self.context.execute_async_v2(
                bindings=self.bindings,
                stream_handle=self.stream.handle
            )
            
            if not success:
                raise RuntimeError("TensorRTæ¨ç†æ‰§è¡Œå¤±è´¥")
            
            # å¤åˆ¶è¾“å‡ºæ•°æ®åˆ°ä¸»æœº
            results = {}
            for output_name, binding_idx in self.output_bindings.items():
                output_shape = self.context.get_binding_shape(binding_idx)
                
                cuda.memcpy_dtoh_async(
                    self.host_outputs[output_name],
                    self.device_outputs[output_name],
                    self.stream
                )
                
                # ç­‰å¾…å®Œæˆå¹¶è½¬æ¢ä¸ºtorch tensor
                self.stream.synchronize()
                output_array = np.array(self.host_outputs[output_name]).reshape(output_shape)
                results[output_name] = torch.from_numpy(output_array.copy())
            
            return results
            
        except Exception as e:
            logger.error(f"TensorRTæ¨ç†å¤±è´¥: {e}")
            return self._fallback_infer(inputs, **kwargs)
    
    def _allocate_buffers(self, inputs: Dict[str, torch.Tensor]):
        """åˆ†é…ç¼“å†²åŒº"""
        # ä¸ºè¾“å…¥åˆ†é…ç¼“å†²åŒº
        for input_name, input_tensor in inputs.items():
            if input_name in self.input_bindings:
                binding_idx = self.input_bindings[input_name]
                
                # åˆ†é…ä¸»æœºå†…å­˜
                input_size = input_tensor.numel()
                input_dtype = trt.nptype(self.engine.get_binding_dtype(binding_idx))
                
                if input_name not in self.host_inputs:
                    self.host_inputs[input_name] = cuda.pagelocked_empty(input_size, input_dtype)
                    self.device_inputs[input_name] = cuda.mem_alloc(input_tensor.nbytes)
                    
                self.bindings[binding_idx] = int(self.device_inputs[input_name])
        
        # ä¸ºè¾“å‡ºåˆ†é…ç¼“å†²åŒº
        for output_name, binding_idx in self.output_bindings.items():
            output_shape = self.context.get_binding_shape(binding_idx)
            output_size = int(np.prod(output_shape))
            output_dtype = trt.nptype(self.engine.get_binding_dtype(binding_idx))
            
            if output_name not in self.host_outputs:
                self.host_outputs[output_name] = cuda.pagelocked_empty(output_size, output_dtype)
                output_nbytes = output_size * np.dtype(output_dtype).itemsize
                self.device_outputs[output_name] = cuda.mem_alloc(output_nbytes)
                
            self.bindings[binding_idx] = int(self.device_outputs[output_name])
    
    def _fallback_infer(
        self,
        inputs: Dict[str, torch.Tensor],
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """å›é€€åˆ°HSTUæ¨¡å‹æ¨ç†"""
        
        if self.hstu_model is not None:
            try:
                # ä½¿ç”¨HSTUæ¨¡å‹è¿›è¡Œæ¨ç†
                results = self.hstu_model.forward(**inputs)
                return results
                
            except Exception as e:
                logger.error(f"HSTUæ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # æœ€ç»ˆå›é€€åˆ°æ¨¡æ‹Ÿç»“æœ
        batch_size = next(iter(inputs.values())).shape[0]
        return {
            'logits': torch.randn(batch_size, 100, 50000),
            'hidden_states': torch.randn(batch_size, 100, 1024),
            'engagement_scores': torch.sigmoid(torch.randn(batch_size, 1)),
            'retention_scores': torch.sigmoid(torch.randn(batch_size, 1)),
            'monetization_scores': torch.sigmoid(torch.randn(batch_size, 1)),
        }
    
    def benchmark_performance(
        self,
        batch_sizes: List[int] = [1, 2, 4, 8],
        sequence_lengths: List[int] = [64, 128, 256, 512],
        num_iterations: int = 100
    ) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        
        if not self.tensorrt_available:
            return {'tensorrt_available': False}
        
        results = {
            'tensorrt_available': True,
            'benchmark_results': {}
        }
        
        for batch_size in batch_sizes:
            for seq_len in sequence_lengths:
                test_key = f"batch_{batch_size}_seq_{seq_len}"
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                inputs = {
                    'input_ids': torch.randint(0, 50000, (batch_size, seq_len), dtype=torch.long),
                    'attention_mask': torch.ones(batch_size, seq_len, dtype=torch.long),
                    'dense_features': torch.randn(batch_size, 1024, dtype=torch.float32),
                }
                
                # é¢„çƒ­
                for _ in range(10):
                    _ = self.infer(inputs)
                
                # åŸºå‡†æµ‹è¯•
                import time
                times = []
                for _ in range(num_iterations):
                    start_time = time.time()
                    _ = self.infer(inputs)
                    end_time = time.time()
                    times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                
                # è®¡ç®—ç»Ÿè®¡
                avg_time = np.mean(times)
                min_time = np.min(times)
                max_time = np.max(times)
                std_time = np.std(times)
                throughput = batch_size / (avg_time / 1000)  # samples/second
                
                results['benchmark_results'][test_key] = {
                    'batch_size': batch_size,
                    'sequence_length': seq_len,
                    'avg_latency_ms': avg_time,
                    'min_latency_ms': min_time,
                    'max_latency_ms': max_time,
                    'std_latency_ms': std_time,
                    'throughput_samples_per_sec': throughput,
                }
        
        return results
    
    def get_engine_info(self) -> Dict[str, Any]:
        """è·å–å¼•æ“ä¿¡æ¯"""
        
        if not self.tensorrt_available:
            return {'tensorrt_available': False}
        
        try:
            info = {
                'tensorrt_available': True,
                'engine_info': {
                    'max_batch_size': self.engine.max_batch_size,
                    'num_bindings': self.engine.num_bindings,
                    'num_layers': self.engine.num_layers,
                    'device_memory_size': self.engine.device_memory_size,
                    'workspace_size': self.engine.workspace_size,
                },
                'input_bindings': self.input_bindings,
                'output_bindings': self.output_bindings,
                'config': {
                    'precision': self.config.precision,
                    'max_workspace_size': self.config.max_workspace_size,
                    'optimization_level': self.config.optimization_level,
                }
            }
            
            return info
            
        except Exception as e:
            logger.error(f"è·å–å¼•æ“ä¿¡æ¯å¤±è´¥: {e}")
            return {'tensorrt_available': False, 'error': str(e)}
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†CUDAå†…å­˜
            for device_buffer in self.device_inputs.values():
                cuda.mem_free(device_buffer)
            for device_buffer in self.device_outputs.values():
                cuda.mem_free(device_buffer)
            
            # æ¸…ç†TensorRTå¯¹è±¡
            if self.context:
                del self.context
            if self.engine:
                del self.engine
            if self.runtime:
                del self.runtime
                
        except Exception as e:
            logger.warning(f"æ¸…ç†TensorRTèµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")


def create_tensorrt_engine(
    onnx_path: Optional[str] = None,
    engine_path: Optional[str] = None,
    precision: str = "fp16",
    max_batch_size: int = 8,
    max_workspace_size: int = 1 << 30,
    hstu_model=None,
    **kwargs
) -> TensorRTOptimizedEngine:
    """åˆ›å»ºTensorRTä¼˜åŒ–å¼•æ“"""
    
    config = TensorRTConfig(
        onnx_path=onnx_path,
        engine_path=engine_path,
        precision=precision,
        max_batch_size=max_batch_size,
        max_workspace_size=max_workspace_size,
        **kwargs
    )
    
    engine = TensorRTOptimizedEngine(config, hstu_model)
    logger.info(f"âœ… TensorRTå¼•æ“åˆ›å»ºæˆåŠŸï¼Œå¯ç”¨æ€§: {engine.tensorrt_available}")
    
    return engine


if __name__ == "__main__":
    # æµ‹è¯•TensorRTå¼•æ“
    config = TensorRTConfig(
        model_name="test-tensorrt",
        precision="fp16",
        max_batch_size=4,
        max_workspace_size=1 << 30,
    )
    
    engine = TensorRTOptimizedEngine(config)
    
    # æµ‹è¯•æ¨ç†
    if engine.tensorrt_available:
        inputs = {
            'input_ids': torch.randint(0, 50000, (2, 64), dtype=torch.long),
            'attention_mask': torch.ones(2, 64, dtype=torch.long),
            'dense_features': torch.randn(2, 1024, dtype=torch.float32),
        }
        
        results = engine.infer(inputs)
        print("æ¨ç†ç»“æœå½¢çŠ¶:")
        for name, tensor in results.items():
            print(f"  {name}: {tensor.shape}")
        
        # æ€§èƒ½æµ‹è¯•
        benchmark = engine.benchmark_performance(
            batch_sizes=[1, 2],
            sequence_lengths=[64, 128],
            num_iterations=10
        )
        
        print("æ€§èƒ½åŸºå‡†æµ‹è¯•:")
        for key, result in benchmark.get('benchmark_results', {}).items():
            print(f"  {key}: {result['avg_latency_ms']:.2f}ms, {result['throughput_samples_per_sec']:.2f} samples/sec")
    else:
        print("TensorRTä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
    
    # å¼•æ“ä¿¡æ¯
    info = engine.get_engine_info()
    print(f"å¼•æ“ä¿¡æ¯: {info}")