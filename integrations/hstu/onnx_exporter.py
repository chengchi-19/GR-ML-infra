#!/usr/bin/env python3
"""
HSTUæ¨¡å‹ONNXå¯¼å‡ºå·¥å…·

æä¾›å®Œæ•´çš„HSTUæ¨¡å‹ONNXå¯¼å‡ºã€éªŒè¯å’Œä¼˜åŒ–åŠŸèƒ½ï¼Œ
æ”¯æŒåŠ¨æ€æ‰¹æ¬¡ã€å¤šè¾“å‡ºå’ŒTensorRTå…¼å®¹æ€§ä¼˜åŒ–ã€‚
"""

import os
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
import torch
import torch.nn as nn

logger = logging.getLogger(__name__)

try:
    import onnx
    import onnxruntime as ort
    import onnxoptimizer
    ONNX_AVAILABLE = True
    logger.info("âœ… ONNXç›¸å…³åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    ONNX_AVAILABLE = False
    logger.warning(f"âš ï¸ ONNXç›¸å…³åº“å¯¼å…¥å¤±è´¥: {e}")


class HSTUOnnxExporter:
    """
    HSTUæ¨¡å‹ONNXå¯¼å‡ºå™¨
    
    æ”¯æŒå®Œæ•´çš„å¯¼å‡ºã€éªŒè¯ã€ä¼˜åŒ–æµç¨‹ï¼Œç¡®ä¿TensorRTå…¼å®¹æ€§
    """
    
    def __init__(self, 
                 model,
                 model_config,
                 export_dir: str = "./models",
                 opset_version: int = 17):
        """
        åˆå§‹åŒ–ONNXå¯¼å‡ºå™¨
        
        Args:
            model: HSTUæ¨¡å‹å®ä¾‹
            model_config: æ¨¡å‹é…ç½®å¯¹è±¡
            export_dir: å¯¼å‡ºç›®å½•
            opset_version: ONNXæ“ä½œé›†ç‰ˆæœ¬
        """
        self.model = model
        self.config = model_config
        self.export_dir = export_dir
        self.opset_version = opset_version
        
        # åˆ›å»ºå¯¼å‡ºç›®å½•
        os.makedirs(export_dir, exist_ok=True)
        
        # æ£€æŸ¥ONNXå¯ç”¨æ€§
        if not ONNX_AVAILABLE:
            raise RuntimeError("ONNXç›¸å…³åº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install onnx onnxruntime onnxoptimizer")
        
        logger.info("âœ… HSTU ONNXå¯¼å‡ºå™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def export_full_model(self,
                         batch_sizes: List[int] = [1, 4, 8],
                         sequence_lengths: List[int] = [64, 128, 256, 512],
                         verify_export: bool = True,
                         optimize_model: bool = True) -> Dict[str, str]:
        """
        å¯¼å‡ºå®Œæ•´çš„HSTUæ¨¡å‹åˆ°ONNX
        
        Args:
            batch_sizes: æ”¯æŒçš„æ‰¹æ¬¡å¤§å°åˆ—è¡¨
            sequence_lengths: æ”¯æŒçš„åºåˆ—é•¿åº¦åˆ—è¡¨  
            verify_export: æ˜¯å¦éªŒè¯å¯¼å‡ºæ¨¡å‹
            optimize_model: æ˜¯å¦ä¼˜åŒ–æ¨¡å‹
            
        Returns:
            åŒ…å«å¯¼å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        
        logger.info("å¼€å§‹å¯¼å‡ºHSTUæ¨¡å‹åˆ°ONNXæ ¼å¼...")
        
        # å‡†å¤‡å¯¼å‡ºè·¯å¾„
        base_name = f"hstu_model_opset{self.opset_version}"
        onnx_path = os.path.join(self.export_dir, f"{base_name}.onnx")
        optimized_path = os.path.join(self.export_dir, f"{base_name}_optimized.onnx")
        
        try:
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # åˆ›å»ºåŠ¨æ€è¾“å…¥ç¤ºä¾‹
            dummy_inputs = self._create_dummy_inputs(
                batch_size=max(batch_sizes),
                seq_len=max(sequence_lengths)
            )
            
            # é…ç½®åŠ¨æ€è½´
            dynamic_axes = self._get_dynamic_axes_config(batch_sizes, sequence_lengths)
            
            # å¯¼å‡ºåˆ°ONNX
            with torch.no_grad():
                torch.onnx.export(
                    model=self.model,
                    args=dummy_inputs,
                    f=onnx_path,
                    input_names=list(dummy_inputs.keys()),
                    output_names=['logits', 'hidden_states', 'engagement_scores', 
                                'retention_scores', 'monetization_scores'],
                    dynamic_axes=dynamic_axes,
                    opset_version=self.opset_version,
                    do_constant_folding=True,
                    export_params=True,
                    keep_initializers_as_inputs=False,
                    verbose=False
                )
            
            logger.info(f"âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {onnx_path}")
            
            result_paths = {"onnx_model": onnx_path}
            
            # éªŒè¯å¯¼å‡ºæ¨¡å‹
            if verify_export:
                logger.info("éªŒè¯å¯¼å‡ºçš„ONNXæ¨¡å‹...")
                is_valid = self._verify_onnx_model(onnx_path, dummy_inputs)
                if not is_valid:
                    logger.error("âŒ ONNXæ¨¡å‹éªŒè¯å¤±è´¥")
                    return result_paths
                logger.info("âœ… ONNXæ¨¡å‹éªŒè¯æˆåŠŸ")
            
            # ä¼˜åŒ–æ¨¡å‹
            if optimize_model:
                logger.info("ä¼˜åŒ–ONNXæ¨¡å‹...")
                success = self._optimize_onnx_model(onnx_path, optimized_path)
                if success:
                    result_paths["optimized_model"] = optimized_path
                    logger.info(f"âœ… ONNXæ¨¡å‹ä¼˜åŒ–æˆåŠŸ: {optimized_path}")
                else:
                    logger.warning("âš ï¸ ONNXæ¨¡å‹ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")
            
            # ç”Ÿæˆæ¨¡å‹ä¿¡æ¯
            self._generate_model_info(result_paths, batch_sizes, sequence_lengths)
            
            return result_paths
            
        except Exception as e:
            logger.error(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def export_inference_only(self,
                            batch_size: int = 1,
                            sequence_length: int = 128,
                            include_past_key_values: bool = False) -> str:
        """
        å¯¼å‡ºä»…æ¨ç†ç‰ˆæœ¬çš„æ¨¡å‹ï¼ˆå»é™¤è®­ç»ƒç›¸å…³ç»„ä»¶ï¼‰
        
        Args:
            batch_size: å›ºå®šæ‰¹æ¬¡å¤§å°
            sequence_length: å›ºå®šåºåˆ—é•¿åº¦
            include_past_key_values: æ˜¯å¦åŒ…å«KVç¼“å­˜
            
        Returns:
            å¯¼å‡ºçš„ONNXæ–‡ä»¶è·¯å¾„
        """
        
        logger.info("å¯¼å‡ºæ¨ç†ä¸“ç”¨ONNXæ¨¡å‹...")
        
        # åˆ›å»ºæ¨ç†ä¸“ç”¨æ¨¡å‹åŒ…è£…å™¨
        inference_model = self._create_inference_wrapper(include_past_key_values)
        
        # å‡†å¤‡å›ºå®šè¾“å…¥
        dummy_inputs = self._create_dummy_inputs(batch_size, sequence_length)
        if not include_past_key_values:
            # ç§»é™¤ä¸å¿…è¦çš„è¾“å…¥
            dummy_inputs = {k: v for k, v in dummy_inputs.items() 
                          if 'past_key_values' not in k}
        
        # å¯¼å‡ºè·¯å¾„
        onnx_path = os.path.join(
            self.export_dir, 
            f"hstu_inference_b{batch_size}_s{sequence_length}.onnx"
        )
        
        try:
            with torch.no_grad():
                torch.onnx.export(
                    model=inference_model,
                    args=tuple(dummy_inputs.values()),
                    f=onnx_path,
                    input_names=list(dummy_inputs.keys()),
                    output_names=['logits', 'engagement_scores', 'retention_scores', 'monetization_scores'],
                    opset_version=self.opset_version,
                    do_constant_folding=True,
                    export_params=True,
                    keep_initializers_as_inputs=False,
                    verbose=False
                )
            
            logger.info(f"âœ… æ¨ç†ä¸“ç”¨ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            logger.error(f"âŒ æ¨ç†ä¸“ç”¨ONNXå¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def _create_dummy_inputs(self, batch_size: int, seq_len: int) -> Dict[str, torch.Tensor]:
        """åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®"""
        
        dummy_inputs = {
            'input_ids': torch.randint(
                0, self.config.vocab_size, 
                (batch_size, seq_len), 
                dtype=torch.long
            ),
            'attention_mask': torch.ones(
                batch_size, seq_len, 
                dtype=torch.long
            ),
        }
        
        # æ·»åŠ å¯é€‰è¾“å…¥
        if hasattr(self.config, 'dense_feature_dim'):
            dummy_inputs['dense_features'] = torch.randn(
                batch_size, self.config.dense_feature_dim,
                dtype=torch.float32
            )
        else:
            dummy_inputs['dense_features'] = torch.randn(
                batch_size, 1024,
                dtype=torch.float32  
            )
        
        # æ·»åŠ ä½ç½®IDï¼ˆå¯é€‰ï¼‰
        dummy_inputs['position_ids'] = torch.arange(
            seq_len, dtype=torch.long
        ).unsqueeze(0).expand(batch_size, -1)
        
        return dummy_inputs
    
    def _get_dynamic_axes_config(self, batch_sizes: List[int], seq_lens: List[int]) -> Dict[str, Dict[int, str]]:
        """è·å–åŠ¨æ€è½´é…ç½®"""
        
        dynamic_axes = {
            # è¾“å…¥åŠ¨æ€è½´
            'input_ids': {0: 'batch_size', 1: 'sequence'},
            'attention_mask': {0: 'batch_size', 1: 'sequence'},
            'dense_features': {0: 'batch_size'},
            'position_ids': {0: 'batch_size', 1: 'sequence'},
            
            # è¾“å‡ºåŠ¨æ€è½´
            'logits': {0: 'batch_size', 1: 'sequence'},
            'hidden_states': {0: 'batch_size', 1: 'sequence'},
            'engagement_scores': {0: 'batch_size'},
            'retention_scores': {0: 'batch_size'},
            'monetization_scores': {0: 'batch_size'},
        }
        
        return dynamic_axes
    
    def _create_inference_wrapper(self, include_kv_cache: bool = False):
        """åˆ›å»ºæ¨ç†ä¸“ç”¨æ¨¡å‹åŒ…è£…å™¨"""
        
        class InferenceWrapper(nn.Module):
            def __init__(self, model):
                super().__init__()
                self.model = model
                
            def forward(self, input_ids, attention_mask, dense_features, position_ids):
                outputs = self.model.forward(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    dense_features=dense_features,
                    position_ids=position_ids,
                    return_dict=True
                )
                
                # åªè¿”å›æ¨ç†éœ€è¦çš„è¾“å‡º
                return (
                    outputs['logits'],
                    outputs['engagement_scores'],
                    outputs['retention_scores'], 
                    outputs['monetization_scores']
                )
        
        return InferenceWrapper(self.model)
    
    def _verify_onnx_model(self, onnx_path: str, dummy_inputs: Dict[str, torch.Tensor]) -> bool:
        """éªŒè¯å¯¼å‡ºçš„ONNXæ¨¡å‹"""
        
        try:
            # åŠ è½½ONNXæ¨¡å‹
            onnx_model = onnx.load(onnx_path)
            
            # æ£€æŸ¥æ¨¡å‹ç»“æ„
            onnx.checker.check_model(onnx_model)
            logger.info("âœ… ONNXæ¨¡å‹ç»“æ„æ£€æŸ¥é€šè¿‡")
            
            # åˆ›å»ºæ¨ç†ä¼šè¯
            ort_session = ort.InferenceSession(
                onnx_path,
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
            )
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            ort_inputs = {}
            for name, tensor in dummy_inputs.items():
                ort_inputs[name] = tensor.detach().cpu().numpy()
            
            # æ‰§è¡Œæ¨ç†
            ort_outputs = ort_session.run(None, ort_inputs)
            
            # ä¸PyTorchæ¨¡å‹å¯¹æ¯”
            with torch.no_grad():
                torch_outputs = self.model(**dummy_inputs)
            
            # æ£€æŸ¥è¾“å‡ºä¸€è‡´æ€§
            torch_logits = torch_outputs['logits'].detach().cpu().numpy()
            onnx_logits = ort_outputs[0]
            
            if np.allclose(torch_logits, onnx_logits, rtol=1e-3, atol=1e-3):
                logger.info("âœ… PyTorchå’ŒONNXè¾“å‡ºä¸€è‡´æ€§éªŒè¯é€šè¿‡")
                return True
            else:
                logger.error("âŒ PyTorchå’ŒONNXè¾“å‡ºä¸ä¸€è‡´")
                logger.error(f"æœ€å¤§å·®å¼‚: {np.max(np.abs(torch_logits - onnx_logits))}")
                return False
            
        except Exception as e:
            logger.error(f"âŒ ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _optimize_onnx_model(self, input_path: str, output_path: str) -> bool:
        """ä¼˜åŒ–ONNXæ¨¡å‹"""
        
        try:
            # åŠ è½½æ¨¡å‹
            model = onnx.load(input_path)
            
            # åº”ç”¨ä¼˜åŒ–
            optimized_model = onnxoptimizer.optimize(
                model,
                passes=[
                    'eliminate_deadend',
                    'eliminate_identity',
                    'eliminate_nop_dropout',
                    'eliminate_nop_monotone_argmax',
                    'eliminate_nop_pad',
                    'extract_constant_to_initializer',
                    'eliminate_unused_initializer',
                    'eliminate_nop_transpose',
                    'fuse_add_bias_into_conv',
                    'fuse_bn_into_conv',
                    'fuse_consecutive_squeezes',
                    'fuse_consecutive_transposes',
                    'fuse_matmul_add_bias_into_gemm',
                    'fuse_pad_into_conv',
                    'fuse_transpose_into_gemm',
                ]
            )
            
            # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
            onnx.save(optimized_model, output_path)
            
            # éªŒè¯ä¼˜åŒ–åçš„æ¨¡å‹
            onnx.checker.check_model(optimized_model)
            
            logger.info("âœ… ONNXæ¨¡å‹ä¼˜åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNXæ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            return False
    
    def _generate_model_info(self, 
                           result_paths: Dict[str, str], 
                           batch_sizes: List[int], 
                           seq_lens: List[int]):
        """ç”Ÿæˆæ¨¡å‹ä¿¡æ¯æ–‡ä»¶"""
        
        info = {
            'model_info': {
                'model_type': 'HSTU_Generative_Recommender',
                'opset_version': self.opset_version,
                'vocab_size': self.config.vocab_size,
                'd_model': self.config.d_model,
                'num_layers': self.config.num_layers,
                'num_heads': self.config.num_heads,
                'max_seq_len': self.config.max_seq_len,
            },
            'export_config': {
                'supported_batch_sizes': batch_sizes,
                'supported_sequence_lengths': seq_lens,
                'dynamic_axes_enabled': True,
            },
            'files': result_paths,
            'usage': {
                'tensorrt_build': f"trtexec --onnx={result_paths.get('optimized_model', result_paths['onnx_model'])} --saveEngine=hstu.trt --fp16",
                'onnxruntime_inference': "ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider'])"
            }
        }
        
        # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
        info_path = os.path.join(self.export_dir, "model_info.json")
        import json
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æ¨¡å‹ä¿¡æ¯ä¿å­˜åˆ°: {info_path}")


def export_hstu_model(model, 
                     model_config,
                     export_dir: str = "./models",
                     batch_sizes: List[int] = [1, 2, 4, 8],
                     sequence_lengths: List[int] = [64, 128, 256, 512],
                     export_inference_only: bool = True,
                     optimize: bool = True) -> Dict[str, Any]:
    """
    å¯¼å‡ºHSTUæ¨¡å‹åˆ°ONNXæ ¼å¼çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: HSTUæ¨¡å‹å®ä¾‹
        model_config: æ¨¡å‹é…ç½®
        export_dir: å¯¼å‡ºç›®å½•
        batch_sizes: æ”¯æŒçš„æ‰¹æ¬¡å¤§å°
        sequence_lengths: æ”¯æŒçš„åºåˆ—é•¿åº¦
        export_inference_only: æ˜¯å¦å¯¼å‡ºæ¨ç†ä¸“ç”¨ç‰ˆæœ¬
        optimize: æ˜¯å¦ä¼˜åŒ–æ¨¡å‹
        
    Returns:
        åŒ…å«å¯¼å‡ºç»“æœçš„å­—å…¸
    """
    
    logger.info("ğŸš€ å¼€å§‹HSTUæ¨¡å‹ONNXå¯¼å‡ºæµç¨‹...")
    
    # åˆ›å»ºå¯¼å‡ºå™¨
    exporter = HSTUOnnxExporter(
        model=model,
        model_config=model_config,
        export_dir=export_dir
    )
    
    results = {}
    
    try:
        # å¯¼å‡ºå®Œæ•´æ¨¡å‹
        logger.info("å¯¼å‡ºå®Œæ•´åŠ¨æ€ONNXæ¨¡å‹...")
        full_model_paths = exporter.export_full_model(
            batch_sizes=batch_sizes,
            sequence_lengths=sequence_lengths,
            verify_export=True,
            optimize_model=optimize
        )
        results.update(full_model_paths)
        
        # å¯¼å‡ºæ¨ç†ä¸“ç”¨æ¨¡å‹
        if export_inference_only:
            logger.info("å¯¼å‡ºæ¨ç†ä¸“ç”¨ONNXæ¨¡å‹...")
            inference_paths = []
            
            # ä¸ºå‡ ä¸ªå¸¸ç”¨é…ç½®å¯¼å‡ºæ¨ç†ä¸“ç”¨æ¨¡å‹
            common_configs = [
                (1, 128),   # å•æ ·æœ¬ä¸­ç­‰é•¿åº¦
                (4, 64),    # å°æ‰¹æ¬¡çŸ­åºåˆ—
                (8, 256),   # å¤§æ‰¹æ¬¡é•¿åºåˆ—
            ]
            
            for batch_size, seq_len in common_configs:
                try:
                    inference_path = exporter.export_inference_only(
                        batch_size=batch_size,
                        sequence_length=seq_len
                    )
                    inference_paths.append(inference_path)
                except Exception as e:
                    logger.warning(f"æ¨ç†ä¸“ç”¨æ¨¡å‹å¯¼å‡ºå¤±è´¥ (b{batch_size}_s{seq_len}): {e}")
            
            results['inference_models'] = inference_paths
        
        logger.info("ğŸ‰ HSTUæ¨¡å‹ONNXå¯¼å‡ºå®Œæˆ!")
        logger.info(f"å¯¼å‡ºæ–‡ä»¶:")
        for key, value in results.items():
            if isinstance(value, list):
                for v in value:
                    logger.info(f"  - {v}")
            else:
                logger.info(f"  - {key}: {value}")
        
        return {
            'success': True,
            'export_paths': results,
            'message': 'HSTUæ¨¡å‹ONNXå¯¼å‡ºæˆåŠŸ'
        }
        
    except Exception as e:
        logger.error(f"âŒ HSTUæ¨¡å‹ONNXå¯¼å‡ºå¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'message': 'HSTUæ¨¡å‹ONNXå¯¼å‡ºå¤±è´¥'
        }


if __name__ == "__main__":
    # æµ‹è¯•ONNXå¯¼å‡ºåŠŸèƒ½
    from .hstu_model import HSTUGenerativeRecommender, HSTUModelConfig
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    config = HSTUModelConfig(
        vocab_size=10000,
        d_model=512,
        num_layers=4,
        num_heads=8,
        max_seq_len=512
    )
    
    model = HSTUGenerativeRecommender(config)
    
    # æ‰§è¡Œå¯¼å‡º
    result = export_hstu_model(
        model=model,
        model_config=config,
        export_dir="./test_models",
        batch_sizes=[1, 2, 4],
        sequence_lengths=[64, 128, 256],
        export_inference_only=True,
        optimize=True
    )
    
    print("å¯¼å‡ºç»“æœ:", result)