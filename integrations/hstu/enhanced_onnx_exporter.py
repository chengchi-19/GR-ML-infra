#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºå·¥å…·

æ”¯æŒTensorRTè‡ªå®šä¹‰æ’ä»¶çš„ONNXå¯¼å‡ºï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹çš„åˆ›å»ºå’Œé›†æˆã€‚
"""

import os
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
import torch
import torch.nn as nn

logger = logging.getLogger(__name__)

try:
    import onnx
    import onnxruntime as ort
    import onnxoptimizer
    from onnx import helper, TensorProto, GraphProto, ModelProto
    ONNX_AVAILABLE = True
    logger.info("âœ… ONNXç›¸å…³åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    ONNX_AVAILABLE = False
    logger.warning(f"âš ï¸ ONNXç›¸å…³åº“å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥TensorRTæ’ä»¶æ”¯æŒ
try:
    from ..tensorrt.plugins.python.tensorrt_plugins import (
        get_plugin_manager,
        create_fused_attention_layernorm_node,
        create_hierarchical_sequence_fusion_node,
        create_interaction_triton_fast_node
    )
    TENSORRT_PLUGINS_AVAILABLE = True
    logger.info("âœ… TensorRTæ’ä»¶æ”¯æŒå¯ç”¨")
except ImportError as e:
    TENSORRT_PLUGINS_AVAILABLE = False
    logger.warning(f"âš ï¸ TensorRTæ’ä»¶æ”¯æŒä¸å¯ç”¨: {e}")


class EnhancedHSTUOnnxExporter:
    """
    å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºå™¨

    æ”¯æŒTensorRTè‡ªå®šä¹‰æ’ä»¶å’Œç®—å­ä¼˜åŒ–
    """

    def __init__(self,
                 model,
                 model_config,
                 export_dir: str = "./models",
                 opset_version: int = 17,
                 enable_custom_ops: bool = True):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆONNXå¯¼å‡ºå™¨

        Args:
            model: HSTUæ¨¡å‹å®ä¾‹
            model_config: æ¨¡å‹é…ç½®å¯¹è±¡
            export_dir: å¯¼å‡ºç›®å½•
            opset_version: ONNXæ“ä½œé›†ç‰ˆæœ¬
            enable_custom_ops: æ˜¯å¦å¯ç”¨è‡ªå®šä¹‰ç®—å­
        """
        self.model = model
        self.config = model_config
        self.export_dir = export_dir
        self.opset_version = opset_version
        self.enable_custom_ops = enable_custom_ops and TENSORRT_PLUGINS_AVAILABLE

        # åˆ›å»ºå¯¼å‡ºç›®å½•
        os.makedirs(export_dir, exist_ok=True)

        # æ£€æŸ¥ONNXå¯ç”¨æ€§
        if not ONNX_AVAILABLE:
            raise RuntimeError("ONNXç›¸å…³åº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install onnx onnxruntime onnxoptimizer")

        # åˆå§‹åŒ–æ’ä»¶ç®¡ç†å™¨
        self.plugin_manager = get_plugin_manager() if TENSORRT_PLUGINS_AVAILABLE else None

        logger.info(f"âœ… å¢å¼ºç‰ˆHSTU ONNXå¯¼å‡ºå™¨åˆå§‹åŒ–æˆåŠŸ (è‡ªå®šä¹‰ç®—å­: {self.enable_custom_ops})")

    def export_with_custom_ops(self,
                              batch_sizes: List[int] = [1, 4, 8],
                              sequence_lengths: List[int] = [64, 128, 256, 512],
                              verify_export: bool = True,
                              optimize_model: bool = True) -> Dict[str, Any]:
        """
        å¯¼å‡ºåŒ…å«è‡ªå®šä¹‰ç®—å­çš„ONNXæ¨¡å‹

        Args:
            batch_sizes: æ”¯æŒçš„æ‰¹æ¬¡å¤§å°åˆ—è¡¨
            sequence_lengths: æ”¯æŒçš„åºåˆ—é•¿åº¦åˆ—è¡¨
            verify_export: æ˜¯å¦éªŒè¯å¯¼å‡ºæ¨¡å‹
            optimize_model: æ˜¯å¦ä¼˜åŒ–æ¨¡å‹

        Returns:
            åŒ…å«å¯¼å‡ºæ–‡ä»¶è·¯å¾„å’Œè‡ªå®šä¹‰èŠ‚ç‚¹ä¿¡æ¯çš„å­—å…¸
        """

        logger.info("å¼€å§‹å¯¼å‡ºåŒ…å«è‡ªå®šä¹‰ç®—å­çš„HSTUæ¨¡å‹...")

        # å‡†å¤‡å¯¼å‡ºè·¯å¾„
        base_name = f"hstu_custom_ops_opset{self.opset_version}"
        onnx_path = os.path.join(self.export_dir, f"{base_name}.onnx")
        custom_ops_path = os.path.join(self.export_dir, f"{base_name}_with_plugins.onnx")

        try:
            # ç¬¬ä¸€æ­¥: å¯¼å‡ºæ ‡å‡†ONNXæ¨¡å‹
            logger.info("å¯¼å‡ºæ ‡å‡†ONNXæ¨¡å‹...")
            standard_export_result = self._export_standard_model(
                onnx_path, batch_sizes, sequence_lengths
            )

            if not standard_export_result:
                raise RuntimeError("æ ‡å‡†ONNXæ¨¡å‹å¯¼å‡ºå¤±è´¥")

            result_paths = {"standard_onnx": onnx_path}

            # ç¬¬äºŒæ­¥: æ’å…¥è‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹
            if self.enable_custom_ops:
                logger.info("æ’å…¥è‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹...")
                custom_ops_result = self._insert_custom_ops_nodes(
                    onnx_path, custom_ops_path
                )

                if custom_ops_result["success"]:
                    result_paths["custom_ops_onnx"] = custom_ops_path
                    result_paths["custom_nodes"] = custom_ops_result["custom_nodes"]
                    logger.info(f"âœ… è‡ªå®šä¹‰ç®—å­ONNXæ¨¡å‹ç”Ÿæˆ: {custom_ops_path}")
                else:
                    logger.warning("âš ï¸ è‡ªå®šä¹‰ç®—å­æ’å…¥å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹")

            # ç¬¬ä¸‰æ­¥: éªŒè¯æ¨¡å‹
            if verify_export:
                logger.info("éªŒè¯å¯¼å‡ºçš„ONNXæ¨¡å‹...")
                dummy_inputs = self._create_dummy_inputs(
                    max(batch_sizes), max(sequence_lengths)
                )

                # éªŒè¯æ ‡å‡†æ¨¡å‹
                if self._verify_onnx_model(onnx_path, dummy_inputs):
                    logger.info("âœ… æ ‡å‡†ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
                else:
                    logger.error("âŒ æ ‡å‡†ONNXæ¨¡å‹éªŒè¯å¤±è´¥")

                # å¦‚æœæœ‰è‡ªå®šä¹‰ç®—å­æ¨¡å‹ï¼ŒéªŒè¯ç»“æ„ï¼ˆä¸æ‰§è¡Œæ¨ç†ï¼‰
                if "custom_ops_onnx" in result_paths:
                    if self._verify_custom_ops_model_structure(custom_ops_path):
                        logger.info("âœ… è‡ªå®šä¹‰ç®—å­ONNXæ¨¡å‹ç»“æ„éªŒè¯é€šè¿‡")
                    else:
                        logger.warning("âš ï¸ è‡ªå®šä¹‰ç®—å­ONNXæ¨¡å‹ç»“æ„éªŒè¯å¤±è´¥")

            # ç¬¬å››æ­¥: ç”ŸæˆTensorRTæ„å»ºé…ç½®
            tensorrt_config = self._generate_tensorrt_build_config(result_paths)
            result_paths["tensorrt_config"] = tensorrt_config

            # ç”Ÿæˆæ¨¡å‹ä¿¡æ¯
            self._generate_enhanced_model_info(result_paths, batch_sizes, sequence_lengths)

            return {
                "success": True,
                "export_paths": result_paths,
                "custom_ops_enabled": self.enable_custom_ops,
                "plugin_info": self._get_plugin_info(),
                "message": "å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºæˆåŠŸ"
            }

        except Exception as e:
            logger.error(f"âŒ å¢å¼ºç‰ˆONNXå¯¼å‡ºå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºå¤±è´¥"
            }

    def _export_standard_model(self, onnx_path: str, batch_sizes: List[int], seq_lens: List[int]) -> bool:
        """å¯¼å‡ºæ ‡å‡†ONNXæ¨¡å‹"""

        try:
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()

            # åˆ›å»ºåŠ¨æ€è¾“å…¥ç¤ºä¾‹
            dummy_inputs = self._create_dummy_inputs(
                batch_size=max(batch_sizes),
                seq_len=max(seq_lens)
            )

            # é…ç½®åŠ¨æ€è½´
            dynamic_axes = self._get_dynamic_axes_config()

            # å¯¼å‡ºåˆ°ONNX
            with torch.no_grad():
                torch.onnx.export(
                    model=self.model,
                    args=dummy_inputs,
                    f=onnx_path,
                    input_names=list(dummy_inputs.keys()),
                    output_names=['logits', 'hidden_states', 'engagement_scores',
                                'retention_scores', 'monetization_scores'],
                    dynamic_axes=dynamic_axes,
                    opset_version=self.opset_version,
                    do_constant_folding=True,
                    export_params=True,
                    keep_initializers_as_inputs=False,
                    verbose=False
                )

            logger.info(f"âœ… æ ‡å‡†ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {onnx_path}")
            return True

        except Exception as e:
            logger.error(f"âŒ æ ‡å‡†ONNXæ¨¡å‹å¯¼å‡ºå¤±è´¥: {e}")
            return False

    def _insert_custom_ops_nodes(self, input_onnx_path: str, output_onnx_path: str) -> Dict[str, Any]:
        """åœ¨ONNXæ¨¡å‹ä¸­æ’å…¥è‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹"""

        try:
            # åŠ è½½åŸå§‹æ¨¡å‹
            original_model = onnx.load(input_onnx_path)
            graph = original_model.graph

            # åˆ†ææ¨¡å‹å›¾ï¼Œè¯†åˆ«å¯ä»¥è¢«è‡ªå®šä¹‰ç®—å­æ›¿æ¢çš„å­å›¾
            replacement_info = self._analyze_graph_for_custom_ops(graph)

            if not replacement_info["replaceable_subgraphs"]:
                logger.info("æœªæ‰¾åˆ°å¯æ›¿æ¢çš„å­å›¾ï¼Œä¿æŒåŸå§‹æ¨¡å‹")
                return {"success": False, "reason": "no_replaceable_subgraphs"}

            # åˆ›å»ºæ–°çš„å›¾
            new_graph = self._create_graph_with_custom_ops(graph, replacement_info)

            # åˆ›å»ºæ–°æ¨¡å‹
            new_model = helper.make_model(
                new_graph,
                producer_name="GR-ML-infra",
                producer_version="1.0",
                opset_imports=[helper.make_opsetid("", self.opset_version)]
            )

            # æ·»åŠ è‡ªå®šä¹‰åŸŸ
            custom_opset = helper.make_opsetid("gr.ml.infra", 1)
            new_model.opset_import.append(custom_opset)

            # ä¿å­˜æ–°æ¨¡å‹
            onnx.save(new_model, output_onnx_path)

            logger.info(f"âœ… è‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹æ’å…¥æˆåŠŸï¼Œæ›¿æ¢äº† {len(replacement_info['replaceable_subgraphs'])} ä¸ªå­å›¾")

            return {
                "success": True,
                "custom_nodes": replacement_info["custom_nodes"],
                "replaced_subgraphs": replacement_info["replaceable_subgraphs"]
            }

        except Exception as e:
            logger.error(f"âŒ è‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹æ’å…¥å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def _analyze_graph_for_custom_ops(self, graph: GraphProto) -> Dict[str, Any]:
        """åˆ†æå›¾ä¸­å¯ä»¥è¢«è‡ªå®šä¹‰ç®—å­æ›¿æ¢çš„å­å›¾"""

        replaceable_subgraphs = []
        custom_nodes = []

        # æŸ¥æ‰¾èåˆæ³¨æ„åŠ›+LayerNormæ¨¡å¼
        attention_patterns = self._find_attention_layernorm_patterns(graph)
        for pattern in attention_patterns:
            custom_node = {
                "type": "FusedAttentionLayerNorm",
                "name": f"FusedAttentionLayerNorm_{len(custom_nodes)}",
                "inputs": pattern["inputs"],
                "outputs": pattern["outputs"],
                "attributes": {
                    "hidden_dim": self.config.d_model,
                    "num_heads": self.config.num_heads,
                    "dropout_rate": getattr(self.config, 'dropout', 0.1),
                    "layer_norm_eps": getattr(self.config, 'layer_norm_eps', 1e-5)
                }
            }
            custom_nodes.append(custom_node)
            replaceable_subgraphs.append(pattern)

        # æŸ¥æ‰¾å±‚æ¬¡åŒ–åºåˆ—èåˆæ¨¡å¼
        hierarchical_patterns = self._find_hierarchical_sequence_patterns(graph)
        for pattern in hierarchical_patterns:
            custom_node = {
                "type": "HierarchicalSequenceFusion",
                "name": f"HierarchicalSequenceFusion_{len(custom_nodes)}",
                "inputs": pattern["inputs"],
                "outputs": pattern["outputs"],
                "attributes": {
                    "hidden_dim": self.config.d_model,
                    "num_levels": 3
                }
            }
            custom_nodes.append(custom_node)
            replaceable_subgraphs.append(pattern)

        # æŸ¥æ‰¾äº¤äº’ç®—å­æ¨¡å¼
        interaction_patterns = self._find_interaction_patterns(graph)
        for pattern in interaction_patterns:
            custom_node = {
                "type": "InteractionTritonFast",
                "name": f"InteractionTritonFast_{len(custom_nodes)}",
                "inputs": pattern["inputs"],
                "outputs": pattern["outputs"],
                "attributes": {
                    "num_features": pattern.get("num_features", 20),
                    "embedding_dim": self.config.d_model
                }
            }
            custom_nodes.append(custom_node)
            replaceable_subgraphs.append(pattern)

        return {
            "replaceable_subgraphs": replaceable_subgraphs,
            "custom_nodes": custom_nodes
        }

    def _find_attention_layernorm_patterns(self, graph: GraphProto) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾æ³¨æ„åŠ›+LayerNormæ¨¡å¼"""
        patterns = []

        # ç®€åŒ–çš„æ¨¡å¼åŒ¹é… - åœ¨å®é™…å®ç°ä¸­éœ€è¦æ›´å¤æ‚çš„å›¾åˆ†æ
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾æ‰¾åˆ°äº†ä¸€äº›æ³¨æ„åŠ›æ¨¡å¼
        for i, node in enumerate(graph.node):
            if "attention" in node.name.lower() or node.op_type == "MultiHeadAttention":
                # æ£€æŸ¥åç»­æ˜¯å¦æœ‰LayerNorm
                following_nodes = graph.node[i+1:i+5]  # æ£€æŸ¥åç»­å‡ ä¸ªèŠ‚ç‚¹
                for follow_node in following_nodes:
                    if follow_node.op_type == "LayerNormalization":
                        pattern = {
                            "start_node": node.name,
                            "end_node": follow_node.name,
                            "inputs": list(node.input),
                            "outputs": list(follow_node.output),
                            "node_range": (i, i + following_nodes.index(follow_node) + 1)
                        }
                        patterns.append(pattern)
                        break

        return patterns

    def _find_hierarchical_sequence_patterns(self, graph: GraphProto) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾å±‚æ¬¡åŒ–åºåˆ—å¤„ç†æ¨¡å¼"""
        patterns = []

        # æŸ¥æ‰¾å¤šå±‚åºåˆ—å¤„ç†æ¨¡å¼
        for i, node in enumerate(graph.node):
            if "sequence" in node.name.lower() and "fusion" in node.name.lower():
                pattern = {
                    "start_node": node.name,
                    "end_node": node.name,
                    "inputs": list(node.input),
                    "outputs": list(node.output),
                    "node_range": (i, i + 1)
                }
                patterns.append(pattern)

        return patterns

    def _find_interaction_patterns(self, graph: GraphProto) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ç‰¹å¾äº¤äº’æ¨¡å¼"""
        patterns = []

        # æŸ¥æ‰¾çŸ©é˜µä¹˜æ³•+ç‚¹ç§¯æ¨¡å¼
        for i, node in enumerate(graph.node):
            if node.op_type == "MatMul" and "interaction" in node.name.lower():
                pattern = {
                    "start_node": node.name,
                    "end_node": node.name,
                    "inputs": list(node.input),
                    "outputs": list(node.output),
                    "node_range": (i, i + 1),
                    "num_features": 20  # é»˜è®¤å€¼ï¼Œå®é™…éœ€è¦ä»å›¾ä¸­æ¨æ–­
                }
                patterns.append(pattern)

        return patterns

    def _create_graph_with_custom_ops(self, original_graph: GraphProto, replacement_info: Dict[str, Any]) -> GraphProto:
        """åˆ›å»ºåŒ…å«è‡ªå®šä¹‰ç®—å­çš„æ–°å›¾"""

        # å¤åˆ¶åŸå§‹å›¾çš„åŸºæœ¬ä¿¡æ¯
        new_nodes = []
        removed_node_names = set()

        # æ”¶é›†è¦åˆ é™¤çš„èŠ‚ç‚¹
        for subgraph in replacement_info["replaceable_subgraphs"]:
            start_idx, end_idx = subgraph["node_range"]
            for i in range(start_idx, end_idx):
                if i < len(original_graph.node):
                    removed_node_names.add(original_graph.node[i].name)

        # å¤åˆ¶æœªè¢«æ›¿æ¢çš„èŠ‚ç‚¹
        for node in original_graph.node:
            if node.name not in removed_node_names:
                new_nodes.append(node)

        # æ·»åŠ è‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹
        for custom_node_info in replacement_info["custom_nodes"]:
            custom_node = self._create_custom_op_node(custom_node_info)
            new_nodes.append(custom_node)

        # åˆ›å»ºæ–°å›¾
        new_graph = helper.make_graph(
            new_nodes,
            original_graph.name + "_with_custom_ops",
            original_graph.input,
            original_graph.output,
            original_graph.initializer
        )

        return new_graph

    def _create_custom_op_node(self, node_info: Dict[str, Any]):
        """åˆ›å»ºè‡ªå®šä¹‰ç®—å­èŠ‚ç‚¹"""

        # åˆ›å»ºå±æ€§
        attributes = []
        for attr_name, attr_value in node_info["attributes"].items():
            if isinstance(attr_value, int):
                attr = helper.make_attribute(attr_name, attr_value)
            elif isinstance(attr_value, float):
                attr = helper.make_attribute(attr_name, attr_value)
            else:
                attr = helper.make_attribute(attr_name, str(attr_value))
            attributes.append(attr)

        # åˆ›å»ºèŠ‚ç‚¹
        custom_node = helper.make_node(
            node_info["type"],
            node_info["inputs"],
            node_info["outputs"],
            node_info["name"],
            domain="gr.ml.infra",
            **{attr.name: attr for attr in attributes}
        )

        return custom_node

    def _generate_tensorrt_build_config(self, result_paths: Dict[str, str]) -> str:
        """ç”ŸæˆTensorRTæ„å»ºé…ç½®æ–‡ä»¶"""

        config_content = {
            "tensorrt_build": {
                "input_model": result_paths.get("custom_ops_onnx", result_paths.get("standard_onnx")),
                "engine_path": "hstu_with_custom_ops.trt",
                "precision": "fp16",
                "max_batch_size": 8,
                "optimization_level": 5,
                "workspace_size": "2GB",
                "dynamic_shapes": {
                    "input_ids": {
                        "min": [1, 8],
                        "opt": [4, 64],
                        "max": [8, 2048]
                    },
                    "attention_mask": {
                        "min": [1, 8],
                        "opt": [4, 64],
                        "max": [8, 2048]
                    },
                    "dense_features": {
                        "min": [1, 1024],
                        "opt": [4, 1024],
                        "max": [8, 1024]
                    }
                },
                "custom_plugins": {
                    "enabled": self.enable_custom_ops,
                    "plugin_library": "libgr_ml_infra_tensorrt_plugins.so",
                    "custom_ops": [node["type"] for node in result_paths.get("custom_nodes", [])]
                }
            }
        }

        # ä¿å­˜é…ç½®
        config_path = os.path.join(self.export_dir, "tensorrt_build_config.json")
        import json
        with open(config_path, 'w') as f:
            json.dump(config_content, f, indent=2)

        logger.info(f"âœ… TensorRTæ„å»ºé…ç½®ä¿å­˜åˆ°: {config_path}")
        return config_path

    def _create_dummy_inputs(self, batch_size: int, seq_len: int) -> Dict[str, torch.Tensor]:
        """åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®"""

        dummy_inputs = {
            'input_ids': torch.randint(
                0, self.config.vocab_size,
                (batch_size, seq_len),
                dtype=torch.long
            ),
            'attention_mask': torch.ones(
                batch_size, seq_len,
                dtype=torch.long
            ),
            'dense_features': torch.randn(
                batch_size, getattr(self.config, 'dense_feature_dim', 1024),
                dtype=torch.float32
            ),
            'position_ids': torch.arange(
                seq_len, dtype=torch.long
            ).unsqueeze(0).expand(batch_size, -1),
        }

        return dummy_inputs

    def _get_dynamic_axes_config(self) -> Dict[str, Dict[int, str]]:
        """è·å–åŠ¨æ€è½´é…ç½®"""

        return {
            'input_ids': {0: 'batch_size', 1: 'sequence'},
            'attention_mask': {0: 'batch_size', 1: 'sequence'},
            'dense_features': {0: 'batch_size'},
            'position_ids': {0: 'batch_size', 1: 'sequence'},
            'logits': {0: 'batch_size', 1: 'sequence'},
            'hidden_states': {0: 'batch_size', 1: 'sequence'},
            'engagement_scores': {0: 'batch_size'},
            'retention_scores': {0: 'batch_size'},
            'monetization_scores': {0: 'batch_size'},
        }

    def _verify_onnx_model(self, onnx_path: str, dummy_inputs: Dict[str, torch.Tensor]) -> bool:
        """éªŒè¯å¯¼å‡ºçš„ONNXæ¨¡å‹"""

        try:
            # åŠ è½½ONNXæ¨¡å‹
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)

            # åˆ›å»ºæ¨ç†ä¼šè¯ï¼ˆä»…ä½¿ç”¨CPUä»¥é¿å…è‡ªå®šä¹‰ç®—å­é—®é¢˜ï¼‰
            ort_session = ort.InferenceSession(
                onnx_path,
                providers=['CPUExecutionProvider']
            )

            # å‡†å¤‡è¾“å…¥æ•°æ®
            ort_inputs = {name: tensor.detach().cpu().numpy()
                         for name, tensor in dummy_inputs.items()}

            # æ‰§è¡Œæ¨ç†
            ort_outputs = ort_session.run(None, ort_inputs)

            logger.info("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
            return True

        except Exception as e:
            logger.error(f"âŒ ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return False

    def _verify_custom_ops_model_structure(self, onnx_path: str) -> bool:
        """éªŒè¯è‡ªå®šä¹‰ç®—å­æ¨¡å‹çš„ç»“æ„"""

        try:
            # åŠ è½½å¹¶æ£€æŸ¥æ¨¡å‹ç»“æ„
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«è‡ªå®šä¹‰ç®—å­
            custom_ops_found = []
            for node in onnx_model.graph.node:
                if node.domain == "gr.ml.infra":
                    custom_ops_found.append(node.op_type)

            if custom_ops_found:
                logger.info(f"âœ… å‘ç°è‡ªå®šä¹‰ç®—å­: {custom_ops_found}")
                return True
            else:
                logger.warning("âš ï¸ æœªå‘ç°è‡ªå®šä¹‰ç®—å­")
                return False

        except Exception as e:
            logger.error(f"âŒ è‡ªå®šä¹‰ç®—å­æ¨¡å‹ç»“æ„éªŒè¯å¤±è´¥: {e}")
            return False

    def _get_plugin_info(self) -> Dict[str, Any]:
        """è·å–æ’ä»¶ä¿¡æ¯"""

        if not self.plugin_manager:
            return {"available": False}

        return {
            "available": self.plugin_manager.is_available(),
            "num_registered": self.plugin_manager.get_num_registered_plugins(),
            "supported_plugins": self.plugin_manager.get_supported_plugins()
        }

    def _generate_enhanced_model_info(self,
                                    result_paths: Dict[str, str],
                                    batch_sizes: List[int],
                                    seq_lens: List[int]):
        """ç”Ÿæˆå¢å¼ºç‰ˆæ¨¡å‹ä¿¡æ¯æ–‡ä»¶"""

        info = {
            'model_info': {
                'model_type': 'HSTU_Generative_Recommender_Enhanced',
                'opset_version': self.opset_version,
                'custom_ops_enabled': self.enable_custom_ops,
                'vocab_size': self.config.vocab_size,
                'd_model': self.config.d_model,
                'num_layers': self.config.num_layers,
                'num_heads': self.config.num_heads,
                'max_seq_len': self.config.max_seq_len,
            },
            'export_config': {
                'supported_batch_sizes': batch_sizes,
                'supported_sequence_lengths': seq_lens,
                'dynamic_axes_enabled': True,
                'tensorrt_plugins_support': self.enable_custom_ops,
            },
            'files': result_paths,
            'custom_ops_info': self._get_plugin_info(),
            'tensorrt_build_instructions': {
                'standard_build': f"trtexec --onnx={result_paths.get('standard_onnx')} --saveEngine=hstu_standard.trt --fp16",
                'custom_ops_build': f"trtexec --onnx={result_paths.get('custom_ops_onnx', 'N/A')} --saveEngine=hstu_custom_ops.trt --fp16 --plugins=libgr_ml_infra_tensorrt_plugins.so" if self.enable_custom_ops else "éœ€è¦ç¼–è¯‘è‡ªå®šä¹‰æ’ä»¶åº“",
                'build_config': result_paths.get('tensorrt_config', 'N/A')
            }
        }

        # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
        info_path = os.path.join(self.export_dir, "enhanced_model_info.json")
        import json
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… å¢å¼ºç‰ˆæ¨¡å‹ä¿¡æ¯ä¿å­˜åˆ°: {info_path}")


def export_hstu_model_with_custom_ops(model,
                                     model_config,
                                     export_dir: str = "./models",
                                     batch_sizes: List[int] = [1, 2, 4, 8],
                                     sequence_lengths: List[int] = [64, 128, 256, 512],
                                     enable_custom_ops: bool = True,
                                     optimize: bool = True) -> Dict[str, Any]:
    """
    å¯¼å‡ºåŒ…å«è‡ªå®šä¹‰ç®—å­çš„HSTUæ¨¡å‹åˆ°ONNXæ ¼å¼

    Args:
        model: HSTUæ¨¡å‹å®ä¾‹
        model_config: æ¨¡å‹é…ç½®
        export_dir: å¯¼å‡ºç›®å½•
        batch_sizes: æ”¯æŒçš„æ‰¹æ¬¡å¤§å°
        sequence_lengths: æ”¯æŒçš„åºåˆ—é•¿åº¦
        enable_custom_ops: æ˜¯å¦å¯ç”¨è‡ªå®šä¹‰ç®—å­
        optimize: æ˜¯å¦ä¼˜åŒ–æ¨¡å‹

    Returns:
        åŒ…å«å¯¼å‡ºç»“æœçš„å­—å…¸
    """

    logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºæµç¨‹...")

    # åˆ›å»ºå¢å¼ºç‰ˆå¯¼å‡ºå™¨
    exporter = EnhancedHSTUOnnxExporter(
        model=model,
        model_config=model_config,
        export_dir=export_dir,
        enable_custom_ops=enable_custom_ops
    )

    try:
        # å¯¼å‡ºåŒ…å«è‡ªå®šä¹‰ç®—å­çš„æ¨¡å‹
        result = exporter.export_with_custom_ops(
            batch_sizes=batch_sizes,
            sequence_lengths=sequence_lengths,
            verify_export=True,
            optimize_model=optimize
        )

        logger.info("ğŸ‰ å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºå®Œæˆ!")
        logger.info(f"å¯¼å‡ºæ–‡ä»¶:")
        for key, value in result["export_paths"].items():
            if isinstance(value, list):
                for v in value:
                    logger.info(f"  - {v}")
            else:
                logger.info(f"  - {key}: {value}")

        return result

    except Exception as e:
        logger.error(f"âŒ å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºå¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'message': 'å¢å¼ºç‰ˆHSTUæ¨¡å‹ONNXå¯¼å‡ºå¤±è´¥'
        }


if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºç‰ˆONNXå¯¼å‡ºåŠŸèƒ½
    from .hstu_model import HSTUGenerativeRecommender, HSTUModelConfig

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    config = HSTUModelConfig(
        vocab_size=10000,
        d_model=512,
        num_layers=4,
        num_heads=8,
        max_seq_len=512
    )

    model = HSTUGenerativeRecommender(config)

    # æ‰§è¡Œå¢å¼ºç‰ˆå¯¼å‡º
    result = export_hstu_model_with_custom_ops(
        model=model,
        model_config=config,
        export_dir="./test_models_enhanced",
        batch_sizes=[1, 2, 4],
        sequence_lengths=[64, 128, 256],
        enable_custom_ops=True,
        optimize=True
    )

    print("å¢å¼ºç‰ˆå¯¼å‡ºç»“æœ:", result)