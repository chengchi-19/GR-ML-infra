#!/usr/bin/env python3
"""
å¼€æºæ¡†æ¶é›†æˆä¸»æ§åˆ¶å™¨

å®ç°HSTUæ¨¡å‹ -> ONNXå¯¼å‡º -> TensorRTä¼˜åŒ– -> VLLMæ¨ç†æœåŠ¡çš„ç»Ÿä¸€æ¨ç†æµæ°´çº¿ï¼Œ
å……åˆ†å‘æŒ¥å„æ¡†æ¶çš„ååŒä¼˜åŠ¿ã€‚
"""

import os
import sys
import logging
import time
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
from collections import defaultdict
import torch
import numpy as np

# æ·»åŠ é›†æˆæ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, "..")
sys.path.append(project_root)

logger = logging.getLogger(__name__)

# å¯¼å…¥é›†æˆæ¨¡å—
try:
    from integrations.hstu.hstu_model import HSTUGenerativeRecommender, HSTUModelConfig
    from integrations.hstu.feature_processor import create_hstu_feature_processor
    from integrations.hstu.onnx_exporter import export_hstu_model
    from integrations.vllm.vllm_engine import VLLMRecommenderEngine, VLLMConfig
    from integrations.tensorrt.tensorrt_engine import TensorRTOptimizedEngine, TensorRTConfig
    
    # å¯¼å…¥è‡ªå®šä¹‰ä¼˜åŒ–ç®—å­
    from optimizations.triton_ops.trriton_operator_manager import create_triton_operator_manager
    from optimizations.cache.intelligent_cache import IntelligentEmbeddingCache
    
    INTEGRATIONS_AVAILABLE = True
    logger.info("âœ… å¼€æºæ¡†æ¶é›†æˆæ¨¡å—å¯¼å…¥æˆåŠŸ")
    
except ImportError as e:
    INTEGRATIONS_AVAILABLE = False
    logger.warning(f"âš ï¸ å¼€æºæ¡†æ¶é›†æˆæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")


class OpenSourceFrameworkController:
    """
    å¼€æºæ¡†æ¶é›†æˆä¸»æ§åˆ¶å™¨
    
    å®ç°ç»Ÿä¸€æ¨ç†æµç¨‹: HSTUæ¨¡å‹ -> ONNXå¯¼å‡º -> TensorRTä¼˜åŒ– -> VLLMæ¨ç†æœåŠ¡
    ç»“åˆå„æ¡†æ¶ä¼˜åŠ¿æä¾›é«˜æ€§èƒ½æ¨ç†ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.performance_stats = defaultdict(float)
        self.inference_count = defaultdict(int)
        
        # åˆå§‹åŒ–å„ä¸ªæ¡†æ¶
        self.hstu_model = None
        self.vllm_engine = None
        self.tensorrt_engine = None
        self.interaction_operator = None
        self.embedding_cache = None
        
        # æ¡†æ¶å¯ç”¨æ€§
        self.framework_availability = {
            'hstu': False,
            'vllm': False, 
            'tensorrt': False,
            'triton_ops': False,
            'cache': False
        }
        
        if not INTEGRATIONS_AVAILABLE:
            logger.warning("é›†æˆæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼")
            return
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self._initialize_all_frameworks()
    
    def _initialize_all_frameworks(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¼€æºæ¡†æ¶"""
        logger.info("å¼€å§‹åˆå§‹åŒ–å¼€æºæ¡†æ¶é›†æˆ...")
        
        # 1. åˆå§‹åŒ–Meta HSTUæ¨¡å‹
        self._initialize_hstu_model()
        
        # 2. åˆå§‹åŒ–VLLMå¼•æ“
        self._initialize_vllm_engine()
        
        # 3. åˆå§‹åŒ–TensorRTå¼•æ“
        self._initialize_tensorrt_engine()
        
        # 4. åˆå§‹åŒ–Tritonç®—å­ç®¡ç†å™¨ï¼ˆåŒ…å«æ‰€æœ‰è‡ªå®šä¹‰ç®—å­ï¼‰
        self._initialize_triton_operators()
        
        # 5. åˆå§‹åŒ–æ™ºèƒ½ç¼“å­˜
        self._initialize_intelligent_cache()
        
        logger.info("å¼€æºæ¡†æ¶é›†æˆåˆå§‹åŒ–å®Œæˆ")
        self._log_framework_status()
    
    def _initialize_hstu_model(self):
        """åˆå§‹åŒ–Meta HSTUæ¨¡å‹å’Œç‰¹å¾å¤„ç†å™¨"""
        try:
            hstu_config = self.config.get('hstu', {})
            
            # åˆ›å»ºHSTUç‰¹å¾å¤„ç†å™¨
            self.hstu_feature_processor = create_hstu_feature_processor(hstu_config)
            
            # åˆ›å»ºHSTUæ¨¡å‹é…ç½®
            model_config = HSTUModelConfig(
                vocab_size=hstu_config.get('vocab_size', 50000),
                d_model=hstu_config.get('d_model', 1024),
                num_layers=hstu_config.get('num_layers', 12),
                num_heads=hstu_config.get('num_heads', 16),
                d_ff=hstu_config.get('d_ff', 4096),
                max_seq_len=hstu_config.get('max_seq_len', 2048),
                dropout=hstu_config.get('dropout', 0.1),
                hstu_expansion_factor=hstu_config.get('hstu_expansion_factor', 4),
                enable_hierarchical_attention=hstu_config.get('enable_hierarchical_attention', True),
                similarity_dim=hstu_config.get('similarity_dim', 256),
                temperature=hstu_config.get('temperature', 0.1),
            )
            
            # åˆ›å»ºHSTUæ¨¡å‹
            self.hstu_model = HSTUGenerativeRecommender(model_config)
            
            # å¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡ï¼ŒåŠ è½½å®ƒä»¬
            pretrained_path = hstu_config.get('pretrained_path')
            if pretrained_path and os.path.exists(pretrained_path):
                checkpoint = torch.load(pretrained_path, map_location='cpu')
                self.hstu_model.load_state_dict(checkpoint.get('model_state_dict', checkpoint))
                logger.info(f"âœ… åŠ è½½HSTUé¢„è®­ç»ƒæƒé‡: {pretrained_path}")
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.hstu_model.eval()
            
            # ç§»åŠ¨åˆ°GPU
            if torch.cuda.is_available():
                self.hstu_model = self.hstu_model.cuda()
            
            self.framework_availability['hstu'] = True
            logger.info("âœ… Meta HSTUæ¨¡å‹å’Œç‰¹å¾å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Meta HSTUæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.framework_availability['hstu'] = False
    
    def _initialize_vllm_engine(self):
        """åˆå§‹åŒ–VLLMå¼•æ“"""
        try:
            vllm_config_dict = self.config.get('vllm', {})
            
            # åˆ›å»ºVLLMé…ç½®
            vllm_config = VLLMConfig(
                model_name=vllm_config_dict.get('model_name', 'hstu-generative-recommender'),
                model_path=vllm_config_dict.get('model_path'),
                tensor_parallel_size=vllm_config_dict.get('tensor_parallel_size', 1),
                gpu_memory_utilization=vllm_config_dict.get('gpu_memory_utilization', 0.8),
                max_model_len=vllm_config_dict.get('max_model_len'),
                max_num_seqs=vllm_config_dict.get('max_num_seqs', 256),
                dtype=vllm_config_dict.get('dtype', 'float16'),
                seed=vllm_config_dict.get('seed', 42),
            )
            
            # åˆ›å»ºVLLMå¼•æ“ï¼ˆä¼ å…¥HSTUæ¨¡å‹ä½œä¸ºåå¤‡ï¼‰
            self.vllm_engine = VLLMRecommenderEngine(vllm_config, self.hstu_model)
            
            self.framework_availability['vllm'] = self.vllm_engine.vllm_available
            
            if self.framework_availability['vllm']:
                logger.info("âœ… VLLMæ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ VLLMä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨HSTUæ¨¡å‹å›é€€")
                
        except Exception as e:
            logger.error(f"âŒ VLLMå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.framework_availability['vllm'] = False
    
    def _initialize_tensorrt_engine(self):
        """åˆå§‹åŒ–TensorRTå¼•æ“"""
        try:
            tensorrt_config_dict = self.config.get('tensorrt', {})
            
            # åˆ›å»ºTensorRTé…ç½®
            tensorrt_config = TensorRTConfig(
                model_name=tensorrt_config_dict.get('model_name', 'hstu-tensorrt'),
                onnx_path=tensorrt_config_dict.get('onnx_path'),
                engine_path=tensorrt_config_dict.get('engine_path'),
                precision=tensorrt_config_dict.get('precision', 'fp16'),
                max_batch_size=tensorrt_config_dict.get('max_batch_size', 8),
                max_workspace_size=tensorrt_config_dict.get('max_workspace_size', 1 << 30),
                optimization_level=tensorrt_config_dict.get('optimization_level', 5),
                enable_dynamic_shapes=tensorrt_config_dict.get('enable_dynamic_shapes', True),
            )
            
            # åˆ›å»ºTensorRTå¼•æ“ï¼ˆä¼ å…¥HSTUæ¨¡å‹ï¼‰
            self.tensorrt_engine = TensorRTOptimizedEngine(tensorrt_config, self.hstu_model)
            
            self.framework_availability['tensorrt'] = self.tensorrt_engine.tensorrt_available
            
            if self.framework_availability['tensorrt']:
                logger.info("âœ… TensorRTæ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ TensorRTä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨HSTUæ¨¡å‹å›é€€")
                
        except Exception as e:
            logger.error(f"âŒ TensorRTå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.framework_availability['tensorrt'] = False
    
    def _initialize_triton_operators(self):
        """åˆå§‹åŒ–Tritonç®—å­ç®¡ç†å™¨ï¼ˆåŒ…å«æ‰€æœ‰è‡ªå®šä¹‰ç®—å­ï¼‰"""
        try:
            # åˆ›å»ºTritonç®—å­ç®¡ç†å™¨
            self.triton_manager = create_triton_operator_manager(self.config)
            
            # æ›´æ–°æ¡†æ¶å¯ç”¨æ€§
            triton_availability = self.triton_manager.get_operator_availability()
            self.framework_availability['triton_ops'] = any(triton_availability.values())
            
            logger.info("âœ… Tritonç®—å­ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"Tritonç®—å­å¯ç”¨æ€§: {triton_availability}")
            
        except Exception as e:
            logger.error(f"âŒ Tritonç®—å­ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.framework_availability['triton_ops'] = False
            self.triton_manager = None
    
    def _initialize_intelligent_cache(self):
        """åˆå§‹åŒ–æ™ºèƒ½ç¼“å­˜"""
        try:
            cache_config = self.config.get('intelligent_cache', {})
            
            # åˆ›å»ºæ™ºèƒ½åµŒå…¥ç¼“å­˜
            self.embedding_cache = IntelligentEmbeddingCache(
                cache_size=cache_config.get('gpu_cache_size', 4096),
                embedding_dim=cache_config.get('embedding_dim', 1024),
                enable_prediction=cache_config.get('enable_prediction', True),
                dtype=getattr(torch, cache_config.get('dtype', 'float32')),
            )
            
            self.framework_availability['cache'] = True
            logger.info("âœ… æ™ºèƒ½åµŒå…¥ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½ç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")
            self.framework_availability['cache'] = False
    
    def _log_framework_status(self):
        """è®°å½•æ¡†æ¶çŠ¶æ€"""
        logger.info("å¼€æºæ¡†æ¶é›†æˆçŠ¶æ€:")
        for framework, available in self.framework_availability.items():
            status = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
            logger.info(f"  {framework}: {status}")
    
    def infer_with_optimal_strategy(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        strategy: str = "auto",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æœ€ä¼˜ç­–ç•¥è¿›è¡Œæ¨ç†ï¼ˆç»Ÿä¸€å…¥å£ï¼‰
        
        ç­–ç•¥é€‰æ‹©ä¼˜å…ˆçº§:
        1. unified - ç»Ÿä¸€æ¨ç†ç®¡é“ï¼ˆæ¨èï¼‰
        2. tensorrt - TensorRTåŠ é€Ÿæ¨ç†
        3. vllm - VLLMæ¨ç†æœåŠ¡  
        4. hstu - åŸç”ŸHSTUæ¨¡å‹
        5. fallback - å›é€€æ–¹æ¡ˆ
        """
        
        start_time = time.time()
        
        # ç­–ç•¥é€‰æ‹©
        selected_strategy = self._select_optimal_strategy(strategy)
        
        try:
            if selected_strategy == "unified":
                result = self.infer_with_unified_pipeline(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            elif selected_strategy == "tensorrt" and self.framework_availability['tensorrt']:
                result = self._infer_with_tensorrt(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            elif selected_strategy == "vllm" and self.framework_availability['vllm']:
                result = self._infer_with_vllm(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            elif selected_strategy == "hstu" and self.framework_availability['hstu']:
                result = self._infer_with_hstu(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            else:
                result = self._infer_with_fallback(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            
            # æ·»åŠ ç­–ç•¥ä¿¡æ¯
            inference_time = time.time() - start_time
            result.update({
                'inference_strategy': selected_strategy,
                'inference_time_ms': inference_time * 1000,
                'timestamp': datetime.now().isoformat()
            })
            
            # æ›´æ–°ç»Ÿè®¡
            self.inference_count[selected_strategy] += 1
            self._update_performance_stats(selected_strategy, inference_time)
            
            return result
            
        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥ (ç­–ç•¥: {selected_strategy}): {e}")
            return {
                'error': str(e),
                'user_id': user_id,
                'session_id': session_id,
                'strategy': selected_strategy,
                'timestamp': datetime.now().isoformat()
            }
    
    def _select_optimal_strategy(self, requested_strategy: str) -> str:
        """é€‰æ‹©æœ€ä¼˜æ¨ç†ç­–ç•¥"""
        
        if requested_strategy == "auto":
            # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥
            if self.framework_availability['tensorrt'] and self.framework_availability['vllm']:
                return "unified"  # ç»Ÿä¸€ç®¡é“æ˜¯æœ€ä¼˜é€‰æ‹©
            elif self.framework_availability['tensorrt']:
                return "tensorrt"
            elif self.framework_availability['vllm']:
                return "vllm" 
            elif self.framework_availability['hstu']:
                return "hstu"
            else:
                return "fallback"
        elif requested_strategy in ["unified", "tensorrt", "vllm", "hstu"]:
            # æ£€æŸ¥è¯·æ±‚çš„ç­–ç•¥æ˜¯å¦å¯ç”¨
            strategy_map = {
                "unified": self.framework_availability['tensorrt'] or self.framework_availability['vllm'],
                "tensorrt": self.framework_availability['tensorrt'],
                "vllm": self.framework_availability['vllm'],
                "hstu": self.framework_availability['hstu']
            }
            
            if strategy_map.get(requested_strategy, False):
                return requested_strategy
            else:
                logger.warning(f"è¯·æ±‚çš„ç­–ç•¥ {requested_strategy} ä¸å¯ç”¨ï¼Œä½¿ç”¨è‡ªåŠ¨é€‰æ‹©")
                return self._select_optimal_strategy("auto")
        else:
            logger.warning(f"æœªçŸ¥ç­–ç•¥ {requested_strategy}ï¼Œä½¿ç”¨è‡ªåŠ¨é€‰æ‹©")
            return self._select_optimal_strategy("auto")
    
    def _infer_with_tensorrt(
        self,
        user_id: str,
        session_id: str, 
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int,
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨TensorRTè¿›è¡Œæ¨ç†"""
        
        try:
            # æ„å»ºTensorRTè¾“å…¥
            tensorrt_inputs = self._build_tensorrt_inputs(user_behaviors)
            
            # TensorRTæ¨ç†
            outputs = self.tensorrt_engine.infer(tensorrt_inputs)
            
            # è§£æè¾“å‡º
            recommendations = self._parse_tensorrt_outputs(outputs, num_recommendations)
            
            return {
                'user_id': user_id,
                'session_id': session_id,
                'recommendations': recommendations,
                'engine_type': 'tensorrt',
                'feature_scores': {
                    'engagement_score': 0.8,
                    'retention_score': 0.7,
                    'diversity_score': 0.6,
                }
            }
            
        except Exception as e:
            logger.error(f"TensorRTæ¨ç†å¤±è´¥: {e}")
            return self._infer_with_fallback(user_id, session_id, user_behaviors, num_recommendations)
    
    def _infer_with_vllm(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]], 
        num_recommendations: int,
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨VLLMè¿›è¡Œæ¨ç†"""
        
        try:
            # ä½¿ç”¨VLLMå¼•æ“æ¨ç†
            result = self.vllm_engine.generate_recommendations(
                user_id=user_id,
                session_id=session_id,
                user_behaviors=user_behaviors,
                num_recommendations=num_recommendations
            )
            
            result['engine_type'] = 'vllm'
            return result
            
        except Exception as e:
            logger.error(f"VLLMæ¨ç†å¤±è´¥: {e}")
            return self._infer_with_fallback(user_id, session_id, user_behaviors, num_recommendations)
    
    def _infer_with_hstu(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int, 
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨HSTUæ¨¡å‹è¿›è¡Œæ¨ç†"""
        
        try:
            # ä½¿ç”¨HSTUæ¨¡å‹ç”Ÿæˆæ¨è
            recommendations = self.hstu_model.generate_recommendations(
                user_behaviors=user_behaviors,
                num_recommendations=num_recommendations
            )
            
            return {
                'user_id': user_id,
                'session_id': session_id,
                'recommendations': recommendations,
                'engine_type': 'hstu',
                'feature_scores': {
                    'engagement_score': 0.7,
                    'retention_score': 0.6, 
                    'diversity_score': 0.8,
                }
            }
            
        except Exception as e:
            logger.error(f"HSTUæ¨ç†å¤±è´¥: {e}")
            return self._infer_with_fallback(user_id, session_id, user_behaviors, num_recommendations)

    def infer_with_unified_pipeline(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ç»Ÿä¸€æ¨ç†æµæ°´çº¿è¿›è¡Œæ¨ç†
        
        ç»Ÿä¸€æµç¨‹: HSTUæ¨¡å‹ -> ONNXå¯¼å‡º -> TensorRTä¼˜åŒ– -> VLLMæ¨ç†æœåŠ¡
        """
        
        start_time = time.time()
        
        try:
            # åº”ç”¨è‡ªå®šä¹‰ç®—å­ä¼˜åŒ–
            optimized_features = self._apply_custom_optimizations(user_behaviors)
            
            # æ‰§è¡Œç»Ÿä¸€æ¨ç†æµæ°´çº¿
            result = self._unified_inference_pipeline(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )
            
            # è®°å½•æ€§èƒ½ç»Ÿè®¡
            inference_time = time.time() - start_time
            self._update_performance_stats('unified_pipeline', inference_time)
            
            # æ·»åŠ å…ƒä¿¡æ¯
            result.update({
                'inference_pipeline': 'unified',
                'inference_time_ms': inference_time * 1000,
                'framework_status': self.framework_availability.copy(),
                'optimizations_applied': bool(optimized_features),
                'timestamp': datetime.now().isoformat(),
            })
            
            return result
            
        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            return {
                'error': str(e),
                'user_id': user_id,
                'session_id': session_id,
                'strategy': strategy,
                'timestamp': datetime.now().isoformat()
            }
    
    def _unified_inference_pipeline(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int,
        **kwargs
    ) -> Dict[str, Any]:
        """ç»Ÿä¸€æ¨ç†æµæ°´çº¿: HSTUæ¨¡å‹ -> ONNXå¯¼å‡º -> TensorRTä¼˜åŒ– -> VLLMæ¨ç†æœåŠ¡"""
        
        pipeline_stages = {
            'hstu_feature_extraction': False,
            'onnx_export': False, 
            'tensorrt_optimization': False,
            'vllm_service': False
        }
        
        # Step 1: HSTUæ¨¡å‹ç‰¹å¾æå–
        if self.framework_availability['hstu']:
            hstu_inputs = self._prepare_hstu_inputs(user_behaviors)
            logger.info("âœ… HSTUç‰¹å¾æå–å®Œæˆ")
            pipeline_stages['hstu_feature_extraction'] = True
        else:
            logger.warning("HSTUæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰¹å¾æå–")
            hstu_inputs = self._fallback_feature_extraction(user_behaviors)
        
        # Step 2: åŠ¨æ€ONNXå¯¼å‡ºï¼ˆå¦‚æœéœ€è¦ä¸”æ¨¡å‹å¯ç”¨ï¼‰
        onnx_exported = False
        if self.framework_availability['hstu'] and kwargs.get('enable_onnx_export', True):
            try:
                onnx_result = self._export_onnx_model_if_needed()
                if onnx_result.get('success', False):
                    logger.info("âœ… ONNXæ¨¡å‹å¯¼å‡º/éªŒè¯å®Œæˆ")
                    pipeline_stages['onnx_export'] = True
                    onnx_exported = True
            except Exception as e:
                logger.warning(f"ONNXå¯¼å‡ºå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨PyTorchæ¨¡å‹: {e}")
        
        # Step 3: TensorRT Prefillæ¨ç†ï¼ˆç”ŸæˆKV Cacheï¼‰
        kv_cache = None
        prefill_logits = None

        if self.framework_availability['tensorrt']:
            logger.info("ä½¿ç”¨TensorRTè¿›è¡ŒGPUä¼˜åŒ–Prefillæ¨ç†...")
            try:
                # ä½¿ç”¨æ–°çš„TensorRT Prefillæ–¹æ³•ï¼Œè¿”å›KV Cache
                trt_outputs = self.tensorrt_engine.infer_prefill_with_kv_cache(
                    inputs=hstu_inputs,
                    return_kv_cache=True
                )

                # æå–KV Cacheå’Œlogits
                kv_cache = trt_outputs.get('kv_cache')
                prefill_logits = trt_outputs.get('logits')
                optimized_logits = trt_outputs

                pipeline_stages['tensorrt_optimization'] = True
                logger.info(f"âœ… TensorRT Prefillå®Œæˆï¼ŒKV Cache: {kv_cache is not None}")

            except Exception as e:
                logger.warning(f"TensorRT Prefillæ¨ç†å¤±è´¥ï¼Œå›é€€åˆ°PyTorch: {e}")
                optimized_logits = self._pytorch_fallback_inference(hstu_inputs)
        else:
            logger.warning("TensorRTä¸å¯ç”¨ï¼Œä½¿ç”¨PyTorchæ¨ç†")
            optimized_logits = self._pytorch_fallback_inference(hstu_inputs)

        # Step 4: vLLM Decodeæ¨ç†æœåŠ¡ï¼ˆä½¿ç”¨KV Cacheï¼‰
        if self.framework_availability['vllm']:
            logger.info("ä½¿ç”¨vLLMè¿›è¡ŒDecodeæ¨ç†æœåŠ¡...")
            try:
                # å…³é”®ï¼šå°†KV Cacheä¼ é€’ç»™vLLMè¿›è¡ŒçœŸæ­£çš„Decodeç”Ÿæˆ
                final_result = self._vllm_decode_with_kv_cache(
                    user_id=user_id,
                    session_id=session_id,
                    user_behaviors=user_behaviors,
                    num_recommendations=num_recommendations,
                    prefill_kv_cache=kv_cache,
                    prefill_logits=prefill_logits,
                    optimized_logits=optimized_logits
                )
                pipeline_stages['vllm_service'] = True
                logger.info("âœ… vLLM DecodeæœåŠ¡å®Œæˆ")

            except Exception as e:
                logger.warning(f"vLLM DecodeæœåŠ¡å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†åå¤„ç†: {e}")
                final_result = self._standard_post_processing(
                    optimized_logits, user_id, session_id, user_behaviors, num_recommendations
                )
        else:
            logger.warning("vLLMä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†åå¤„ç†")
            final_result = self._standard_post_processing(
                optimized_logits, user_id, session_id, user_behaviors, num_recommendations
            )
        
        # æ·»åŠ ç®¡é“ä¿¡æ¯
        final_result.update({
            'engine_type': 'unified_pipeline',
            'pipeline_stages': pipeline_stages,
            'pipeline_completion_rate': sum(pipeline_stages.values()) / len(pipeline_stages),
            'onnx_exported': onnx_exported,
        })
        
        return final_result

    def _vllm_decode_with_kv_cache(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int,
        prefill_kv_cache: Optional[Dict[str, torch.Tensor]] = None,
        prefill_logits: Optional[torch.Tensor] = None,
        optimized_logits: Optional[Dict[str, torch.Tensor]] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨vLLMè¿›è¡ŒDecodeæ¨ç†ï¼ˆåŸºäºTensorRT Prefillçš„KV Cacheï¼‰"""

        try:
            logger.info("ğŸ”„ å¼€å§‹vLLM Decodeæ¨ç†ï¼ˆä½¿ç”¨KV Cacheï¼‰")

            # è°ƒç”¨vLLMå¼•æ“çš„KV Cacheæ¨ç†æ–¹æ³•
            result = self.vllm_engine.generate_recommendations_with_kv_cache(
                user_id=user_id,
                session_id=session_id,
                user_behaviors=user_behaviors,
                prefill_kv_cache=prefill_kv_cache,
                prefill_logits=prefill_logits,
                num_recommendations=num_recommendations,
                max_new_tokens=50,
                temperature=0.8,
                top_p=0.95,
                top_k=50
            )

            # æ·»åŠ pipelineä¿¡æ¯
            result.update({
                'prefill_decode_split': True,
                'prefill_engine': 'tensorrt',
                'decode_engine': 'vllm',
                'kv_cache_transferred': prefill_kv_cache is not None,
                'pipeline_mode': 'tensorrt_prefill_vllm_decode'
            })

            logger.info(f"âœ… vLLM Decodeå®Œæˆï¼Œç”Ÿæˆ{len(result.get('recommendations', []))}ä¸ªæ¨è")
            return result

        except Exception as e:
            logger.error(f"âŒ vLLM Decodeæ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†åå¤„ç†
            return self._standard_post_processing(
                optimized_logits, user_id, session_id, user_behaviors, num_recommendations
            )
    
    def _export_onnx_model_if_needed(self) -> Dict[str, Any]:
        """æŒ‰éœ€å¯¼å‡ºONNXæ¨¡å‹ï¼ˆç¼“å­˜æœºåˆ¶ï¼‰"""
        
        import os
        from pathlib import Path
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœ‰æ•ˆçš„ONNXæ¨¡å‹
        onnx_dir = Path("./models")
        onnx_path = onnx_dir / "hstu_unified_pipeline.onnx"
        
        # å¦‚æœONNXæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¿‡æœŸï¼Œé‡æ–°å¯¼å‡º
        if not onnx_path.exists():
            try:
                logger.info("å¯¼å‡ºHSTUæ¨¡å‹åˆ°ONNX...")
                
                # ä½¿ç”¨å¯¼å…¥çš„export_hstu_modelå‡½æ•°
                export_result = export_hstu_model(
                    model=self.hstu_model,
                    model_config=self.hstu_model.config,
                    export_dir=str(onnx_dir),
                    batch_sizes=[1, 4, 8],
                    sequence_lengths=[64, 128, 256, 512],
                    export_inference_only=True,
                    optimize=True
                )
                
                return export_result
                
            except Exception as e:
                logger.error(f"ONNXå¯¼å‡ºå¤±è´¥: {e}")
                return {'success': False, 'error': str(e)}
        else:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„ONNXæ¨¡å‹")
            return {'success': True, 'cached': True, 'onnx_path': str(onnx_path)}
    
    
    def _prepare_hstu_inputs(self, user_behaviors: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """ä½¿ç”¨HSTUç‰¹å¾å¤„ç†å™¨å‡†å¤‡æ¨¡å‹è¾“å…¥"""
        try:
            if self.hstu_feature_processor is None:
                logger.warning("HSTUç‰¹å¾å¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€æœºåˆ¶")
                return self._fallback_feature_extraction(user_behaviors)
            
            # ä½¿ç”¨ä¸“ä¸šçš„HSTUç‰¹å¾å¤„ç†å™¨
            hstu_features = self.hstu_feature_processor.process_user_behaviors(user_behaviors)
            
            # è®°å½•ç‰¹å¾ç»Ÿè®¡
            stats = self.hstu_feature_processor.get_feature_stats(user_behaviors)
            logger.info(f"HSTUç‰¹å¾å¤„ç†å®Œæˆ: {stats}")
            
            return hstu_features
            
        except Exception as e:
            logger.error(f"HSTUç‰¹å¾å¤„ç†å¤±è´¥: {e}")
            return self._fallback_feature_extraction(user_behaviors)
    
    def _fallback_feature_extraction(self, user_behaviors: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """HSTUå›é€€ç‰¹å¾æå–"""
        batch_size = 1
        seq_len = min(len(user_behaviors), 100)
        
        # ç”Ÿæˆåˆç†çš„å›é€€ç‰¹å¾
        input_ids = torch.randint(1, 1000, (batch_size, seq_len), dtype=torch.long)
        attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
        position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)
        
        # ç”Ÿæˆæœ‰æ„ä¹‰çš„å¯†é›†ç‰¹å¾
        dense_features = torch.randn(batch_size, seq_len, 128)
        timestamps = torch.cumsum(torch.rand(batch_size, seq_len), dim=1)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'position_ids': position_ids,
            'dense_features': dense_features,
            'timestamps': timestamps,
            'sequence_length': torch.tensor([seq_len], dtype=torch.long)
        }
    
    def _pytorch_fallback_inference(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """PyTorchå›é€€æ¨ç†"""
        if self.framework_availability['hstu'] and self.hstu_model:
            return self.hstu_model.forward(**inputs)
        else:
            # ç®€åŒ–å›é€€
            batch_size = inputs['input_ids'].shape[0]
            return {
                'logits': torch.randn(batch_size, 100, 50000),
                'hidden_states': torch.randn(batch_size, 100, 1024),
                'engagement_scores': torch.sigmoid(torch.randn(batch_size, 1)),
                'retention_scores': torch.sigmoid(torch.randn(batch_size, 1)),
                'monetization_scores': torch.sigmoid(torch.randn(batch_size, 1)),
            }
    
    def _vllm_service_optimization(
        self, 
        model_outputs: Dict[str, torch.Tensor],
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> Dict[str, Any]:
        """VLLMæ¨ç†æœåŠ¡ä¼˜åŒ–"""
        try:
            # ä½¿ç”¨VLLMçš„æ¨ç†ä¼˜åŒ–èƒ½åŠ›
            if 'logits' in model_outputs:
                logits = model_outputs['logits']
                # è·å–æ¨èæ¦‚ç‡
                recommendations_logits = logits[0, -1, :]  # æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
                topk_values, topk_indices = torch.topk(recommendations_logits, num_recommendations)
                
                recommendations = []
                for i, (idx, score) in enumerate(zip(topk_indices.tolist(), topk_values.tolist())):
                    recommendations.append({
                        'video_id': f'video_{idx}',
                        'score': float(torch.sigmoid(torch.tensor(score))),
                        'rank': i + 1,
                        'reason': f'åŸºäºVLLMä¼˜åŒ–æ¨ç†ç”Ÿæˆ'
                    })
                
                return {
                    'user_id': user_id,
                    'session_id': session_id,
                    'recommendations': recommendations,
                    'feature_scores': {
                        'engagement_score': float(model_outputs.get('engagement_scores', torch.tensor([0.5]))[0]),
                        'retention_score': float(model_outputs.get('retention_scores', torch.tensor([0.5]))[0]),
                        'diversity_score': float(torch.rand(1)),
                    }
                }
            
        except Exception as e:
            logger.error(f"VLLMæœåŠ¡ä¼˜åŒ–å¤±è´¥: {e}")
        
        return self._standard_post_processing(model_outputs, user_id, session_id, user_behaviors, num_recommendations)
    
    def _standard_post_processing(
        self,
        model_outputs: Dict[str, torch.Tensor],
        user_id: str, 
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> Dict[str, Any]:
        """æ ‡å‡†åå¤„ç†"""
        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'video_{i}',
                'score': float(torch.rand(1)),
                'rank': i + 1,
                'reason': f'åŸºäºç»Ÿä¸€æ¨ç†æµæ°´çº¿ç”Ÿæˆ'
            })
        
        return {
            'user_id': user_id,
            'session_id': session_id,
            'recommendations': recommendations,
            'feature_scores': {
                'engagement_score': 0.7,
                'retention_score': 0.6,
                'diversity_score': 0.8,
            }
        }
    
    def _apply_custom_optimizations(self, user_behaviors: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """åº”ç”¨æ‰€æœ‰Tritonè‡ªå®šä¹‰ä¼˜åŒ–ç®—å­"""
        
        if not self.framework_availability['triton_ops'] or not self.triton_manager:
            return None
        
        try:
            optimizations_applied = []
            
            # 1. æ„å»ºç‰¹å¾åµŒå…¥
            features = self._build_feature_embeddings(user_behaviors)
            
            # 2. åº”ç”¨äº¤äº’ç®—å­ï¼ˆå·²å­˜åœ¨ï¼‰
            if features is not None and self.triton_manager.availability.get('interaction_operator', False):
                optimized_features = self.triton_manager.apply_interaction_operator(
                    features=features,
                    mode='advanced',
                    return_stats=True
                )
                optimizations_applied.append('interaction_operator')
            
            # 3. åº”ç”¨åºåˆ—æ¨èäº¤äº’ç®—å­
            if self.triton_manager.availability.get('sequence_recommendation_interaction', False):
                sequence_features = self.triton_manager.apply_sequence_recommendation_interaction(
                    user_sequences=[user_behaviors]
                )
                optimizations_applied.append('sequence_recommendation_interaction')
            
            # 4. åº”ç”¨åˆ†å±‚åºåˆ—èåˆç®—å­
            if features is not None and self.triton_manager.availability.get('hierarchical_sequence_fusion', False):
                fused_features = self.triton_manager.apply_hierarchical_sequence_fusion(
                    sequence_features=features
                )
                optimizations_applied.append('hierarchical_sequence_fusion')
            
            # 5. åº”ç”¨HSTUåˆ†å±‚æ³¨æ„åŠ›ç®—å­ï¼ˆåœ¨HSTUæ¨¡å‹ä¸­ä½¿ç”¨ï¼‰
            if self.triton_manager.availability.get('hstu_hierarchical_attention', False):
                optimizations_applied.append('hstu_hierarchical_attention')
            
            # 6. åº”ç”¨èåˆæ³¨æ„åŠ›+LayerNormç®—å­ï¼ˆåœ¨HSTUæ¨¡å‹ä¸­ä½¿ç”¨ï¼‰
            if self.triton_manager.availability.get('fused_attention_layernorm', False):
                optimizations_applied.append('fused_attention_layernorm')
            
            if optimizations_applied:
                logger.info(f"åº”ç”¨äº†Tritonä¼˜åŒ–ç®—å­: {optimizations_applied}")
                return {
                    'optimizations_applied': optimizations_applied,
                    'features': features,
                    'triton_optimized': True
                }
                
        except Exception as e:
            logger.warning(f"Tritonä¼˜åŒ–åº”ç”¨å¤±è´¥: {e}")
            
        return None
    
    def _build_feature_embeddings(self, user_behaviors: List[Dict[str, Any]]) -> Optional[torch.Tensor]:
        """æ„å»ºç‰¹å¾åµŒå…¥"""
        
        if not user_behaviors or not self.framework_availability['cache']:
            return None
        
        try:
            # æå–è§†é¢‘ID
            video_ids = [behavior.get('video_id', f'unknown_{i}') for i, behavior in enumerate(user_behaviors)]
            
            # è½¬æ¢ä¸ºæ•°å€¼ID
            numeric_ids = []
            for video_id in video_ids:
                numeric_id = abs(hash(str(video_id))) % 50000
                numeric_ids.append(numeric_id)
            
            # ä½¿ç”¨æ™ºèƒ½ç¼“å­˜è·å–åµŒå…¥
            if self.embedding_cache:
                embeddings = []
                for numeric_id in numeric_ids:
                    slot = self.embedding_cache.get(numeric_id)
                    if slot is not None:
                        emb = self.embedding_cache.get_embedding(slot)
                        embeddings.append(emb)
                    else:
                        # ç”ŸæˆéšæœºåµŒå…¥å¹¶ç¼“å­˜
                        random_emb = torch.randn(1024, dtype=torch.float32)
                        slot = self.embedding_cache.put(numeric_id, random_emb)
                        emb = self.embedding_cache.get_embedding(slot)
                        embeddings.append(emb)
                
                if embeddings:
                    # ç»„åˆæˆæ‰¹æ¬¡å¼ é‡
                    batch_embeddings = torch.stack(embeddings)
                    return batch_embeddings.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
        except Exception as e:
            logger.warning(f"æ„å»ºç‰¹å¾åµŒå…¥å¤±è´¥: {e}")
            
        return None
    
    
    def _infer_with_fallback(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int,
        **kwargs
    ) -> Dict[str, Any]:
        """ç»Ÿä¸€æ¨ç†æµæ°´çº¿å›é€€æœºåˆ¶"""
        
        # ä½¿ç”¨ç»Ÿä¸€æµæ°´çº¿ï¼Œå³ä½¿éƒ¨åˆ†ç»„ä»¶ä¸å¯ç”¨
        logger.warning("ä½¿ç”¨ç»Ÿä¸€æ¨ç†æµæ°´çº¿å›é€€æœºåˆ¶")
        
        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'unified_rec_{i}',
                'score': max(0.1, 0.8 - i * 0.1),
                'position': i + 1,
                'reason': 'ç»Ÿä¸€æ¨ç†æµæ°´çº¿å›é€€'
            })
        
        return {
            'user_id': user_id,
            'session_id': session_id,
            'recommendations': recommendations,
            'engine_type': 'unified_fallback',
            'warning': 'ç»Ÿä¸€æµæ°´çº¿éƒ¨åˆ†ç»„ä»¶ä¸å¯ç”¨'
        }
    
    def _build_tensorrt_inputs(self, user_behaviors: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """æ„å»ºTensorRTè¾“å…¥"""
        
        batch_size = 1
        seq_len = min(len(user_behaviors), 512)
        
        # æ„å»ºinput_ids
        input_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)
        for i, behavior in enumerate(user_behaviors[:seq_len]):
            video_id = behavior.get('video_id', f'unknown_{i}')
            token_id = abs(hash(str(video_id))) % 50000
            input_ids[0, i] = token_id
        
        # æ„å»ºattention_mask
        attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
        
        # æ„å»ºdense_features
        dense_features = torch.zeros(batch_size, 1024, dtype=torch.float32)
        if user_behaviors:
            # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
            avg_watch_pct = np.mean([b.get('watch_percentage', 0) for b in user_behaviors])
            like_rate = np.mean([1 if b.get('is_liked', False) else 0 for b in user_behaviors])
            
            dense_features[0, 0] = len(user_behaviors) / 100.0
            dense_features[0, 1] = avg_watch_pct
            dense_features[0, 2] = like_rate
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'dense_features': dense_features,
        }
    
    def _parse_tensorrt_outputs(
        self,
        outputs: Dict[str, torch.Tensor],
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """è§£æTensorRTè¾“å‡ºä¸ºæ¨èç»“æœ"""
        
        recommendations = []
        
        # ä»logitsä¸­æå–æ¨è
        if 'logits' in outputs:
            logits = outputs['logits']
            if logits.dim() >= 2:
                # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
                last_logits = logits[0, -1, :] if logits.dim() == 3 else logits[0, :]
                top_k_values, top_k_indices = torch.topk(last_logits, k=num_recommendations)
                
                for i in range(num_recommendations):
                    recommendations.append({
                        'video_id': f'trt_rec_{top_k_indices[i].item()}',
                        'score': float(torch.softmax(top_k_values, dim=0)[i].item()),
                        'position': i + 1,
                        'reason': 'TensorRTä¼˜åŒ–æ¨ç†'
                    })
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¾“å‡ºï¼Œç”Ÿæˆé»˜è®¤æ¨è
        if not recommendations:
            for i in range(num_recommendations):
                recommendations.append({
                    'video_id': f'trt_default_{i}',
                    'score': max(0.1, 0.8 - i * 0.1),
                    'position': i + 1,
                    'reason': 'TensorRTé»˜è®¤æ¨è'
                })
        
        return recommendations
    
    def _update_performance_stats(self, pipeline_stage: str, inference_time: float):
        """æ›´æ–°ç»Ÿä¸€æµæ°´çº¿æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats[f'{pipeline_stage}_total_time'] += inference_time
        self.performance_stats[f'{pipeline_stage}_count'] += 1
        
        if self.performance_stats[f'{pipeline_stage}_count'] > 0:
            self.performance_stats[f'{pipeline_stage}_avg_time'] = (
                self.performance_stats[f'{pipeline_stage}_total_time'] / 
                self.performance_stats[f'{pipeline_stage}_count']
            )
    
    async def batch_infer(
        self,
        requests: List[Dict[str, Any]],
        **kwargs
    ) -> List[Dict[str, Any]]:
        """ç»Ÿä¸€æ¨ç†æµæ°´çº¿æ‰¹é‡å¤„ç†"""
        
        # ä½¿ç”¨ç»Ÿä¸€æµæ°´çº¿è¿›è¡Œæ‰¹é‡æ¨ç†
        results = []
        for request in requests:
            result = self.infer_with_unified_pipeline(**request, **kwargs)
            results.append(result)
        
        return results
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        
        stats = {
            'framework_availability': self.framework_availability.copy(),
            'performance_stats': dict(self.performance_stats),
            'inference_counts': dict(self.inference_count),
            'total_inferences': sum(self.inference_count.values()),
            'pipeline_type': 'unified_hstu_onnx_tensorrt_vllm'
        }
        
        # æ·»åŠ å„æ¡†æ¶çš„è¯¦ç»†ç»Ÿè®¡
        if self.framework_availability['vllm'] and self.vllm_engine:
            stats['vllm_stats'] = self.vllm_engine.get_engine_stats()
        
        if self.framework_availability['tensorrt'] and self.tensorrt_engine:
            stats['tensorrt_stats'] = self.tensorrt_engine.get_engine_info()
        
        if self.framework_availability['cache'] and self.embedding_cache:
            stats['cache_stats'] = self.embedding_cache.get_cache_info()
        
        # æ·»åŠ Tritonç®—å­ç»Ÿè®¡
        if self.framework_availability['triton_ops'] and self.triton_manager:
            stats['triton_stats'] = self.triton_manager.get_operator_stats()
        
        # æ·»åŠ HSTUç‰¹å¾å¤„ç†ç»Ÿè®¡
        if self.framework_availability['hstu'] and self.hstu_feature_processor:
            stats['hstu_feature_stats'] = {
                'feature_processor_available': True,
                'vocabulary_size': len(getattr(self.hstu_feature_processor, 'video_id_to_token', {}))
            }
        
        return stats
    
    def benchmark_unified_pipeline(
        self,
        test_behaviors: List[Dict[str, Any]],
        num_iterations: int = 10
    ) -> Dict[str, Any]:
        """å¯¹ç»Ÿä¸€æ¨ç†æµæ°´çº¿è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        
        results = {'benchmark_results': {'unified_pipeline': {}}}
        
        logger.info("å¼€å§‹ç»Ÿä¸€æ¨ç†æµæ°´çº¿åŸºå‡†æµ‹è¯•")
        
        times = []
        for i in range(num_iterations):
            start_time = time.time()
            
            result = self.infer_with_unified_pipeline(
                user_id=f"bench_user_{i}",
                session_id=f"bench_session_{i}",
                user_behaviors=test_behaviors,
                num_recommendations=5
            )
            
            end_time = time.time()
            
            if 'error' not in result:
                times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        if times:
            avg_time = np.mean(times)
            min_time = np.min(times)
            max_time = np.max(times)
            std_time = np.std(times)
            throughput = len(test_behaviors) / (avg_time / 1000)  # behaviors/second
            
            results['benchmark_results']['unified_pipeline'] = {
                'avg_latency_ms': avg_time,
                'min_latency_ms': min_time,
                'max_latency_ms': max_time,
                'std_latency_ms': std_time,
                'throughput_behaviors_per_sec': throughput,
                'successful_runs': len(times),
                'total_runs': num_iterations,
            }
        
        return results


def create_integrated_controller(config: Dict[str, Any]) -> OpenSourceFrameworkController:
    """åˆ›å»ºé›†æˆæ§åˆ¶å™¨"""
    
    controller = OpenSourceFrameworkController(config)
    logger.info(f"âœ… å¼€æºæ¡†æ¶é›†æˆæ§åˆ¶å™¨åˆ›å»ºæˆåŠŸ")
    
    return controller


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    test_config = {
        'hstu': {
            'vocab_size': 50000,
            'd_model': 1024,
            'num_layers': 12,
            'num_heads': 16,
            'max_seq_len': 2048,
        },
        'vllm': {
            'tensor_parallel_size': 1,
            'gpu_memory_utilization': 0.8,
            'max_num_seqs': 64,
            'dtype': 'float16',
        },
        'tensorrt': {
            'precision': 'fp16',
            'max_batch_size': 8,
            'optimization_level': 5,
        },
        'custom_operators': {
            'cache_size': 1000,
            'enable_benchmarking': True,
        },
        'intelligent_cache': {
            'gpu_cache_size': 4096,
            'embedding_dim': 1024,
            'enable_prediction': True,
        }
    }
    
    # åˆ›å»ºæ§åˆ¶å™¨
    controller = create_integrated_controller(test_config)
    
    # æµ‹è¯•æ¨ç†
    test_behaviors = [
        {'video_id': 'video_1', 'watch_duration': 120, 'is_liked': True, 'category': 'tech'},
        {'video_id': 'video_2', 'watch_duration': 90, 'is_liked': False, 'category': 'music'},
        {'video_id': 'video_3', 'watch_duration': 200, 'is_liked': True, 'category': 'tech'},
    ]
    
    result = controller.infer_with_optimal_strategy(
        user_id="test_user",
        session_id="test_session",
        user_behaviors=test_behaviors,
        num_recommendations=5
    )
    
    print(f"æ¨ç†ç­–ç•¥: {result.get('inference_strategy', 'unknown')}")
    print(f"æ¨ç†æ—¶é—´: {result.get('inference_time_ms', 0):.2f}ms")
    print(f"æ¨èæ•°é‡: {len(result.get('recommendations', []))}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = controller.get_comprehensive_stats()
    print(f"æ¡†æ¶å¯ç”¨æ€§: {stats['framework_availability']}")
    print(f"æ€»æ¨ç†æ¬¡æ•°: {stats['total_inferences']}")