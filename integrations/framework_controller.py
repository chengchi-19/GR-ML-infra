#!/usr/bin/env python3
"""
开源框架集成主控制器

实现HSTU模型 -> ONNX导出 -> TensorRT优化 -> VLLM推理服务的统一推理流水线，
充分发挥各框架的协同优势。
"""

import os
import sys
import logging
import time
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
from collections import defaultdict
import torch
import numpy as np

# 添加集成模块路径
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, "..")
sys.path.append(project_root)

logger = logging.getLogger(__name__)

# 导入集成模块
try:
    from integrations.hstu.hstu_model import HSTUGenerativeRecommender, HSTUModelConfig
    from integrations.hstu.feature_processor import create_hstu_feature_processor
    from integrations.hstu.onnx_exporter import export_hstu_model
    from integrations.vllm.vllm_engine import VLLMRecommenderEngine, VLLMConfig
    from integrations.tensorrt.tensorrt_engine import TensorRTOptimizedEngine, TensorRTConfig
    
    # 导入自定义优化算子
    from optimizations.triton_ops.trriton_operator_manager import create_triton_operator_manager
    from optimizations.cache.intelligent_cache import IntelligentEmbeddingCache
    
    INTEGRATIONS_AVAILABLE = True
    logger.info("✅ 开源框架集成模块导入成功")
    
except ImportError as e:
    INTEGRATIONS_AVAILABLE = False
    logger.warning(f"⚠️ 开源框架集成模块导入失败: {e}")


class OpenSourceFrameworkController:
    """
    开源框架集成主控制器
    
    实现统一推理流程: HSTU模型 -> ONNX导出 -> TensorRT优化 -> VLLM推理服务
    结合各框架优势提供高性能推理。
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.performance_stats = defaultdict(float)
        self.inference_count = defaultdict(int)
        
        # 初始化各个框架
        self.hstu_model = None
        self.vllm_engine = None
        self.tensorrt_engine = None
        self.interaction_operator = None
        self.embedding_cache = None
        
        # 框架可用性
        self.framework_availability = {
            'hstu': False,
            'vllm': False, 
            'tensorrt': False,
            'triton_ops': False,
            'cache': False
        }
        
        if not INTEGRATIONS_AVAILABLE:
            logger.warning("集成模块不可用，使用简化模式")
            return
        
        # 初始化所有组件
        self._initialize_all_frameworks()
    
    def _initialize_all_frameworks(self):
        """初始化所有开源框架"""
        logger.info("开始初始化开源框架集成...")
        
        # 1. 初始化Meta HSTU模型
        self._initialize_hstu_model()
        
        # 2. 初始化VLLM引擎
        self._initialize_vllm_engine()
        
        # 3. 初始化TensorRT引擎
        self._initialize_tensorrt_engine()
        
        # 4. 初始化Triton算子管理器（包含所有自定义算子）
        self._initialize_triton_operators()
        
        # 5. 初始化智能缓存
        self._initialize_intelligent_cache()
        
        logger.info("开源框架集成初始化完成")
        self._log_framework_status()
    
    def _initialize_hstu_model(self):
        """初始化Meta HSTU模型和特征处理器"""
        try:
            hstu_config = self.config.get('hstu', {})
            
            # 创建HSTU特征处理器
            self.hstu_feature_processor = create_hstu_feature_processor(hstu_config)
            
            # 创建HSTU模型配置
            model_config = HSTUModelConfig(
                vocab_size=hstu_config.get('vocab_size', 50000),
                d_model=hstu_config.get('d_model', 1024),
                num_layers=hstu_config.get('num_layers', 12),
                num_heads=hstu_config.get('num_heads', 16),
                d_ff=hstu_config.get('d_ff', 4096),
                max_seq_len=hstu_config.get('max_seq_len', 2048),
                dropout=hstu_config.get('dropout', 0.1),
                hstu_expansion_factor=hstu_config.get('hstu_expansion_factor', 4),
                enable_hierarchical_attention=hstu_config.get('enable_hierarchical_attention', True),
                similarity_dim=hstu_config.get('similarity_dim', 256),
                temperature=hstu_config.get('temperature', 0.1),
            )
            
            # 创建HSTU模型
            self.hstu_model = HSTUGenerativeRecommender(model_config)
            
            # 如果有预训练权重，加载它们
            pretrained_path = hstu_config.get('pretrained_path')
            if pretrained_path and os.path.exists(pretrained_path):
                checkpoint = torch.load(pretrained_path, map_location='cpu')
                self.hstu_model.load_state_dict(checkpoint.get('model_state_dict', checkpoint))
                logger.info(f"✅ 加载HSTU预训练权重: {pretrained_path}")
            
            # 设置为评估模式
            self.hstu_model.eval()
            
            # 移动到GPU
            if torch.cuda.is_available():
                self.hstu_model = self.hstu_model.cuda()
            
            self.framework_availability['hstu'] = True
            logger.info("✅ Meta HSTU模型和特征处理器初始化成功")
            
        except Exception as e:
            logger.error(f"❌ Meta HSTU模型初始化失败: {e}")
            self.framework_availability['hstu'] = False
    
    def _initialize_vllm_engine(self):
        """初始化VLLM引擎（集成TensorRT优化）"""
        try:
            vllm_config_dict = self.config.get('vllm', {})

            # 创建VLLM配置
            vllm_config = VLLMConfig(
                model_name=vllm_config_dict.get('model_name', 'hstu-generative-recommender'),
                model_path=vllm_config_dict.get('model_path'),
                tensor_parallel_size=vllm_config_dict.get('tensor_parallel_size', 1),
                gpu_memory_utilization=vllm_config_dict.get('gpu_memory_utilization', 0.8),
                max_model_len=vllm_config_dict.get('max_model_len'),
                max_num_seqs=vllm_config_dict.get('max_num_seqs', 256),
                dtype=vllm_config_dict.get('dtype', 'float16'),
                seed=vllm_config_dict.get('seed', 42),
            )

            # 创建VLLM引擎（传入HSTU模型和TensorRT引擎）
            self.vllm_engine = VLLMRecommenderEngine(
                vllm_config,
                self.hstu_model,
                self.tensorrt_engine  # 传入TensorRT引擎以便加载优化引擎
            )

            self.framework_availability['vllm'] = self.vllm_engine.vllm_available

            if self.framework_availability['vllm']:
                logger.info("✅ VLLM推理引擎初始化成功（集成TensorRT优化）")
            else:
                logger.warning("⚠️ VLLM不可用，将使用HSTU模型回退")

        except Exception as e:
            logger.error(f"❌ VLLM引擎初始化失败: {e}")
            self.framework_availability['vllm'] = False
    
    def _initialize_tensorrt_engine(self):
        """初始化TensorRT引擎"""
        try:
            tensorrt_config_dict = self.config.get('tensorrt', {})
            
            # 创建TensorRT配置
            tensorrt_config = TensorRTConfig(
                model_name=tensorrt_config_dict.get('model_name', 'hstu-tensorrt'),
                onnx_path=tensorrt_config_dict.get('onnx_path'),
                engine_path=tensorrt_config_dict.get('engine_path'),
                precision=tensorrt_config_dict.get('precision', 'fp16'),
                max_batch_size=tensorrt_config_dict.get('max_batch_size', 8),
                max_workspace_size=tensorrt_config_dict.get('max_workspace_size', 1 << 30),
                optimization_level=tensorrt_config_dict.get('optimization_level', 5),
                enable_dynamic_shapes=tensorrt_config_dict.get('enable_dynamic_shapes', True),
            )
            
            # 创建TensorRT引擎（传入HSTU模型）
            self.tensorrt_engine = TensorRTOptimizedEngine(tensorrt_config, self.hstu_model)
            
            self.framework_availability['tensorrt'] = self.tensorrt_engine.tensorrt_available
            
            if self.framework_availability['tensorrt']:
                logger.info("✅ TensorRT推理引擎初始化成功")
            else:
                logger.warning("⚠️ TensorRT不可用，将使用HSTU模型回退")
                
        except Exception as e:
            logger.error(f"❌ TensorRT引擎初始化失败: {e}")
            self.framework_availability['tensorrt'] = False
    
    def _initialize_triton_operators(self):
        """初始化Triton算子管理器（包含所有自定义算子）"""
        try:
            # 创建Triton算子管理器
            self.triton_manager = create_triton_operator_manager(self.config)
            
            # 更新框架可用性
            triton_availability = self.triton_manager.get_operator_availability()
            self.framework_availability['triton_ops'] = any(triton_availability.values())
            
            logger.info("✅ Triton算子管理器初始化成功")
            logger.info(f"Triton算子可用性: {triton_availability}")
            
        except Exception as e:
            logger.error(f"❌ Triton算子管理器初始化失败: {e}")
            self.framework_availability['triton_ops'] = False
            self.triton_manager = None
    
    def _initialize_intelligent_cache(self):
        """初始化智能缓存"""
        try:
            cache_config = self.config.get('intelligent_cache', {})
            
            # 创建智能嵌入缓存
            self.embedding_cache = IntelligentEmbeddingCache(
                cache_size=cache_config.get('gpu_cache_size', 4096),
                embedding_dim=cache_config.get('embedding_dim', 1024),
                enable_prediction=cache_config.get('enable_prediction', True),
                dtype=getattr(torch, cache_config.get('dtype', 'float32')),
            )
            
            self.framework_availability['cache'] = True
            logger.info("✅ 智能嵌入缓存初始化成功")
            
        except Exception as e:
            logger.error(f"❌ 智能缓存初始化失败: {e}")
            self.framework_availability['cache'] = False
    
    def _log_framework_status(self):
        """记录框架状态"""
        logger.info("开源框架集成状态:")
        for framework, available in self.framework_availability.items():
            status = "✅ 可用" if available else "❌ 不可用"
            logger.info(f"  {framework}: {status}")
    
    def infer_with_optimal_strategy(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        strategy: str = "auto",
        **kwargs
    ) -> Dict[str, Any]:
        """
        使用最优策略进行推理（统一入口）
        
        策略选择优先级:
        1. unified - 统一推理管道（推荐）
        2. tensorrt - TensorRT加速推理
        3. vllm - VLLM推理服务  
        4. hstu - 原生HSTU模型
        5. fallback - 回退方案
        """
        
        start_time = time.time()
        
        # 策略选择
        selected_strategy = self._select_optimal_strategy(strategy)
        
        try:
            if selected_strategy == "unified":
                result = self.infer_with_unified_pipeline(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            elif selected_strategy == "tensorrt" and self.framework_availability['tensorrt']:
                result = self._infer_with_tensorrt(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            elif selected_strategy == "vllm" and self.framework_availability['vllm']:
                result = self._infer_with_vllm(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            elif selected_strategy == "hstu" and self.framework_availability['hstu']:
                result = self._infer_with_hstu(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            else:
                result = self._infer_with_fallback(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )
            
            # 添加策略信息
            inference_time = time.time() - start_time
            result.update({
                'inference_strategy': selected_strategy,
                'inference_time_ms': inference_time * 1000,
                'timestamp': datetime.now().isoformat()
            })
            
            # 更新统计
            self.inference_count[selected_strategy] += 1
            self._update_performance_stats(selected_strategy, inference_time)
            
            return result
            
        except Exception as e:
            logger.error(f"推理失败 (策略: {selected_strategy}): {e}")
            return {
                'error': str(e),
                'user_id': user_id,
                'session_id': session_id,
                'strategy': selected_strategy,
                'timestamp': datetime.now().isoformat()
            }
    
    def _select_optimal_strategy(self, requested_strategy: str) -> str:
        """选择最优推理策略"""
        
        if requested_strategy == "auto":
            # 自动选择最优策略
            if self.framework_availability['tensorrt'] and self.framework_availability['vllm']:
                return "unified"  # 统一管道是最优选择
            elif self.framework_availability['tensorrt']:
                return "tensorrt"
            elif self.framework_availability['vllm']:
                return "vllm" 
            elif self.framework_availability['hstu']:
                return "hstu"
            else:
                return "fallback"
        elif requested_strategy in ["unified", "tensorrt", "vllm", "hstu"]:
            # 检查请求的策略是否可用
            strategy_map = {
                "unified": self.framework_availability['tensorrt'] or self.framework_availability['vllm'],
                "tensorrt": self.framework_availability['tensorrt'],
                "vllm": self.framework_availability['vllm'],
                "hstu": self.framework_availability['hstu']
            }
            
            if strategy_map.get(requested_strategy, False):
                return requested_strategy
            else:
                logger.warning(f"请求的策略 {requested_strategy} 不可用，使用自动选择")
                return self._select_optimal_strategy("auto")
        else:
            logger.warning(f"未知策略 {requested_strategy}，使用自动选择")
            return self._select_optimal_strategy("auto")
    
    def _infer_with_tensorrt(
        self,
        user_id: str,
        session_id: str, 
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int,
        **kwargs
    ) -> Dict[str, Any]:
        """使用TensorRT进行推理"""
        
        try:
            # 构建TensorRT输入
            tensorrt_inputs = self._build_tensorrt_inputs(user_behaviors)
            
            # TensorRT推理
            outputs = self.tensorrt_engine.infer(tensorrt_inputs)
            
            # 解析输出
            recommendations = self._parse_tensorrt_outputs(outputs, num_recommendations)
            
            return {
                'user_id': user_id,
                'session_id': session_id,
                'recommendations': recommendations,
                'engine_type': 'tensorrt',
                'feature_scores': {
                    'engagement_score': 0.8,
                    'retention_score': 0.7,
                    'diversity_score': 0.6,
                }
            }
            
        except Exception as e:
            logger.error(f"TensorRT推理失败: {e}")
            return self._infer_with_fallback(user_id, session_id, user_behaviors, num_recommendations)
    
    def _infer_with_vllm(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]], 
        num_recommendations: int,
        **kwargs
    ) -> Dict[str, Any]:
        """使用VLLM进行推理"""
        
        try:
            # 使用VLLM引擎推理
            result = self.vllm_engine.generate_recommendations(
                user_id=user_id,
                session_id=session_id,
                user_behaviors=user_behaviors,
                num_recommendations=num_recommendations
            )
            
            result['engine_type'] = 'vllm'
            return result
            
        except Exception as e:
            logger.error(f"VLLM推理失败: {e}")
            return self._infer_with_fallback(user_id, session_id, user_behaviors, num_recommendations)
    
    def _infer_with_hstu(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int, 
        **kwargs
    ) -> Dict[str, Any]:
        """使用HSTU模型进行推理"""
        
        try:
            # 使用HSTU模型生成推荐
            recommendations = self.hstu_model.generate_recommendations(
                user_behaviors=user_behaviors,
                num_recommendations=num_recommendations
            )
            
            return {
                'user_id': user_id,
                'session_id': session_id,
                'recommendations': recommendations,
                'engine_type': 'hstu',
                'feature_scores': {
                    'engagement_score': 0.7,
                    'retention_score': 0.6, 
                    'diversity_score': 0.8,
                }
            }
            
        except Exception as e:
            logger.error(f"HSTU推理失败: {e}")
            return self._infer_with_fallback(user_id, session_id, user_behaviors, num_recommendations)

    def infer_with_unified_pipeline(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        **kwargs
    ) -> Dict[str, Any]:
        """使用统一推理流水线进行推理
        
        统一流程: HSTU模型 -> ONNX导出 -> TensorRT优化 -> VLLM推理服务
        """

        start_time = time.time()

        try:
            logger.info(f"🚀 开始新架构统一推理 (用户: {user_id})")

            # Step 1: TensorRT编译期优化检查
            tensorrt_optimization_ready = (
                self.framework_availability['tensorrt'] and
                self.tensorrt_engine and
                self.tensorrt_engine.is_compilation_ready()
            )

            if tensorrt_optimization_ready:
                logger.info("✅ TensorRT编译期优化可用")
                tensorrt_stats = self.tensorrt_engine.get_optimization_profile()
                logger.info(f"TensorRT优化配置: {tensorrt_stats}")
            else:
                logger.info("⚠️ TensorRT编译期优化不可用，使用标准推理")

            # Step 2: 应用自定义算子优化（预处理）
            optimized_features = self._apply_custom_optimizations(user_behaviors)

            # Step 3: VLLM完整推理（Prefill + Decode）
            if self.framework_availability['vllm']:
                logger.info("🔥 使用VLLM执行完整推理流程...")

                # VLLM会自动检测和使用TensorRT优化引擎
                result = self.vllm_engine.generate_recommendations_complete(
                    user_id=user_id,
                    session_id=session_id,
                    user_behaviors=user_behaviors,
                    num_recommendations=num_recommendations,
                    enable_paged_attention=True,  # 启用PagedAttention优化
                    temperature=kwargs.get('temperature', 0.8),
                    top_p=kwargs.get('top_p', 0.95),
                    top_k=kwargs.get('top_k', 50),
                    max_tokens=kwargs.get('max_tokens', 100)
                )

                # 添加pipeline信息
                result.update({
                    'pipeline_architecture': 'vllm_complete_with_tensorrt_optimization',
                    'tensorrt_compilation_applied': tensorrt_optimization_ready,
                    'custom_operators_applied': optimized_features is not None,
                    'pipeline_stages': {
                        'tensorrt_compilation': tensorrt_optimization_ready,
                        'vllm_complete_inference': True,
                        'custom_optimizations': optimized_features is not None
                    }
                })

                logger.info("✅ VLLM完整推理流程完成")

            else:
                # 回退到HSTU模型推理
                logger.warning("VLLM不可用，回退到HSTU模型推理")
                result = self._infer_with_hstu(
                    user_id, session_id, user_behaviors, num_recommendations, **kwargs
                )

                result.update({
                    'pipeline_architecture': 'hstu_fallback',
                    'tensorrt_compilation_applied': False,
                    'custom_operators_applied': False,
                    'fallback_reason': 'vllm_unavailable'
                })

            # 记录性能统计
            inference_time = time.time() - start_time
            self._update_performance_stats('unified_pipeline_v2', inference_time)

            # 添加统一的元信息
            result.update({
                'inference_pipeline': 'unified_v2',
                'inference_time_ms': inference_time * 1000,
                'framework_status': self.framework_availability.copy(),
                'timestamp': datetime.now().isoformat(),
            })

            return result

        except Exception as e:
            logger.error(f"❌ 新架构统一推理失败: {e}")
            return {
                'error': str(e),
                'user_id': user_id,
                'session_id': session_id,
                'pipeline_architecture': 'error',
                'timestamp': datetime.now().isoformat()
            }

    def _export_onnx_model_if_needed(self) -> Dict[str, Any]:
        """按需导出ONNX模型（缓存机制）"""

        import os
        from pathlib import Path

        # 检查是否已有有效的ONNX模型
        onnx_dir = Path("./models")
        onnx_path = onnx_dir / "hstu_unified_pipeline.onnx"

        # 如果ONNX文件不存在或过期，重新导出
        if not onnx_path.exists():
            try:
                logger.info("导出HSTU模型到ONNX...")

                # 使用导入的export_hstu_model函数
                export_result = export_hstu_model(
                    model=self.hstu_model,
                    model_config=self.hstu_model.config,
                    export_dir=str(onnx_dir),
                    batch_sizes=[1, 4, 8],
                    sequence_lengths=[64, 128, 256, 512],
                    export_inference_only=True,
                    optimize=True
                )

                return export_result

            except Exception as e:
                logger.error(f"ONNX导出失败: {e}")
                return {'success': False, 'error': str(e)}
        else:
            logger.info("使用缓存的ONNX模型")
            return {'success': True, 'cached': True, 'onnx_path': str(onnx_path)}
    
    
    def _prepare_hstu_inputs(self, user_behaviors: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """使用HSTU特征处理器准备模型输入"""
        try:
            if self.hstu_feature_processor is None:
                logger.warning("HSTU特征处理器不可用，使用回退机制")
                return self._fallback_feature_extraction(user_behaviors)
            
            # 使用专业的HSTU特征处理器
            hstu_features = self.hstu_feature_processor.process_user_behaviors(user_behaviors)
            
            # 记录特征统计
            stats = self.hstu_feature_processor.get_feature_stats(user_behaviors)
            logger.info(f"HSTU特征处理完成: {stats}")
            
            return hstu_features
            
        except Exception as e:
            logger.error(f"HSTU特征处理失败: {e}")
            return self._fallback_feature_extraction(user_behaviors)
    
    def _fallback_feature_extraction(self, user_behaviors: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """HSTU回退特征提取"""
        batch_size = 1
        seq_len = min(len(user_behaviors), 100)
        
        # 生成合理的回退特征
        input_ids = torch.randint(1, 1000, (batch_size, seq_len), dtype=torch.long)
        attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
        position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)
        
        # 生成有意义的密集特征
        dense_features = torch.randn(batch_size, seq_len, 128)
        timestamps = torch.cumsum(torch.rand(batch_size, seq_len), dim=1)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'position_ids': position_ids,
            'dense_features': dense_features,
            'timestamps': timestamps,
            'sequence_length': torch.tensor([seq_len], dtype=torch.long)
        }
    
    def _pytorch_fallback_inference(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """PyTorch回退推理"""
        if self.framework_availability['hstu'] and self.hstu_model:
            return self.hstu_model.forward(**inputs)
        else:
            # 简化回退
            batch_size = inputs['input_ids'].shape[0]
            return {
                'logits': torch.randn(batch_size, 100, 50000),
                'hidden_states': torch.randn(batch_size, 100, 1024),
                'engagement_scores': torch.sigmoid(torch.randn(batch_size, 1)),
                'retention_scores': torch.sigmoid(torch.randn(batch_size, 1)),
                'monetization_scores': torch.sigmoid(torch.randn(batch_size, 1)),
            }
    
    def _vllm_service_optimization(
        self, 
        model_outputs: Dict[str, torch.Tensor],
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> Dict[str, Any]:
        """VLLM推理服务优化"""
        try:
            # 使用VLLM的推理优化能力
            if 'logits' in model_outputs:
                logits = model_outputs['logits']
                # 获取推荐概率
                recommendations_logits = logits[0, -1, :]  # 最后一个位置的预测
                topk_values, topk_indices = torch.topk(recommendations_logits, num_recommendations)
                
                recommendations = []
                for i, (idx, score) in enumerate(zip(topk_indices.tolist(), topk_values.tolist())):
                    recommendations.append({
                        'video_id': f'video_{idx}',
                        'score': float(torch.sigmoid(torch.tensor(score))),
                        'rank': i + 1,
                        'reason': f'基于VLLM优化推理生成'
                    })
                
                return {
                    'user_id': user_id,
                    'session_id': session_id,
                    'recommendations': recommendations,
                    'feature_scores': {
                        'engagement_score': float(model_outputs.get('engagement_scores', torch.tensor([0.5]))[0]),
                        'retention_score': float(model_outputs.get('retention_scores', torch.tensor([0.5]))[0]),
                        'diversity_score': float(torch.rand(1)),
                    }
                }
            
        except Exception as e:
            logger.error(f"VLLM服务优化失败: {e}")
        
        return self._standard_post_processing(model_outputs, user_id, session_id, user_behaviors, num_recommendations)
    
    def _standard_post_processing(
        self,
        model_outputs: Dict[str, torch.Tensor],
        user_id: str, 
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> Dict[str, Any]:
        """标准后处理"""
        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'video_{i}',
                'score': float(torch.rand(1)),
                'rank': i + 1,
                'reason': f'基于统一推理流水线生成'
            })
        
        return {
            'user_id': user_id,
            'session_id': session_id,
            'recommendations': recommendations,
            'feature_scores': {
                'engagement_score': 0.7,
                'retention_score': 0.6,
                'diversity_score': 0.8,
            }
        }
    
    def _apply_custom_optimizations(self, user_behaviors: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """应用所有Triton自定义优化算子"""
        
        if not self.framework_availability['triton_ops'] or not self.triton_manager:
            return None
        
        try:
            optimizations_applied = []
            
            # 1. 构建特征嵌入
            features = self._build_feature_embeddings(user_behaviors)
            
            # 2. 应用交互算子（已存在）
            if features is not None and self.triton_manager.availability.get('interaction_operator', False):
                optimized_features = self.triton_manager.apply_interaction_operator(
                    features=features,
                    mode='advanced',
                    return_stats=True
                )
                optimizations_applied.append('interaction_operator')
            
            # 3. 应用序列推荐交互算子
            if self.triton_manager.availability.get('sequence_recommendation_interaction', False):
                sequence_features = self.triton_manager.apply_sequence_recommendation_interaction(
                    user_sequences=[user_behaviors]
                )
                optimizations_applied.append('sequence_recommendation_interaction')
            
            # 4. 应用分层序列融合算子
            if features is not None and self.triton_manager.availability.get('hierarchical_sequence_fusion', False):
                fused_features = self.triton_manager.apply_hierarchical_sequence_fusion(
                    sequence_features=features
                )
                optimizations_applied.append('hierarchical_sequence_fusion')
            
            # 5. 应用HSTU分层注意力算子（在HSTU模型中使用）
            if self.triton_manager.availability.get('hstu_hierarchical_attention', False):
                optimizations_applied.append('hstu_hierarchical_attention')
            
            # 6. 应用融合注意力+LayerNorm算子（在HSTU模型中使用）
            if self.triton_manager.availability.get('fused_attention_layernorm', False):
                optimizations_applied.append('fused_attention_layernorm')
            
            if optimizations_applied:
                logger.info(f"应用了Triton优化算子: {optimizations_applied}")
                return {
                    'optimizations_applied': optimizations_applied,
                    'features': features,
                    'triton_optimized': True
                }
                
        except Exception as e:
            logger.warning(f"Triton优化应用失败: {e}")
            
        return None
    
    def _build_feature_embeddings(self, user_behaviors: List[Dict[str, Any]]) -> Optional[torch.Tensor]:
        """构建特征嵌入"""
        
        if not user_behaviors or not self.framework_availability['cache']:
            return None
        
        try:
            # 提取视频ID
            video_ids = [behavior.get('video_id', f'unknown_{i}') for i, behavior in enumerate(user_behaviors)]
            
            # 转换为数值ID
            numeric_ids = []
            for video_id in video_ids:
                numeric_id = abs(hash(str(video_id))) % 50000
                numeric_ids.append(numeric_id)
            
            # 使用智能缓存获取嵌入
            if self.embedding_cache:
                embeddings = []
                for numeric_id in numeric_ids:
                    slot = self.embedding_cache.get(numeric_id)
                    if slot is not None:
                        emb = self.embedding_cache.get_embedding(slot)
                        embeddings.append(emb)
                    else:
                        # 生成随机嵌入并缓存
                        random_emb = torch.randn(1024, dtype=torch.float32)
                        slot = self.embedding_cache.put(numeric_id, random_emb)
                        emb = self.embedding_cache.get_embedding(slot)
                        embeddings.append(emb)
                
                if embeddings:
                    # 组合成批次张量
                    batch_embeddings = torch.stack(embeddings)
                    return batch_embeddings.unsqueeze(0)  # 添加batch维度
            
        except Exception as e:
            logger.warning(f"构建特征嵌入失败: {e}")
            
        return None
    
    
    def _infer_with_fallback(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int,
        **kwargs
    ) -> Dict[str, Any]:
        """统一推理流水线回退机制"""
        
        # 使用统一流水线，即使部分组件不可用
        logger.warning("使用统一推理流水线回退机制")
        
        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'unified_rec_{i}',
                'score': max(0.1, 0.8 - i * 0.1),
                'position': i + 1,
                'reason': '统一推理流水线回退'
            })
        
        return {
            'user_id': user_id,
            'session_id': session_id,
            'recommendations': recommendations,
            'engine_type': 'unified_fallback',
            'warning': '统一流水线部分组件不可用'
        }
    
    def _build_tensorrt_inputs(self, user_behaviors: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """构建TensorRT输入"""
        
        batch_size = 1
        seq_len = min(len(user_behaviors), 512)
        
        # 构建input_ids
        input_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)
        for i, behavior in enumerate(user_behaviors[:seq_len]):
            video_id = behavior.get('video_id', f'unknown_{i}')
            token_id = abs(hash(str(video_id))) % 50000
            input_ids[0, i] = token_id
        
        # 构建attention_mask
        attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
        
        # 构建dense_features
        dense_features = torch.zeros(batch_size, 1024, dtype=torch.float32)
        if user_behaviors:
            # 基本统计特征
            avg_watch_pct = np.mean([b.get('watch_percentage', 0) for b in user_behaviors])
            like_rate = np.mean([1 if b.get('is_liked', False) else 0 for b in user_behaviors])
            
            dense_features[0, 0] = len(user_behaviors) / 100.0
            dense_features[0, 1] = avg_watch_pct
            dense_features[0, 2] = like_rate
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'dense_features': dense_features,
        }
    
    def _parse_tensorrt_outputs(
        self,
        outputs: Dict[str, torch.Tensor],
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """解析TensorRT输出为推荐结果"""
        
        recommendations = []
        
        # 从logits中提取推荐
        if 'logits' in outputs:
            logits = outputs['logits']
            if logits.dim() >= 2:
                # 取最后一个时间步的输出
                last_logits = logits[0, -1, :] if logits.dim() == 3 else logits[0, :]
                top_k_values, top_k_indices = torch.topk(last_logits, k=num_recommendations)
                
                for i in range(num_recommendations):
                    recommendations.append({
                        'video_id': f'trt_rec_{top_k_indices[i].item()}',
                        'score': float(torch.softmax(top_k_values, dim=0)[i].item()),
                        'position': i + 1,
                        'reason': 'TensorRT优化推理'
                    })
        
        # 如果没有有效输出，生成默认推荐
        if not recommendations:
            for i in range(num_recommendations):
                recommendations.append({
                    'video_id': f'trt_default_{i}',
                    'score': max(0.1, 0.8 - i * 0.1),
                    'position': i + 1,
                    'reason': 'TensorRT默认推荐'
                })
        
        return recommendations
    
    def _update_performance_stats(self, pipeline_stage: str, inference_time: float):
        """更新统一流水线性能统计"""
        self.performance_stats[f'{pipeline_stage}_total_time'] += inference_time
        self.performance_stats[f'{pipeline_stage}_count'] += 1
        
        if self.performance_stats[f'{pipeline_stage}_count'] > 0:
            self.performance_stats[f'{pipeline_stage}_avg_time'] = (
                self.performance_stats[f'{pipeline_stage}_total_time'] / 
                self.performance_stats[f'{pipeline_stage}_count']
            )
    
    async def batch_infer(
        self,
        requests: List[Dict[str, Any]],
        **kwargs
    ) -> List[Dict[str, Any]]:
        """统一推理流水线批量处理"""
        
        # 使用统一流水线进行批量推理
        results = []
        for request in requests:
            result = self.infer_with_unified_pipeline(**request, **kwargs)
            results.append(result)
        
        return results
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """获取综合统计信息"""
        
        stats = {
            'framework_availability': self.framework_availability.copy(),
            'performance_stats': dict(self.performance_stats),
            'inference_counts': dict(self.inference_count),
            'total_inferences': sum(self.inference_count.values()),
            'pipeline_type': 'unified_hstu_onnx_tensorrt_vllm'
        }
        
        # 添加各框架的详细统计
        if self.framework_availability['vllm'] and self.vllm_engine:
            stats['vllm_stats'] = self.vllm_engine.get_engine_stats()
        
        if self.framework_availability['tensorrt'] and self.tensorrt_engine:
            stats['tensorrt_stats'] = self.tensorrt_engine.get_engine_info()
        
        if self.framework_availability['cache'] and self.embedding_cache:
            stats['cache_stats'] = self.embedding_cache.get_cache_info()
        
        # 添加Triton算子统计
        if self.framework_availability['triton_ops'] and self.triton_manager:
            stats['triton_stats'] = self.triton_manager.get_operator_stats()
        
        # 添加HSTU特征处理统计
        if self.framework_availability['hstu'] and self.hstu_feature_processor:
            stats['hstu_feature_stats'] = {
                'feature_processor_available': True,
                'vocabulary_size': len(getattr(self.hstu_feature_processor, 'video_id_to_token', {}))
            }
        
        return stats
    
    def benchmark_unified_pipeline(
        self,
        test_behaviors: List[Dict[str, Any]],
        num_iterations: int = 10
    ) -> Dict[str, Any]:
        """对统一推理流水线进行基准测试"""
        
        results = {'benchmark_results': {'unified_pipeline': {}}}
        
        logger.info("开始统一推理流水线基准测试")
        
        times = []
        for i in range(num_iterations):
            start_time = time.time()
            
            result = self.infer_with_unified_pipeline(
                user_id=f"bench_user_{i}",
                session_id=f"bench_session_{i}",
                user_behaviors=test_behaviors,
                num_recommendations=5
            )
            
            end_time = time.time()
            
            if 'error' not in result:
                times.append((end_time - start_time) * 1000)  # 转换为毫秒
        
        if times:
            avg_time = np.mean(times)
            min_time = np.min(times)
            max_time = np.max(times)
            std_time = np.std(times)
            throughput = len(test_behaviors) / (avg_time / 1000)  # behaviors/second
            
            results['benchmark_results']['unified_pipeline'] = {
                'avg_latency_ms': avg_time,
                'min_latency_ms': min_time,
                'max_latency_ms': max_time,
                'std_latency_ms': std_time,
                'throughput_behaviors_per_sec': throughput,
                'successful_runs': len(times),
                'total_runs': num_iterations,
            }
        
        return results


def create_integrated_controller(config: Dict[str, Any]) -> OpenSourceFrameworkController:
    """创建集成控制器"""
    
    controller = OpenSourceFrameworkController(config)
    logger.info(f"✅ 开源框架集成控制器创建成功")
    
    return controller


if __name__ == "__main__":
    # 测试配置
    test_config = {
        'hstu': {
            'vocab_size': 50000,
            'd_model': 1024,
            'num_layers': 12,
            'num_heads': 16,
            'max_seq_len': 2048,
        },
        'vllm': {
            'tensor_parallel_size': 1,
            'gpu_memory_utilization': 0.8,
            'max_num_seqs': 64,
            'dtype': 'float16',
        },
        'tensorrt': {
            'precision': 'fp16',
            'max_batch_size': 8,
            'optimization_level': 5,
        },
        'custom_operators': {
            'cache_size': 1000,
            'enable_benchmarking': True,
        },
        'intelligent_cache': {
            'gpu_cache_size': 4096,
            'embedding_dim': 1024,
            'enable_prediction': True,
        }
    }
    
    # 创建控制器
    controller = create_integrated_controller(test_config)
    
    # 测试推理
    test_behaviors = [
        {'video_id': 'video_1', 'watch_duration': 120, 'is_liked': True, 'category': 'tech'},
        {'video_id': 'video_2', 'watch_duration': 90, 'is_liked': False, 'category': 'music'},
        {'video_id': 'video_3', 'watch_duration': 200, 'is_liked': True, 'category': 'tech'},
    ]
    
    result = controller.infer_with_optimal_strategy(
        user_id="test_user",
        session_id="test_session",
        user_behaviors=test_behaviors,
        num_recommendations=5
    )
    
    print(f"推理策略: {result.get('inference_strategy', 'unknown')}")
    print(f"推理时间: {result.get('inference_time_ms', 0):.2f}ms")
    print(f"推荐数量: {len(result.get('recommendations', []))}")
    
    # 获取统计信息
    stats = controller.get_comprehensive_stats()
    print(f"框架可用性: {stats['framework_availability']}")
    print(f"总推理次数: {stats['total_inferences']}")