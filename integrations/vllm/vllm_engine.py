#!/usr/bin/env python3
"""
VLLM推理优化框架集成适配器

基于vllm-project/vllm开源框架，提供高性能LLM推理服务，
集成PagedAttention和Continuous Batching优化技术。
"""

import os
import sys
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union, AsyncGenerator
import torch

# 添加VLLM路径
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, "..", "..")
vllm_path = os.path.join(project_root, "external", "vllm")
sys.path.append(vllm_path)

logger = logging.getLogger(__name__)

try:
    # 导入VLLM核心组件
    from vllm import LLM, SamplingParams
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.utils import Counter
    from vllm.outputs import RequestOutput
    from vllm.transformers_utils.tokenizer import get_tokenizer
    
    VLLM_AVAILABLE = True
    logger.info("✅ VLLM框架导入成功")
    
except ImportError as e:
    VLLM_AVAILABLE = False
    logger.warning(f"⚠️ VLLM框架导入失败: {e}")


class VLLMConfig:
    """VLLM引擎配置"""
    def __init__(
        self,
        model_name: str = "hstu-generative-recommender",
        model_path: Optional[str] = None,
        tensor_parallel_size: int = 1,
        pipeline_parallel_size: int = 1,
        max_model_len: Optional[int] = None,
        gpu_memory_utilization: float = 0.9,
        swap_space: int = 4,
        max_num_seqs: int = 256,
        max_num_batched_tokens: Optional[int] = None,
        block_size: int = 16,
        seed: int = 0,
        trust_remote_code: bool = False,
        revision: Optional[str] = None,
        tokenizer: Optional[str] = None,
        tokenizer_revision: Optional[str] = None,
        dtype: str = "float16",
        quantization: Optional[str] = None,
        enforce_eager: bool = False,
        max_context_len_to_capture: int = 8192,
        disable_custom_all_reduce: bool = False,
        enable_chunked_prefill: bool = False,
        max_num_on_the_fly_seq_groups: int = 64,
        **kwargs
    ):
        self.model_name = model_name
        self.model_path = model_path
        self.tensor_parallel_size = tensor_parallel_size
        self.pipeline_parallel_size = pipeline_parallel_size
        self.max_model_len = max_model_len
        self.gpu_memory_utilization = gpu_memory_utilization
        self.swap_space = swap_space
        self.max_num_seqs = max_num_seqs
        self.max_num_batched_tokens = max_num_batched_tokens
        self.block_size = block_size
        self.seed = seed
        self.trust_remote_code = trust_remote_code
        self.revision = revision
        self.tokenizer = tokenizer
        self.tokenizer_revision = tokenizer_revision
        self.dtype = dtype
        self.quantization = quantization
        self.enforce_eager = enforce_eager
        self.max_context_len_to_capture = max_context_len_to_capture
        self.disable_custom_all_reduce = disable_custom_all_reduce
        self.enable_chunked_prefill = enable_chunked_prefill
        self.max_num_on_the_fly_seq_groups = max_num_on_the_fly_seq_groups
        
        # 添加其他参数
        for key, value in kwargs.items():
            setattr(self, key, value)


class VLLMRecommenderEngine:
    """
    基于VLLM的推荐系统推理引擎

    负责完整的Prefill + Decode推理流程，集成PagedAttention优化，
    可加载TensorRT优化的引擎进行加速
    """

    def __init__(self, config: VLLMConfig, hstu_model=None, tensorrt_engine=None):
        self.config = config
        self.hstu_model = hstu_model
        self.tensorrt_engine = tensorrt_engine  # 用于加载TensorRT优化引擎
        self.request_counter = Counter()

        # TensorRT优化引擎集成
        self.tensorrt_optimized_engine_path = None
        self.tensorrt_optimization_applied = False

        if not VLLM_AVAILABLE:
            logger.warning("VLLM不可用，使用HSTU模型回退")
            self.vllm_available = False
            return

        self.vllm_available = True

        # 检查是否有TensorRT优化引擎
        self._check_tensorrt_optimization()

        # 初始化VLLM引擎（可能集成TensorRT优化）
        self._initialize_vllm_engine_with_optimization()
    
    def _check_tensorrt_optimization(self):
        """检查TensorRT优化引擎是否可用"""
        try:
            if self.tensorrt_engine is not None:
                # 从TensorRT引擎获取优化的engine路径
                optimized_path = self.tensorrt_engine.get_optimized_engine_path()

                if optimized_path and os.path.exists(optimized_path):
                    self.tensorrt_optimized_engine_path = optimized_path
                    self.tensorrt_optimization_applied = True
                    logger.info(f"✅ 发现TensorRT优化引擎: {optimized_path}")

                    # 获取优化配置信息
                    optimization_profile = self.tensorrt_engine.get_optimization_profile()
                    logger.info(f"TensorRT优化配置: {optimization_profile}")
                else:
                    logger.warning("TensorRT引擎未提供有效的优化文件")
            else:
                logger.info("未提供TensorRT引擎，将使用标准VLLM推理")

        except Exception as e:
            logger.warning(f"检查TensorRT优化失败: {e}")
            self.tensorrt_optimization_applied = False

    def _initialize_vllm_engine_with_optimization(self):
        """初始化集成了TensorRT优化的VLLM引擎"""
        try:
            # 如果有TensorRT优化引擎，尝试集成
            if self.tensorrt_optimization_applied and self.tensorrt_optimized_engine_path:
                logger.info("🔥 初始化集成TensorRT优化的VLLM引擎...")
                success = self._initialize_tensorrt_accelerated_vllm()

                if success:
                    logger.info("✅ TensorRT加速的VLLM引擎初始化成功")
                    return
                else:
                    logger.warning("TensorRT加速初始化失败，回退到标准VLLM")

            # 标准VLLM初始化
            logger.info("初始化标准VLLM引擎...")
            self._initialize_standard_vllm_engine()

        except Exception as e:
            logger.error(f"VLLM引擎初始化失败: {e}")
            self.vllm_available = False

    def _initialize_tensorrt_accelerated_vllm(self) -> bool:
        """初始化TensorRT加速的VLLM引擎"""
        try:
            # 注意: 实际的VLLM + TensorRT集成需要VLLM官方支持
            # 这里实现一种模拟的集成方式，将TensorRT优化的模型路径传递给VLLM

            # 创建TensorRT优化的模型配置
            tensorrt_model_config = self._create_tensorrt_model_config()

            # 使用优化配置创建VLLM引擎
            engine_args = AsyncEngineArgs(
                model=tensorrt_model_config['model_path'],
                tensor_parallel_size=self.config.tensor_parallel_size,
                pipeline_parallel_size=self.config.pipeline_parallel_size,
                max_model_len=self.config.max_model_len,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                swap_space=self.config.swap_space,
                max_num_seqs=self.config.max_num_seqs,
                max_num_batched_tokens=self.config.max_num_batched_tokens,
                block_size=self.config.block_size,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
                dtype=self.config.dtype,
                quantization=self.config.quantization,
                enforce_eager=False,  # 启用图模式以获得更好的性能
                enable_chunked_prefill=True,  # 启用分块预填充优化
                # 传递TensorRT优化信息
                **tensorrt_model_config
            )

            # 创建异步引擎
            self.async_engine = AsyncLLMEngine.from_engine_args(engine_args)

            # 创建同步引擎
            self.sync_engine = LLM(
                model=tensorrt_model_config['model_path'],
                tensor_parallel_size=self.config.tensor_parallel_size,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                max_model_len=self.config.max_model_len,
                dtype=self.config.dtype,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
                enforce_eager=False,
                enable_chunked_prefill=True
            )

            # 获取tokenizer
            self.tokenizer = get_tokenizer(
                self.config.tokenizer or tensorrt_model_config['model_path'],
                trust_remote_code=self.config.trust_remote_code,
                revision=self.config.tokenizer_revision,
            )

            logger.info("TensorRT加速的VLLM引擎初始化成功")
            return True

        except Exception as e:
            logger.error(f"TensorRT加速的VLLM引擎初始化失败: {e}")
            return False

    def _create_tensorrt_model_config(self) -> Dict[str, Any]:
        """创建TensorRT优化的模型配置"""
        config = {
            'model_path': self.config.model_path or "microsoft/DialoGPT-medium",
            'tensorrt_engine_path': self.tensorrt_optimized_engine_path,
            'tensorrt_optimized': True,
            'optimization_level': 'high',
            'use_fp16': True,
            'enable_dynamic_shapes': True
        }

        # 如果有HSTU模型，使用HSTU的配置
        if self.hstu_model is not None:
            config['base_model'] = 'hstu-generative-recommender'
            config['model_architecture'] = 'hstu-transformer'

        return config

    def _initialize_standard_vllm_engine(self):
        """初始化标准VLLM引擎"""
        try:
            # 确定模型路径
            if self.config.model_path and os.path.exists(self.config.model_path):
                model_path = self.config.model_path
                logger.info(f"使用指定模型路径: {model_path}")
            elif self.hstu_model is not None:
                logger.info("使用HSTU模型进行VLLM推理")
                model_path = None  # 将直接使用模型对象
            else:
                model_path = "microsoft/DialoGPT-medium"
                logger.info(f"使用默认模型: {model_path}")

            # 创建AsyncEngineArgs
            engine_args = AsyncEngineArgs(
                model=model_path if model_path else "microsoft/DialoGPT-medium",
                tensor_parallel_size=self.config.tensor_parallel_size,
                pipeline_parallel_size=self.config.pipeline_parallel_size,
                max_model_len=self.config.max_model_len,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                swap_space=self.config.swap_space,
                max_num_seqs=self.config.max_num_seqs,
                max_num_batched_tokens=self.config.max_num_batched_tokens,
                block_size=self.config.block_size,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
                revision=self.config.revision,
                tokenizer=self.config.tokenizer,
                tokenizer_revision=self.config.tokenizer_revision,
                dtype=self.config.dtype,
                quantization=self.config.quantization,
                enforce_eager=self.config.enforce_eager,
                max_context_len_to_capture=self.config.max_context_len_to_capture,
                disable_custom_all_reduce=self.config.disable_custom_all_reduce,
                enable_chunked_prefill=self.config.enable_chunked_prefill,
            )

            # 创建异步引擎
            self.async_engine = AsyncLLMEngine.from_engine_args(engine_args)

            # 创建同步引擎用于简单推理
            self.sync_engine = LLM(
                model=model_path if model_path else "microsoft/DialoGPT-medium",
                tensor_parallel_size=self.config.tensor_parallel_size,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                max_model_len=self.config.max_model_len,
                dtype=self.config.dtype,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
            )

            # 获取tokenizer
            self.tokenizer = get_tokenizer(
                self.config.tokenizer or (model_path if model_path else "microsoft/DialoGPT-medium"),
                trust_remote_code=self.config.trust_remote_code,
                revision=self.config.tokenizer_revision,
            )

            logger.info("✅ 标准VLLM引擎初始化成功")

        except Exception as e:
            logger.error(f"❌ 标准VLLM引擎初始化失败: {e}")
            self.vllm_available = False
    
    def generate_recommendations_complete(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        temperature: float = 0.8,
        top_p: float = 0.95,
        top_k: int = 50,
        max_tokens: int = 100,
        enable_paged_attention: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        完整的推荐生成流程（Prefill + Decode）

        这是新的主要接口，VLLM负责完整的推理流程，包括：
        1. Prefill阶段：处理用户行为序列
        2. Decode阶段：生成推荐结果
        3. KV Cache管理：自动处理缓存复用
        """

        if not self.vllm_available:
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )

        try:
            logger.info(f"🚀 开始完整推理流程 (用户: {user_id}, 推荐数: {num_recommendations})")

            # Step 1: 构建推荐生成的prompt
            prompt = self._build_recommendation_prompt(user_behaviors, num_recommendations)

            # Step 2: 配置生成参数（针对推荐任务优化）
            sampling_params = self._create_recommendation_sampling_params(
                num_recommendations, temperature, top_p, top_k, max_tokens
            )

            # Step 3: 执行完整的推理（VLLM自动处理Prefill和Decode）
            if enable_paged_attention:
                # 使用PagedAttention进行内存优化推理
                outputs = self._execute_paged_attention_inference(prompt, sampling_params)
            else:
                # 标准推理
                outputs = self._execute_standard_inference(prompt, sampling_params)

            # Step 4: 解析生成结果为推荐列表
            recommendations = self._parse_generation_to_recommendations(
                outputs, user_behaviors, num_recommendations
            )

            # Step 5: 添加推理元信息
            inference_info = self._collect_inference_stats()

            result = {
                'user_id': user_id,
                'session_id': session_id,
                'recommendations': recommendations,
                'inference_engine': 'vllm_complete',
                'tensorrt_accelerated': self.tensorrt_optimization_applied,
                'paged_attention_enabled': enable_paged_attention,
                'inference_stats': inference_info,
                'generation_params': {
                    'temperature': temperature,
                    'top_p': top_p,
                    'top_k': top_k,
                    'max_tokens': max_tokens
                }
            }

            logger.info(f"✅ 完整推理完成，生成 {len(recommendations)} 个推荐")
            return result

        except Exception as e:
            logger.error(f"❌ 完整推理失败: {e}")
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )

    def _build_recommendation_prompt(
        self,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> str:
        """构建推荐生成的prompt"""

        # 构建用户行为序列描述
        behavior_sequence = []
        for i, behavior in enumerate(user_behaviors[-20:]):  # 只取最近20个行为
            video_id = behavior.get('video_id', f'video_{i}')
            watch_duration = behavior.get('watch_duration', 0)
            category = behavior.get('category', 'unknown')
            is_liked = behavior.get('is_liked', False)

            behavior_desc = f"视频{video_id}(类别:{category},观看:{watch_duration}秒"
            if is_liked:
                behavior_desc += ",已点赞"
            behavior_desc += ")"

            behavior_sequence.append(behavior_desc)

        # 构建推荐生成prompt
        prompt = f"""用户行为序列: {' -> '.join(behavior_sequence)}

基于以上用户行为历史，生成{num_recommendations}个个性化视频推荐。每个推荐包含视频ID和推荐理由。

推荐列表:"""

        return prompt

    def _create_recommendation_sampling_params(
        self,
        num_recommendations: int,
        temperature: float,
        top_p: float,
        top_k: int,
        max_tokens: int
    ) -> 'SamplingParams':
        """创建推荐任务的采样参数"""

        return SamplingParams(
            n=1,  # 生成一个序列
            best_of=None,
            presence_penalty=0.1,  # 轻微的重复惩罚
            frequency_penalty=0.2,  # 避免频繁重复
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            min_p=0.01,
            use_beam_search=False,  # 推荐任务通常不需要beam search
            length_penalty=1.0,
            early_stopping=True,
            stop=None,
            stop_token_ids=None,
            include_stop_str_in_output=False,
            ignore_eos=False,
            max_tokens=max_tokens,
            seed=None,
            logprobs=None,
            prompt_logprobs=None,
            skip_special_tokens=True,
        )

    def _execute_paged_attention_inference(
        self,
        prompt: str,
        sampling_params: 'SamplingParams'
    ) -> List['RequestOutput']:
        """使用PagedAttention执行推理"""

        try:
            logger.info("🔥 使用PagedAttention执行推理...")

            # VLLM的PagedAttention是自动启用的，这里主要是记录
            outputs = self.sync_engine.generate(
                prompts=[prompt],
                sampling_params=sampling_params,
                use_tqdm=False
            )

            logger.info("✅ PagedAttention推理完成")
            return outputs

        except Exception as e:
            logger.error(f"PagedAttention推理失败: {e}")
            # 回退到标准推理
            return self._execute_standard_inference(prompt, sampling_params)

    def _execute_standard_inference(
        self,
        prompt: str,
        sampling_params: 'SamplingParams'
    ) -> List['RequestOutput']:
        """执行标准推理"""

        try:
            logger.info("执行标准推理...")

            outputs = self.sync_engine.generate(
                prompts=[prompt],
                sampling_params=sampling_params,
                use_tqdm=False
            )

            logger.info("✅ 标准推理完成")
            return outputs

        except Exception as e:
            logger.error(f"标准推理失败: {e}")
            raise

    def _parse_generation_to_recommendations(
        self,
        outputs: List['RequestOutput'],
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """将生成结果解析为推荐列表"""

        recommendations = []

        try:
            if outputs and len(outputs) > 0:
                output = outputs[0]
                generated_text = output.outputs[0].text if output.outputs else ""

                # 解析生成的文本为推荐列表
                recommendations = self._extract_recommendations_from_text(
                    generated_text, num_recommendations
                )

            # 如果解析失败或推荐数量不足，使用回退策略
            if len(recommendations) < num_recommendations:
                logger.warning(f"生成推荐数量不足({len(recommendations)}<{num_recommendations})，使用回退策略")
                fallback_recs = self._generate_fallback_recommendations(
                    user_behaviors, num_recommendations - len(recommendations)
                )
                recommendations.extend(fallback_recs)

            # 确保推荐数量
            recommendations = recommendations[:num_recommendations]

        except Exception as e:
            logger.error(f"解析生成结果失败: {e}")
            recommendations = self._generate_fallback_recommendations(user_behaviors, num_recommendations)

        return recommendations

    def _extract_recommendations_from_text(
        self,
        generated_text: str,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """从生成文本中提取推荐列表"""

        recommendations = []

        try:
            # 简单的文本解析策略
            lines = generated_text.strip().split('\n')

            for i, line in enumerate(lines):
                if i >= num_recommendations:
                    break

                line = line.strip()
                if line and not line.startswith('用户行为序列'):
                    # 提取视频ID和理由
                    video_id = f"rec_video_{i+1}"
                    reason = line

                    # 尝试从文本中提取更多信息
                    if '视频' in line and '(' in line:
                        parts = line.split('(')
                        if len(parts) > 1:
                            video_id = parts[0].replace('视频', '').strip()
                            reason = parts[1].replace(')', '').strip()

                    recommendation = {
                        'video_id': video_id,
                        'score': max(0.1, 0.9 - i * 0.1),  # 递减分数
                        'rank': i + 1,
                        'reason': reason or f'基于VLLM完整推理生成',
                        'source': 'vllm_generation'
                    }

                    recommendations.append(recommendation)

        except Exception as e:
            logger.warning(f"文本解析失败: {e}")

        return recommendations

    def _generate_fallback_recommendations(
        self,
        user_behaviors: List[Dict[str, Any]],
        num_needed: int
    ) -> List[Dict[str, Any]]:
        """生成回退推荐"""

        recommendations = []

        # 基于用户行为生成相关推荐
        categories = set()
        for behavior in user_behaviors:
            category = behavior.get('category', 'general')
            categories.add(category)

        category_list = list(categories) if categories else ['general']

        for i in range(num_needed):
            category = category_list[i % len(category_list)]

            recommendation = {
                'video_id': f'fallback_rec_{category}_{i+1}',
                'score': max(0.1, 0.7 - i * 0.1),
                'rank': i + 1,
                'reason': f'基于{category}类别的相关推荐',
                'source': 'fallback_generation'
            }

            recommendations.append(recommendation)

        return recommendations

    def _collect_inference_stats(self) -> Dict[str, Any]:
        """收集推理统计信息"""

        stats = {
            'engine_type': 'vllm_complete',
            'tensorrt_accelerated': self.tensorrt_optimization_applied,
            'model_loaded': hasattr(self, 'sync_engine') and self.sync_engine is not None,
            'async_engine_loaded': hasattr(self, 'async_engine') and self.async_engine is not None
        }

        # 如果有TensorRT优化，添加相关统计
        if self.tensorrt_optimization_applied and self.tensorrt_engine:
            tensorrt_stats = self.tensorrt_engine.get_optimization_profile()
            stats['tensorrt_optimization'] = tensorrt_stats

        return stats

    # 保留向后兼容的方法（委托给新的完整推理方法）
    def generate_recommendations(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        **kwargs
    ) -> Dict[str, Any]:
        """向后兼容的推荐生成方法，委托给完整推理"""
        return self.generate_recommendations_complete(
            user_id=user_id,
            session_id=session_id,
            user_behaviors=user_behaviors,
            num_recommendations=num_recommendations,
            **kwargs
        )

    def _generate_initial_tokens_from_behaviors(self, user_behaviors: List[Dict[str, Any]]) -> List[int]:
        """从用户行为生成初始token序列"""

        try:
            if not user_behaviors:
                return [1]  # 默认开始token

            # 简化的token生成：基于最后一个行为
            last_behavior = user_behaviors[-1]
            video_id = last_behavior.get('video_id', 'default')

            # 将video_id转换为token id（简化方法）
            # 实际应该使用tokenizer进行转换
            token_id = abs(hash(video_id)) % 50000  # 假设vocab size为50000

            return [token_id]

        except Exception as e:
            logger.error(f"从用户行为生成初始token失败: {e}")
            return [1]

    def _vllm_decode_generation(
        self,
        user_id: str,
        session_id: str,
        kv_cache: Optional[Dict],
        initial_tokens: List[int],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """使用vLLM引擎进行真正的Decode生成"""

        try:
            # 如果有KV Cache，使用它进行生成
            if kv_cache is not None:
                logger.info("使用KV Cache进行vLLM生成")
                return self._vllm_generate_with_cache(
                    kv_cache, initial_tokens, max_new_tokens, temperature, top_p, top_k, num_recommendations
                )
            else:
                logger.info("没有KV Cache，使用标准vLLM生成")
                return self._vllm_generate_standard(
                    initial_tokens, max_new_tokens, temperature, top_p, top_k, num_recommendations
                )

        except Exception as e:
            logger.error(f"vLLM decode generation失败: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _vllm_generate_with_cache(
        self,
        kv_cache: Dict,
        initial_tokens: List[int],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """使用KV Cache进行vLLM生成（核心方法）"""

        try:
            # 创建带有KV Cache的prompt
            # 注意：这里需要根据vLLM的具体API调整
            prompt_tokens = initial_tokens

            # 设置生成参数
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_new_tokens,
                seed=self.config.seed,
            )

            # 调用vLLM引擎的generate_recommendations方法
            # 这里是关键：将KV Cache注入到vLLM的生成过程中
            if hasattr(self.sync_engine, 'generate_with_kv_cache'):
                # 如果vLLM引擎支持KV Cache接口
                outputs = self.sync_engine.generate_with_kv_cache(
                    prompt_token_ids=prompt_tokens,
                    kv_cache=kv_cache,
                    sampling_params=sampling_params
                )
            else:
                # 回退到标准生成方法
                logger.warning("vLLM引擎不支持KV Cache接口，使用标准生成")
                prompt_text = self._tokens_to_text(prompt_tokens)
                outputs = self.sync_engine.generate([prompt_text], sampling_params)

            # 解析生成结果
            if outputs:
                output = outputs[0]
                if hasattr(output, 'outputs') and output.outputs:
                    generated_tokens = output.outputs[0].token_ids
                    generated_text = output.outputs[0].text

                    # 将生成的token转换为推荐
                    recommendations = self._tokens_to_recommendations(
                        generated_tokens, generated_text, num_recommendations
                    )

                    logger.info(f"✅ 使用KV Cache生成{len(recommendations)}个推荐")
                    return recommendations

            # 如果没有输出，返回回退推荐
            logger.warning("vLLM KV Cache生成无输出，使用回退推荐")
            return self._generate_fallback_recommendations(num_recommendations)

        except Exception as e:
            logger.error(f"vLLM KV Cache生成失败: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _vllm_generate_standard(
        self,
        initial_tokens: List[int],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """标准vLLM生成（无KV Cache）"""

        try:
            # 将token转换为文本
            prompt_text = self._tokens_to_text(initial_tokens)

            # 设置生成参数
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_new_tokens,
                seed=self.config.seed,
            )

            # 标准vLLM生成
            outputs = self.sync_engine.generate([prompt_text], sampling_params)

            if outputs:
                output = outputs[0]
                if hasattr(output, 'outputs') and output.outputs:
                    generated_text = output.outputs[0].text
                    return self._parse_generated_recommendations(generated_text, num_recommendations)

            return self._generate_fallback_recommendations(num_recommendations)

        except Exception as e:
            logger.error(f"标准vLLM生成失败: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _tokens_to_text(self, tokens: List[int]) -> str:
        """将token ID转换为文本"""
        try:
            if hasattr(self, 'tokenizer') and self.tokenizer:
                return self.tokenizer.decode(tokens)
            else:
                # 简化的转换
                return f"<tokens:{','.join(map(str, tokens))}>"
        except Exception as e:
            logger.error(f"Token转文本失败: {e}")
            return "<decode_error>"

    def _tokens_to_recommendations(
        self,
        token_ids: List[int],
        text: str,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """将生成的token转换为推荐结果"""

        recommendations = []

        try:
            # 方法1: 直接从token_ids生成推荐
            for i, token_id in enumerate(token_ids[:num_recommendations]):
                video_id = f"video_{token_id % 10000}"  # 映射到video ID范围
                score = 1.0 / (i + 1)  # 简单的排序分数

                recommendations.append({
                    'video_id': video_id,
                    'score': score,
                    'rank': i + 1,
                    'reason': f'vLLM生成 (token:{token_id})',
                    'token_id': token_id
                })

            # 方法2: 如果有文本，尝试解析
            if text and len(recommendations) < num_recommendations:
                text_recommendations = self._parse_generated_recommendations(text, num_recommendations)
                # 合并文本解析的推荐
                for rec in text_recommendations:
                    if len(recommendations) < num_recommendations:
                        rec['source'] = 'text_parsing'
                        recommendations.append(rec)

            # 确保推荐数量
            while len(recommendations) < num_recommendations:
                recommendations.append({
                    'video_id': f'fallback_video_{len(recommendations)}',
                    'score': 0.1,
                    'rank': len(recommendations) + 1,
                    'reason': 'vLLM回退推荐',
                    'source': 'fallback'
                })

            return recommendations[:num_recommendations]

        except Exception as e:
            logger.error(f"Token转推荐失败: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _generate_fallback_recommendations(self, num_recommendations: int) -> List[Dict[str, Any]]:
        """生成回退推荐"""
        import random

        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'fallback_video_{random.randint(1000, 9999)}',
                'score': random.uniform(0.3, 0.8),
                'rank': i + 1,
                'reason': 'vLLM回退推荐',
                'source': 'fallback'
            })

        return recommendations

    async def generate_recommendations_async(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        temperature: float = 0.8,
        top_p: float = 0.95,
        top_k: int = 50,
        max_tokens: int = 100,
        **kwargs
    ) -> Dict[str, Any]:
        """异步生成推荐结果"""
        
        if not self.vllm_available:
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )
        
        try:
            # 构建推理提示
            prompt = self._build_recommendation_prompt(user_behaviors, num_recommendations)
            
            # 设置采样参数
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_tokens,
                seed=self.config.seed,
            )
            
            # 生成请求ID
            request_id = f"{user_id}_{session_id}_{self.request_counter.next()}"
            
            # 异步生成
            results_generator = self.async_engine.generate(prompt, sampling_params, request_id)
            
            # 等待生成完成
            final_output = None
            async for request_output in results_generator:
                final_output = request_output
            
            if final_output and final_output.outputs:
                generated_text = final_output.outputs[0].text
                
                # 解析生成的推荐结果
                recommendations = self._parse_generated_recommendations(
                    generated_text, num_recommendations
                )
                
                return {
                    'user_id': user_id,
                    'session_id': session_id,
                    'request_id': request_id,
                    'recommendations': recommendations,
                    'engine_type': 'vllm_async',
                    'generated_text': generated_text,
                    'finished': final_output.finished,
                    'usage_stats': {
                        'prompt_tokens': len(final_output.prompt_token_ids),
                        'completion_tokens': sum(len(o.token_ids) for o in final_output.outputs),
                        'total_tokens': len(final_output.prompt_token_ids) + sum(len(o.token_ids) for o in final_output.outputs),
                    }
                }
            else:
                raise RuntimeError("异步VLLM生成失败")
                
        except Exception as e:
            logger.error(f"异步VLLM推理失败: {e}")
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )
    
    def _build_recommendation_prompt(
        self, 
        user_behaviors: List[Dict[str, Any]], 
        num_recommendations: int
    ) -> str:
        """构建推荐提示"""
        
        # 分析用户行为模式
        behavior_summary = self._analyze_user_behaviors(user_behaviors)
        
        # 构建结构化提示
        prompt = f"""作为一个智能推荐系统，请基于用户的历史行为为其推荐{num_recommendations}个相关内容。

用户行为分析：
{behavior_summary}

请按以下JSON格式返回推荐结果：
{{
    "recommendations": [
        {{"item_id": "推荐内容ID", "score": 推荐分数(0-1), "reason": "推荐理由"}},
        ...
    ]
}}

推荐结果："""
        
        return prompt
    
    def _analyze_user_behaviors(self, user_behaviors: List[Dict[str, Any]]) -> str:
        """分析用户行为模式"""
        if not user_behaviors:
            return "用户无历史行为记录"
        
        # 计算基本统计
        total_behaviors = len(user_behaviors)
        avg_watch_duration = sum(b.get('watch_duration', 0) for b in user_behaviors) / total_behaviors
        like_rate = sum(1 for b in user_behaviors if b.get('is_liked', False)) / total_behaviors
        
        # 识别偏好类别
        categories = {}
        for behavior in user_behaviors:
            category = behavior.get('category', 'unknown')
            categories[category] = categories.get(category, 0) + 1
        
        top_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # 时间模式分析
        recent_behaviors = user_behaviors[-5:]  # 最近5个行为
        
        analysis = f"""
- 总行为数量: {total_behaviors}
- 平均观看时长: {avg_watch_duration:.1f}秒
- 点赞率: {like_rate:.2%}
- 主要偏好类别: {', '.join([f'{cat}({count})' for cat, count in top_categories])}
- 最近行为: {len(recent_behaviors)}个内容
        """
        
        return analysis.strip()
    
    def _parse_generated_recommendations(
        self, 
        generated_text: str, 
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """解析生成的推荐结果"""
        
        try:
            import json
            import re
            
            # 尝试提取JSON部分
            json_pattern = r'\{.*\}'
            json_match = re.search(json_pattern, generated_text, re.DOTALL)
            
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                if 'recommendations' in result:
                    recommendations = result['recommendations'][:num_recommendations]
                    
                    # 标准化格式
                    for i, rec in enumerate(recommendations):
                        rec['position'] = i + 1
                        rec['video_id'] = rec.get('item_id', f'vllm_rec_{i}')
                        if 'score' not in rec:
                            rec['score'] = max(0.1, 0.9 - i * 0.1)
                        if 'reason' not in rec:
                            rec['reason'] = 'VLLM生成式推荐'
                    
                    return recommendations
            
            # 如果JSON解析失败，生成默认推荐
            logger.warning("无法解析生成的推荐结果，使用默认推荐")
            return self._generate_default_recommendations(num_recommendations, "VLLM解析失败")
            
        except Exception as e:
            logger.error(f"解析推荐结果失败: {e}")
            return self._generate_default_recommendations(num_recommendations, "VLLM解析错误")
    
    def _generate_default_recommendations(
        self, 
        num_recommendations: int, 
        reason: str = "默认推荐"
    ) -> List[Dict[str, Any]]:
        """生成默认推荐结果"""
        
        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'default_vllm_{i}',
                'score': max(0.1, 0.9 - i * 0.08),
                'position': i + 1,
                'reason': reason
            })
        
        return recommendations
    
    def _fallback_generate_recommendations(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        **kwargs
    ) -> Dict[str, Any]:
        """回退到HSTU模型生成推荐"""
        
        if self.hstu_model is not None:
            try:
                recommendations = self.hstu_model.generate_recommendations(
                    user_behaviors, num_recommendations
                )
                
                return {
                    'user_id': user_id,
                    'session_id': session_id,
                    'recommendations': recommendations,
                    'engine_type': 'hstu_fallback',
                    'reason': 'VLLM不可用，使用HSTU模型'
                }
                
            except Exception as e:
                logger.error(f"HSTU模型推理失败: {e}")
        
        # 最终回退到默认推荐
        return {
            'user_id': user_id,
            'session_id': session_id,
            'recommendations': self._generate_default_recommendations(num_recommendations, "系统回退推荐"),
            'engine_type': 'fallback',
            'reason': '所有推理引擎不可用'
        }
    
    async def batch_generate_recommendations(
        self,
        requests: List[Dict[str, Any]],
        **kwargs
    ) -> List[Dict[str, Any]]:
        """批量生成推荐"""
        
        if not self.vllm_available:
            # 串行处理回退请求
            results = []
            for request in requests:
                result = self._fallback_generate_recommendations(**request)
                results.append(result)
            return results
        
        # 使用VLLM的异步批处理能力
        tasks = []
        for request in requests:
            task = self.generate_recommendations_async(**request)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 处理异常结果
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"批量推理第{i}个请求失败: {result}")
                processed_results.append(
                    self._fallback_generate_recommendations(**requests[i])
                )
            else:
                processed_results.append(result)
        
        return processed_results
    
    def get_engine_stats(self) -> Dict[str, Any]:
        """获取引擎状态统计"""
        
        if not self.vllm_available:
            return {
                'vllm_available': False,
                'fallback_mode': True
            }
        
        try:
            # 获取VLLM引擎统计
            stats = {
                'vllm_available': True,
                'model_config': {
                    'model_name': self.config.model_name,
                    'tensor_parallel_size': self.config.tensor_parallel_size,
                    'max_num_seqs': self.config.max_num_seqs,
                    'gpu_memory_utilization': self.config.gpu_memory_utilization,
                    'dtype': self.config.dtype,
                },
                'request_counter': self.request_counter.value,
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"获取引擎统计失败: {e}")
            return {
                'vllm_available': False,
                'error': str(e)
            }


def create_vllm_engine(
    model_path: Optional[str] = None,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.9,
    max_model_len: Optional[int] = None,
    dtype: str = "float16",
    hstu_model=None,
    **kwargs
) -> VLLMRecommenderEngine:
    """创建VLLM推荐引擎"""
    
    config = VLLMConfig(
        model_path=model_path,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory_utilization,
        max_model_len=max_model_len,
        dtype=dtype,
        **kwargs
    )
    
    engine = VLLMRecommenderEngine(config, hstu_model)
    logger.info(f"✅ VLLM推荐引擎创建成功，可用性: {engine.vllm_available}")
    
    return engine


if __name__ == "__main__":
    # 测试VLLM引擎
    config = VLLMConfig(
        model_name="test-recommender",
        tensor_parallel_size=1,
        gpu_memory_utilization=0.8,
        max_num_seqs=64,
    )
    
    engine = VLLMRecommenderEngine(config)
    
    # 测试推荐生成
    user_behaviors = [
        {'video_id': 'video_1', 'watch_duration': 120, 'is_liked': True, 'category': 'tech'},
        {'video_id': 'video_2', 'watch_duration': 90, 'is_liked': False, 'category': 'music'},
        {'video_id': 'video_3', 'watch_duration': 200, 'is_liked': True, 'category': 'tech'},
    ]
    
    result = engine.generate_recommendations(
        user_id="test_user",
        session_id="test_session", 
        user_behaviors=user_behaviors,
        num_recommendations=5
    )
    
    print(f"推荐引擎类型: {result['engine_type']}")
    print(f"生成了 {len(result['recommendations'])} 个推荐")
    for rec in result['recommendations']:
        print(f"  {rec['video_id']}: {rec['score']:.4f} - {rec['reason']}")
    
    # 测试引擎统计
    stats = engine.get_engine_stats()
    print(f"引擎统计: {stats}")