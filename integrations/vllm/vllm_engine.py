#!/usr/bin/env python3
"""
VLLMæ¨ç†ä¼˜åŒ–æ¡†æ¶é›†æˆé€‚é…å™¨

åŸºäºvllm-project/vllmå¼€æºæ¡†æ¶ï¼Œæä¾›é«˜æ€§èƒ½LLMæ¨ç†æœåŠ¡ï¼Œ
é›†æˆPagedAttentionå’ŒContinuous Batchingä¼˜åŒ–æŠ€æœ¯ã€‚
"""

import os
import sys
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union, AsyncGenerator
import torch

# æ·»åŠ VLLMè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(current_dir, "..", "..")
vllm_path = os.path.join(project_root, "external", "vllm")
sys.path.append(vllm_path)

logger = logging.getLogger(__name__)

try:
    # å¯¼å…¥VLLMæ ¸å¿ƒç»„ä»¶
    from vllm import LLM, SamplingParams
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.utils import Counter
    from vllm.outputs import RequestOutput
    from vllm.transformers_utils.tokenizer import get_tokenizer
    
    VLLM_AVAILABLE = True
    logger.info("âœ… VLLMæ¡†æ¶å¯¼å…¥æˆåŠŸ")
    
except ImportError as e:
    VLLM_AVAILABLE = False
    logger.warning(f"âš ï¸ VLLMæ¡†æ¶å¯¼å…¥å¤±è´¥: {e}")


class VLLMConfig:
    """VLLMå¼•æ“é…ç½®"""
    def __init__(
        self,
        model_name: str = "hstu-generative-recommender",
        model_path: Optional[str] = None,
        tensor_parallel_size: int = 1,
        pipeline_parallel_size: int = 1,
        max_model_len: Optional[int] = None,
        gpu_memory_utilization: float = 0.9,
        swap_space: int = 4,
        max_num_seqs: int = 256,
        max_num_batched_tokens: Optional[int] = None,
        block_size: int = 16,
        seed: int = 0,
        trust_remote_code: bool = False,
        revision: Optional[str] = None,
        tokenizer: Optional[str] = None,
        tokenizer_revision: Optional[str] = None,
        dtype: str = "float16",
        quantization: Optional[str] = None,
        enforce_eager: bool = False,
        max_context_len_to_capture: int = 8192,
        disable_custom_all_reduce: bool = False,
        enable_chunked_prefill: bool = False,
        max_num_on_the_fly_seq_groups: int = 64,
        **kwargs
    ):
        self.model_name = model_name
        self.model_path = model_path
        self.tensor_parallel_size = tensor_parallel_size
        self.pipeline_parallel_size = pipeline_parallel_size
        self.max_model_len = max_model_len
        self.gpu_memory_utilization = gpu_memory_utilization
        self.swap_space = swap_space
        self.max_num_seqs = max_num_seqs
        self.max_num_batched_tokens = max_num_batched_tokens
        self.block_size = block_size
        self.seed = seed
        self.trust_remote_code = trust_remote_code
        self.revision = revision
        self.tokenizer = tokenizer
        self.tokenizer_revision = tokenizer_revision
        self.dtype = dtype
        self.quantization = quantization
        self.enforce_eager = enforce_eager
        self.max_context_len_to_capture = max_context_len_to_capture
        self.disable_custom_all_reduce = disable_custom_all_reduce
        self.enable_chunked_prefill = enable_chunked_prefill
        self.max_num_on_the_fly_seq_groups = max_num_on_the_fly_seq_groups
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        for key, value in kwargs.items():
            setattr(self, key, value)


class VLLMRecommenderEngine:
    """
    åŸºäºVLLMçš„æ¨èç³»ç»Ÿæ¨ç†å¼•æ“

    è´Ÿè´£å®Œæ•´çš„Prefill + Decodeæ¨ç†æµç¨‹ï¼Œé›†æˆPagedAttentionä¼˜åŒ–ï¼Œ
    å¯åŠ è½½TensorRTä¼˜åŒ–çš„å¼•æ“è¿›è¡ŒåŠ é€Ÿ
    """

    def __init__(self, config: VLLMConfig, hstu_model=None, tensorrt_engine=None):
        self.config = config
        self.hstu_model = hstu_model
        self.tensorrt_engine = tensorrt_engine  # ç”¨äºåŠ è½½TensorRTä¼˜åŒ–å¼•æ“
        self.request_counter = Counter()

        # TensorRTä¼˜åŒ–å¼•æ“é›†æˆ
        self.tensorrt_optimized_engine_path = None
        self.tensorrt_optimization_applied = False

        if not VLLM_AVAILABLE:
            logger.warning("VLLMä¸å¯ç”¨ï¼Œä½¿ç”¨HSTUæ¨¡å‹å›é€€")
            self.vllm_available = False
            return

        self.vllm_available = True

        # æ£€æŸ¥æ˜¯å¦æœ‰TensorRTä¼˜åŒ–å¼•æ“
        self._check_tensorrt_optimization()

        # åˆå§‹åŒ–VLLMå¼•æ“ï¼ˆå¯èƒ½é›†æˆTensorRTä¼˜åŒ–ï¼‰
        self._initialize_vllm_engine_with_optimization()
    
    def _check_tensorrt_optimization(self):
        """æ£€æŸ¥TensorRTä¼˜åŒ–å¼•æ“æ˜¯å¦å¯ç”¨"""
        try:
            if self.tensorrt_engine is not None:
                # ä»TensorRTå¼•æ“è·å–ä¼˜åŒ–çš„engineè·¯å¾„
                optimized_path = self.tensorrt_engine.get_optimized_engine_path()

                if optimized_path and os.path.exists(optimized_path):
                    self.tensorrt_optimized_engine_path = optimized_path
                    self.tensorrt_optimization_applied = True
                    logger.info(f"âœ… å‘ç°TensorRTä¼˜åŒ–å¼•æ“: {optimized_path}")

                    # è·å–ä¼˜åŒ–é…ç½®ä¿¡æ¯
                    optimization_profile = self.tensorrt_engine.get_optimization_profile()
                    logger.info(f"TensorRTä¼˜åŒ–é…ç½®: {optimization_profile}")
                else:
                    logger.warning("TensorRTå¼•æ“æœªæä¾›æœ‰æ•ˆçš„ä¼˜åŒ–æ–‡ä»¶")
            else:
                logger.info("æœªæä¾›TensorRTå¼•æ“ï¼Œå°†ä½¿ç”¨æ ‡å‡†VLLMæ¨ç†")

        except Exception as e:
            logger.warning(f"æ£€æŸ¥TensorRTä¼˜åŒ–å¤±è´¥: {e}")
            self.tensorrt_optimization_applied = False

    def _initialize_vllm_engine_with_optimization(self):
        """åˆå§‹åŒ–é›†æˆäº†TensorRTä¼˜åŒ–çš„VLLMå¼•æ“"""
        try:
            # å¦‚æœæœ‰TensorRTä¼˜åŒ–å¼•æ“ï¼Œå°è¯•é›†æˆ
            if self.tensorrt_optimization_applied and self.tensorrt_optimized_engine_path:
                logger.info("ğŸ”¥ åˆå§‹åŒ–é›†æˆTensorRTä¼˜åŒ–çš„VLLMå¼•æ“...")
                success = self._initialize_tensorrt_accelerated_vllm()

                if success:
                    logger.info("âœ… TensorRTåŠ é€Ÿçš„VLLMå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
                    return
                else:
                    logger.warning("TensorRTåŠ é€Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†VLLM")

            # æ ‡å‡†VLLMåˆå§‹åŒ–
            logger.info("åˆå§‹åŒ–æ ‡å‡†VLLMå¼•æ“...")
            self._initialize_standard_vllm_engine()

        except Exception as e:
            logger.error(f"VLLMå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vllm_available = False

    def _initialize_tensorrt_accelerated_vllm(self) -> bool:
        """åˆå§‹åŒ–TensorRTåŠ é€Ÿçš„VLLMå¼•æ“"""
        try:
            # æ³¨æ„: å®é™…çš„VLLM + TensorRTé›†æˆéœ€è¦VLLMå®˜æ–¹æ”¯æŒ
            # è¿™é‡Œå®ç°ä¸€ç§æ¨¡æ‹Ÿçš„é›†æˆæ–¹å¼ï¼Œå°†TensorRTä¼˜åŒ–çš„æ¨¡å‹è·¯å¾„ä¼ é€’ç»™VLLM

            # åˆ›å»ºTensorRTä¼˜åŒ–çš„æ¨¡å‹é…ç½®
            tensorrt_model_config = self._create_tensorrt_model_config()

            # ä½¿ç”¨ä¼˜åŒ–é…ç½®åˆ›å»ºVLLMå¼•æ“
            engine_args = AsyncEngineArgs(
                model=tensorrt_model_config['model_path'],
                tensor_parallel_size=self.config.tensor_parallel_size,
                pipeline_parallel_size=self.config.pipeline_parallel_size,
                max_model_len=self.config.max_model_len,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                swap_space=self.config.swap_space,
                max_num_seqs=self.config.max_num_seqs,
                max_num_batched_tokens=self.config.max_num_batched_tokens,
                block_size=self.config.block_size,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
                dtype=self.config.dtype,
                quantization=self.config.quantization,
                enforce_eager=False,  # å¯ç”¨å›¾æ¨¡å¼ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
                enable_chunked_prefill=True,  # å¯ç”¨åˆ†å—é¢„å¡«å……ä¼˜åŒ–
                # ä¼ é€’TensorRTä¼˜åŒ–ä¿¡æ¯
                **tensorrt_model_config
            )

            # åˆ›å»ºå¼‚æ­¥å¼•æ“
            self.async_engine = AsyncLLMEngine.from_engine_args(engine_args)

            # åˆ›å»ºåŒæ­¥å¼•æ“
            self.sync_engine = LLM(
                model=tensorrt_model_config['model_path'],
                tensor_parallel_size=self.config.tensor_parallel_size,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                max_model_len=self.config.max_model_len,
                dtype=self.config.dtype,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
                enforce_eager=False,
                enable_chunked_prefill=True
            )

            # è·å–tokenizer
            self.tokenizer = get_tokenizer(
                self.config.tokenizer or tensorrt_model_config['model_path'],
                trust_remote_code=self.config.trust_remote_code,
                revision=self.config.tokenizer_revision,
            )

            logger.info("TensorRTåŠ é€Ÿçš„VLLMå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"TensorRTåŠ é€Ÿçš„VLLMå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def _create_tensorrt_model_config(self) -> Dict[str, Any]:
        """åˆ›å»ºTensorRTä¼˜åŒ–çš„æ¨¡å‹é…ç½®"""
        config = {
            'model_path': self.config.model_path or "microsoft/DialoGPT-medium",
            'tensorrt_engine_path': self.tensorrt_optimized_engine_path,
            'tensorrt_optimized': True,
            'optimization_level': 'high',
            'use_fp16': True,
            'enable_dynamic_shapes': True
        }

        # å¦‚æœæœ‰HSTUæ¨¡å‹ï¼Œä½¿ç”¨HSTUçš„é…ç½®
        if self.hstu_model is not None:
            config['base_model'] = 'hstu-generative-recommender'
            config['model_architecture'] = 'hstu-transformer'

        return config

    def _initialize_standard_vllm_engine(self):
        """åˆå§‹åŒ–æ ‡å‡†VLLMå¼•æ“"""
        try:
            # ç¡®å®šæ¨¡å‹è·¯å¾„
            if self.config.model_path and os.path.exists(self.config.model_path):
                model_path = self.config.model_path
                logger.info(f"ä½¿ç”¨æŒ‡å®šæ¨¡å‹è·¯å¾„: {model_path}")
            elif self.hstu_model is not None:
                logger.info("ä½¿ç”¨HSTUæ¨¡å‹è¿›è¡ŒVLLMæ¨ç†")
                model_path = None  # å°†ç›´æ¥ä½¿ç”¨æ¨¡å‹å¯¹è±¡
            else:
                model_path = "microsoft/DialoGPT-medium"
                logger.info(f"ä½¿ç”¨é»˜è®¤æ¨¡å‹: {model_path}")

            # åˆ›å»ºAsyncEngineArgs
            engine_args = AsyncEngineArgs(
                model=model_path if model_path else "microsoft/DialoGPT-medium",
                tensor_parallel_size=self.config.tensor_parallel_size,
                pipeline_parallel_size=self.config.pipeline_parallel_size,
                max_model_len=self.config.max_model_len,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                swap_space=self.config.swap_space,
                max_num_seqs=self.config.max_num_seqs,
                max_num_batched_tokens=self.config.max_num_batched_tokens,
                block_size=self.config.block_size,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
                revision=self.config.revision,
                tokenizer=self.config.tokenizer,
                tokenizer_revision=self.config.tokenizer_revision,
                dtype=self.config.dtype,
                quantization=self.config.quantization,
                enforce_eager=self.config.enforce_eager,
                max_context_len_to_capture=self.config.max_context_len_to_capture,
                disable_custom_all_reduce=self.config.disable_custom_all_reduce,
                enable_chunked_prefill=self.config.enable_chunked_prefill,
            )

            # åˆ›å»ºå¼‚æ­¥å¼•æ“
            self.async_engine = AsyncLLMEngine.from_engine_args(engine_args)

            # åˆ›å»ºåŒæ­¥å¼•æ“ç”¨äºç®€å•æ¨ç†
            self.sync_engine = LLM(
                model=model_path if model_path else "microsoft/DialoGPT-medium",
                tensor_parallel_size=self.config.tensor_parallel_size,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                max_model_len=self.config.max_model_len,
                dtype=self.config.dtype,
                seed=self.config.seed,
                trust_remote_code=self.config.trust_remote_code,
            )

            # è·å–tokenizer
            self.tokenizer = get_tokenizer(
                self.config.tokenizer or (model_path if model_path else "microsoft/DialoGPT-medium"),
                trust_remote_code=self.config.trust_remote_code,
                revision=self.config.tokenizer_revision,
            )

            logger.info("âœ… æ ‡å‡†VLLMå¼•æ“åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ æ ‡å‡†VLLMå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vllm_available = False
    
    def generate_recommendations_complete(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        temperature: float = 0.8,
        top_p: float = 0.95,
        top_k: int = 50,
        max_tokens: int = 100,
        enable_paged_attention: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ¨èç”Ÿæˆæµç¨‹ï¼ˆPrefill + Decodeï¼‰

        è¿™æ˜¯æ–°çš„ä¸»è¦æ¥å£ï¼ŒVLLMè´Ÿè´£å®Œæ•´çš„æ¨ç†æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
        1. Prefillé˜¶æ®µï¼šå¤„ç†ç”¨æˆ·è¡Œä¸ºåºåˆ—
        2. Decodeé˜¶æ®µï¼šç”Ÿæˆæ¨èç»“æœ
        3. KV Cacheç®¡ç†ï¼šè‡ªåŠ¨å¤„ç†ç¼“å­˜å¤ç”¨
        """

        if not self.vllm_available:
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )

        try:
            logger.info(f"ğŸš€ å¼€å§‹å®Œæ•´æ¨ç†æµç¨‹ (ç”¨æˆ·: {user_id}, æ¨èæ•°: {num_recommendations})")

            # Step 1: æ„å»ºæ¨èç”Ÿæˆçš„prompt
            prompt = self._build_recommendation_prompt(user_behaviors, num_recommendations)

            # Step 2: é…ç½®ç”Ÿæˆå‚æ•°ï¼ˆé’ˆå¯¹æ¨èä»»åŠ¡ä¼˜åŒ–ï¼‰
            sampling_params = self._create_recommendation_sampling_params(
                num_recommendations, temperature, top_p, top_k, max_tokens
            )

            # Step 3: æ‰§è¡Œå®Œæ•´çš„æ¨ç†ï¼ˆVLLMè‡ªåŠ¨å¤„ç†Prefillå’ŒDecodeï¼‰
            if enable_paged_attention:
                # ä½¿ç”¨PagedAttentionè¿›è¡Œå†…å­˜ä¼˜åŒ–æ¨ç†
                outputs = self._execute_paged_attention_inference(prompt, sampling_params)
            else:
                # æ ‡å‡†æ¨ç†
                outputs = self._execute_standard_inference(prompt, sampling_params)

            # Step 4: è§£æç”Ÿæˆç»“æœä¸ºæ¨èåˆ—è¡¨
            recommendations = self._parse_generation_to_recommendations(
                outputs, user_behaviors, num_recommendations
            )

            # Step 5: æ·»åŠ æ¨ç†å…ƒä¿¡æ¯
            inference_info = self._collect_inference_stats()

            result = {
                'user_id': user_id,
                'session_id': session_id,
                'recommendations': recommendations,
                'inference_engine': 'vllm_complete',
                'tensorrt_accelerated': self.tensorrt_optimization_applied,
                'paged_attention_enabled': enable_paged_attention,
                'inference_stats': inference_info,
                'generation_params': {
                    'temperature': temperature,
                    'top_p': top_p,
                    'top_k': top_k,
                    'max_tokens': max_tokens
                }
            }

            logger.info(f"âœ… å®Œæ•´æ¨ç†å®Œæˆï¼Œç”Ÿæˆ {len(recommendations)} ä¸ªæ¨è")
            return result

        except Exception as e:
            logger.error(f"âŒ å®Œæ•´æ¨ç†å¤±è´¥: {e}")
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )

    def _build_recommendation_prompt(
        self,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> str:
        """æ„å»ºæ¨èç”Ÿæˆçš„prompt"""

        # æ„å»ºç”¨æˆ·è¡Œä¸ºåºåˆ—æè¿°
        behavior_sequence = []
        for i, behavior in enumerate(user_behaviors[-20:]):  # åªå–æœ€è¿‘20ä¸ªè¡Œä¸º
            video_id = behavior.get('video_id', f'video_{i}')
            watch_duration = behavior.get('watch_duration', 0)
            category = behavior.get('category', 'unknown')
            is_liked = behavior.get('is_liked', False)

            behavior_desc = f"è§†é¢‘{video_id}(ç±»åˆ«:{category},è§‚çœ‹:{watch_duration}ç§’"
            if is_liked:
                behavior_desc += ",å·²ç‚¹èµ"
            behavior_desc += ")"

            behavior_sequence.append(behavior_desc)

        # æ„å»ºæ¨èç”Ÿæˆprompt
        prompt = f"""ç”¨æˆ·è¡Œä¸ºåºåˆ—: {' -> '.join(behavior_sequence)}

åŸºäºä»¥ä¸Šç”¨æˆ·è¡Œä¸ºå†å²ï¼Œç”Ÿæˆ{num_recommendations}ä¸ªä¸ªæ€§åŒ–è§†é¢‘æ¨èã€‚æ¯ä¸ªæ¨èåŒ…å«è§†é¢‘IDå’Œæ¨èç†ç”±ã€‚

æ¨èåˆ—è¡¨:"""

        return prompt

    def _create_recommendation_sampling_params(
        self,
        num_recommendations: int,
        temperature: float,
        top_p: float,
        top_k: int,
        max_tokens: int
    ) -> 'SamplingParams':
        """åˆ›å»ºæ¨èä»»åŠ¡çš„é‡‡æ ·å‚æ•°"""

        return SamplingParams(
            n=1,  # ç”Ÿæˆä¸€ä¸ªåºåˆ—
            best_of=None,
            presence_penalty=0.1,  # è½»å¾®çš„é‡å¤æƒ©ç½š
            frequency_penalty=0.2,  # é¿å…é¢‘ç¹é‡å¤
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            min_p=0.01,
            use_beam_search=False,  # æ¨èä»»åŠ¡é€šå¸¸ä¸éœ€è¦beam search
            length_penalty=1.0,
            early_stopping=True,
            stop=None,
            stop_token_ids=None,
            include_stop_str_in_output=False,
            ignore_eos=False,
            max_tokens=max_tokens,
            seed=None,
            logprobs=None,
            prompt_logprobs=None,
            skip_special_tokens=True,
        )

    def _execute_paged_attention_inference(
        self,
        prompt: str,
        sampling_params: 'SamplingParams'
    ) -> List['RequestOutput']:
        """ä½¿ç”¨PagedAttentionæ‰§è¡Œæ¨ç†"""

        try:
            logger.info("ğŸ”¥ ä½¿ç”¨PagedAttentionæ‰§è¡Œæ¨ç†...")

            # VLLMçš„PagedAttentionæ˜¯è‡ªåŠ¨å¯ç”¨çš„ï¼Œè¿™é‡Œä¸»è¦æ˜¯è®°å½•
            outputs = self.sync_engine.generate(
                prompts=[prompt],
                sampling_params=sampling_params,
                use_tqdm=False
            )

            logger.info("âœ… PagedAttentionæ¨ç†å®Œæˆ")
            return outputs

        except Exception as e:
            logger.error(f"PagedAttentionæ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†æ¨ç†
            return self._execute_standard_inference(prompt, sampling_params)

    def _execute_standard_inference(
        self,
        prompt: str,
        sampling_params: 'SamplingParams'
    ) -> List['RequestOutput']:
        """æ‰§è¡Œæ ‡å‡†æ¨ç†"""

        try:
            logger.info("æ‰§è¡Œæ ‡å‡†æ¨ç†...")

            outputs = self.sync_engine.generate(
                prompts=[prompt],
                sampling_params=sampling_params,
                use_tqdm=False
            )

            logger.info("âœ… æ ‡å‡†æ¨ç†å®Œæˆ")
            return outputs

        except Exception as e:
            logger.error(f"æ ‡å‡†æ¨ç†å¤±è´¥: {e}")
            raise

    def _parse_generation_to_recommendations(
        self,
        outputs: List['RequestOutput'],
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """å°†ç”Ÿæˆç»“æœè§£æä¸ºæ¨èåˆ—è¡¨"""

        recommendations = []

        try:
            if outputs and len(outputs) > 0:
                output = outputs[0]
                generated_text = output.outputs[0].text if output.outputs else ""

                # è§£æç”Ÿæˆçš„æ–‡æœ¬ä¸ºæ¨èåˆ—è¡¨
                recommendations = self._extract_recommendations_from_text(
                    generated_text, num_recommendations
                )

            # å¦‚æœè§£æå¤±è´¥æˆ–æ¨èæ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
            if len(recommendations) < num_recommendations:
                logger.warning(f"ç”Ÿæˆæ¨èæ•°é‡ä¸è¶³({len(recommendations)}<{num_recommendations})ï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
                fallback_recs = self._generate_fallback_recommendations(
                    user_behaviors, num_recommendations - len(recommendations)
                )
                recommendations.extend(fallback_recs)

            # ç¡®ä¿æ¨èæ•°é‡
            recommendations = recommendations[:num_recommendations]

        except Exception as e:
            logger.error(f"è§£æç”Ÿæˆç»“æœå¤±è´¥: {e}")
            recommendations = self._generate_fallback_recommendations(user_behaviors, num_recommendations)

        return recommendations

    def _extract_recommendations_from_text(
        self,
        generated_text: str,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–æ¨èåˆ—è¡¨"""

        recommendations = []

        try:
            # ç®€å•çš„æ–‡æœ¬è§£æç­–ç•¥
            lines = generated_text.strip().split('\n')

            for i, line in enumerate(lines):
                if i >= num_recommendations:
                    break

                line = line.strip()
                if line and not line.startswith('ç”¨æˆ·è¡Œä¸ºåºåˆ—'):
                    # æå–è§†é¢‘IDå’Œç†ç”±
                    video_id = f"rec_video_{i+1}"
                    reason = line

                    # å°è¯•ä»æ–‡æœ¬ä¸­æå–æ›´å¤šä¿¡æ¯
                    if 'è§†é¢‘' in line and '(' in line:
                        parts = line.split('(')
                        if len(parts) > 1:
                            video_id = parts[0].replace('è§†é¢‘', '').strip()
                            reason = parts[1].replace(')', '').strip()

                    recommendation = {
                        'video_id': video_id,
                        'score': max(0.1, 0.9 - i * 0.1),  # é€’å‡åˆ†æ•°
                        'rank': i + 1,
                        'reason': reason or f'åŸºäºVLLMå®Œæ•´æ¨ç†ç”Ÿæˆ',
                        'source': 'vllm_generation'
                    }

                    recommendations.append(recommendation)

        except Exception as e:
            logger.warning(f"æ–‡æœ¬è§£æå¤±è´¥: {e}")

        return recommendations

    def _generate_fallback_recommendations(
        self,
        user_behaviors: List[Dict[str, Any]],
        num_needed: int
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå›é€€æ¨è"""

        recommendations = []

        # åŸºäºç”¨æˆ·è¡Œä¸ºç”Ÿæˆç›¸å…³æ¨è
        categories = set()
        for behavior in user_behaviors:
            category = behavior.get('category', 'general')
            categories.add(category)

        category_list = list(categories) if categories else ['general']

        for i in range(num_needed):
            category = category_list[i % len(category_list)]

            recommendation = {
                'video_id': f'fallback_rec_{category}_{i+1}',
                'score': max(0.1, 0.7 - i * 0.1),
                'rank': i + 1,
                'reason': f'åŸºäº{category}ç±»åˆ«çš„ç›¸å…³æ¨è',
                'source': 'fallback_generation'
            }

            recommendations.append(recommendation)

        return recommendations

    def _collect_inference_stats(self) -> Dict[str, Any]:
        """æ”¶é›†æ¨ç†ç»Ÿè®¡ä¿¡æ¯"""

        stats = {
            'engine_type': 'vllm_complete',
            'tensorrt_accelerated': self.tensorrt_optimization_applied,
            'model_loaded': hasattr(self, 'sync_engine') and self.sync_engine is not None,
            'async_engine_loaded': hasattr(self, 'async_engine') and self.async_engine is not None
        }

        # å¦‚æœæœ‰TensorRTä¼˜åŒ–ï¼Œæ·»åŠ ç›¸å…³ç»Ÿè®¡
        if self.tensorrt_optimization_applied and self.tensorrt_engine:
            tensorrt_stats = self.tensorrt_engine.get_optimization_profile()
            stats['tensorrt_optimization'] = tensorrt_stats

        return stats

    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆå§”æ‰˜ç»™æ–°çš„å®Œæ•´æ¨ç†æ–¹æ³•ï¼‰
    def generate_recommendations(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        **kwargs
    ) -> Dict[str, Any]:
        """å‘åå…¼å®¹çš„æ¨èç”Ÿæˆæ–¹æ³•ï¼Œå§”æ‰˜ç»™å®Œæ•´æ¨ç†"""
        return self.generate_recommendations_complete(
            user_id=user_id,
            session_id=session_id,
            user_behaviors=user_behaviors,
            num_recommendations=num_recommendations,
            **kwargs
        )
    
    def generate_recommendations_with_kv_cache(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        prefill_kv_cache: Optional[Dict[str, torch.Tensor]] = None,
        prefill_logits: Optional[torch.Tensor] = None,
        num_recommendations: int = 10,
        max_new_tokens: int = 50,
        temperature: float = 0.8,
        top_p: float = 0.95,
        top_k: int = 50,
        **kwargs
    ) -> Dict[str, Any]:
        """åŸºäºPrefillé˜¶æ®µçš„KV Cacheè¿›è¡ŒDecodeç”Ÿæˆæ¨è

        Args:
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            user_behaviors: ç”¨æˆ·è¡Œä¸ºåºåˆ—
            prefill_kv_cache: Prefillé˜¶æ®µè®¡ç®—çš„KV Cache
            prefill_logits: Prefillé˜¶æ®µçš„logitsè¾“å‡º
            num_recommendations: æ¨èæ•°é‡
            max_new_tokens: æœ€å¤§æ–°ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: Top-pé‡‡æ ·å‚æ•°
            top_k: Top-ké‡‡æ ·å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åŒ…å«æ¨èç»“æœçš„å­—å…¸
        """

        logger.info("ğŸš€ å¼€å§‹vLLM Decodeæ¨ç† (ä½¿ç”¨KV Cache)")

        if not self.vllm_available:
            logger.warning("vLLMä¸å¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†æ¨èç”Ÿæˆ")
            return self.generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )

        try:
            # è½¬æ¢KV Cacheæ ¼å¼
            vllm_kv_cache = self._convert_kv_cache_to_vllm_format(prefill_kv_cache)

            # å‡†å¤‡åˆå§‹tokenåºåˆ—ï¼ˆä»prefill logitsè·å–ï¼‰
            if prefill_logits is not None:
                # ä»prefill_logitsä¸­è·å–ä¸‹ä¸€ä¸ªtoken
                next_token_logits = prefill_logits[0, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                next_token_id = torch.argmax(next_token_logits).item()
                initial_tokens = [next_token_id]
            else:
                # å›é€€åˆ°åŸºäºè¡Œä¸ºçš„tokenç”Ÿæˆ
                initial_tokens = self._generate_initial_tokens_from_behaviors(user_behaviors)

            # è°ƒç”¨vLLMå¼•æ“è¿›è¡ŒçœŸæ­£çš„decodeç”Ÿæˆ
            recommendations = self._vllm_decode_generation(
                user_id=user_id,
                session_id=session_id,
                kv_cache=vllm_kv_cache,
                initial_tokens=initial_tokens,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                num_recommendations=num_recommendations
            )

            logger.info(f"âœ… vLLM Decodeå®Œæˆï¼Œç”Ÿæˆ{len(recommendations)}ä¸ªæ¨è")

            return {
                'user_id': user_id,
                'session_id': session_id,
                'recommendations': recommendations,
                'engine_type': 'vllm_with_kv_cache',
                'decode_method': 'kv_cache_continuation',
                'prefill_engine': 'tensorrt',
                'decode_engine': 'vllm',
                'kv_cache_used': prefill_kv_cache is not None,
                'generation_mode': 'prefill_decode_split'
            }

        except Exception as e:
            logger.error(f"âŒ vLLM KV Cacheæ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†æ¨èç”Ÿæˆ
            logger.info("å›é€€åˆ°æ ‡å‡†vLLMæ¨èç”Ÿæˆ")
            return self.generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )

    def _convert_kv_cache_to_vllm_format(self, kv_cache: Optional[Dict[str, torch.Tensor]]) -> Optional[Dict]:
        """å°†TensorRTçš„KV Cacheæ ¼å¼è½¬æ¢ä¸ºvLLMæ ¼å¼"""

        if kv_cache is None:
            logger.warning("KV Cacheä¸ºç©ºï¼Œæ— æ³•è½¬æ¢")
            return None

        try:
            logger.info("è½¬æ¢KV Cacheæ ¼å¼: TensorRT -> vLLM")

            vllm_kv_cache = {}

            # éå†æ¯ä¸€å±‚çš„KV Cache
            for layer_name, layer_cache in kv_cache.items():
                if isinstance(layer_cache, dict) and 'key' in layer_cache and 'value' in layer_cache:
                    key_tensor = layer_cache['key']
                    value_tensor = layer_cache['value']

                    # vLLMæœŸæœ›çš„KV Cacheæ ¼å¼ï¼š[num_blocks, num_heads, block_size, head_dim]
                    # è¿™é‡Œéœ€è¦æ ¹æ®vLLMçš„å…·ä½“è¦æ±‚è¿›è¡Œè½¬æ¢

                    # ç®€åŒ–çš„è½¬æ¢ï¼ˆå®é™…éœ€è¦æ ¹æ®vLLMçš„PagedAttentionæ ¼å¼ï¼‰
                    batch_size, seq_len, num_heads, head_dim = key_tensor.shape

                    # vLLMçš„block-wiseæ ¼å¼è½¬æ¢
                    block_size = 16  # vLLMé»˜è®¤block size
                    num_blocks = (seq_len + block_size - 1) // block_size

                    # é‡å¡‘ä¸ºblockæ ¼å¼
                    padded_seq_len = num_blocks * block_size
                    if seq_len < padded_seq_len:
                        # Paddingåˆ°blockè¾¹ç•Œ
                        padding_key = torch.zeros(batch_size, padded_seq_len - seq_len, num_heads, head_dim,
                                                 dtype=key_tensor.dtype, device=key_tensor.device)
                        key_tensor = torch.cat([key_tensor, padding_key], dim=1)

                        padding_value = torch.zeros(batch_size, padded_seq_len - seq_len, num_heads, head_dim,
                                                   dtype=value_tensor.dtype, device=value_tensor.device)
                        value_tensor = torch.cat([value_tensor, padding_value], dim=1)

                    # é‡å¡‘ä¸ºvLLM blockæ ¼å¼: [batch_size, num_blocks, block_size, num_heads, head_dim]
                    key_blocks = key_tensor.view(batch_size, num_blocks, block_size, num_heads, head_dim)
                    value_blocks = value_tensor.view(batch_size, num_blocks, block_size, num_heads, head_dim)

                    # è½¬æ¢ä¸ºvLLMæœŸæœ›çš„æ ¼å¼: [num_blocks, num_heads, block_size, head_dim]
                    # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦è€ƒè™‘batchç»´åº¦çš„å¤„ç†
                    vllm_kv_cache[layer_name] = {
                        'key': key_blocks[0],  # å–ç¬¬ä¸€ä¸ªbatch
                        'value': value_blocks[0],
                        'block_info': {
                            'num_blocks': num_blocks,
                            'block_size': block_size,
                            'original_seq_len': seq_len
                        }
                    }

            logger.info(f"âœ… KV Cacheæ ¼å¼è½¬æ¢å®Œæˆï¼Œ{len(vllm_kv_cache)}å±‚")
            return vllm_kv_cache

        except Exception as e:
            logger.error(f"âŒ KV Cacheæ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            return None

    def _generate_initial_tokens_from_behaviors(self, user_behaviors: List[Dict[str, Any]]) -> List[int]:
        """ä»ç”¨æˆ·è¡Œä¸ºç”Ÿæˆåˆå§‹tokenåºåˆ—"""

        try:
            if not user_behaviors:
                return [1]  # é»˜è®¤å¼€å§‹token

            # ç®€åŒ–çš„tokenç”Ÿæˆï¼šåŸºäºæœ€åä¸€ä¸ªè¡Œä¸º
            last_behavior = user_behaviors[-1]
            video_id = last_behavior.get('video_id', 'default')

            # å°†video_idè½¬æ¢ä¸ºtoken idï¼ˆç®€åŒ–æ–¹æ³•ï¼‰
            # å®é™…åº”è¯¥ä½¿ç”¨tokenizerè¿›è¡Œè½¬æ¢
            token_id = abs(hash(video_id)) % 50000  # å‡è®¾vocab sizeä¸º50000

            return [token_id]

        except Exception as e:
            logger.error(f"ä»ç”¨æˆ·è¡Œä¸ºç”Ÿæˆåˆå§‹tokenå¤±è´¥: {e}")
            return [1]

    def _vllm_decode_generation(
        self,
        user_id: str,
        session_id: str,
        kv_cache: Optional[Dict],
        initial_tokens: List[int],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨vLLMå¼•æ“è¿›è¡ŒçœŸæ­£çš„Decodeç”Ÿæˆ"""

        try:
            # å¦‚æœæœ‰KV Cacheï¼Œä½¿ç”¨å®ƒè¿›è¡Œç”Ÿæˆ
            if kv_cache is not None:
                logger.info("ä½¿ç”¨KV Cacheè¿›è¡ŒvLLMç”Ÿæˆ")
                return self._vllm_generate_with_cache(
                    kv_cache, initial_tokens, max_new_tokens, temperature, top_p, top_k, num_recommendations
                )
            else:
                logger.info("æ²¡æœ‰KV Cacheï¼Œä½¿ç”¨æ ‡å‡†vLLMç”Ÿæˆ")
                return self._vllm_generate_standard(
                    initial_tokens, max_new_tokens, temperature, top_p, top_k, num_recommendations
                )

        except Exception as e:
            logger.error(f"vLLM decode generationå¤±è´¥: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _vllm_generate_with_cache(
        self,
        kv_cache: Dict,
        initial_tokens: List[int],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨KV Cacheè¿›è¡ŒvLLMç”Ÿæˆï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰"""

        try:
            # åˆ›å»ºå¸¦æœ‰KV Cacheçš„prompt
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®vLLMçš„å…·ä½“APIè°ƒæ•´
            prompt_tokens = initial_tokens

            # è®¾ç½®ç”Ÿæˆå‚æ•°
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_new_tokens,
                seed=self.config.seed,
            )

            # è°ƒç”¨vLLMå¼•æ“çš„generate_recommendationsæ–¹æ³•
            # è¿™é‡Œæ˜¯å…³é”®ï¼šå°†KV Cacheæ³¨å…¥åˆ°vLLMçš„ç”Ÿæˆè¿‡ç¨‹ä¸­
            if hasattr(self.sync_engine, 'generate_with_kv_cache'):
                # å¦‚æœvLLMå¼•æ“æ”¯æŒKV Cacheæ¥å£
                outputs = self.sync_engine.generate_with_kv_cache(
                    prompt_token_ids=prompt_tokens,
                    kv_cache=kv_cache,
                    sampling_params=sampling_params
                )
            else:
                # å›é€€åˆ°æ ‡å‡†ç”Ÿæˆæ–¹æ³•
                logger.warning("vLLMå¼•æ“ä¸æ”¯æŒKV Cacheæ¥å£ï¼Œä½¿ç”¨æ ‡å‡†ç”Ÿæˆ")
                prompt_text = self._tokens_to_text(prompt_tokens)
                outputs = self.sync_engine.generate([prompt_text], sampling_params)

            # è§£æç”Ÿæˆç»“æœ
            if outputs:
                output = outputs[0]
                if hasattr(output, 'outputs') and output.outputs:
                    generated_tokens = output.outputs[0].token_ids
                    generated_text = output.outputs[0].text

                    # å°†ç”Ÿæˆçš„tokenè½¬æ¢ä¸ºæ¨è
                    recommendations = self._tokens_to_recommendations(
                        generated_tokens, generated_text, num_recommendations
                    )

                    logger.info(f"âœ… ä½¿ç”¨KV Cacheç”Ÿæˆ{len(recommendations)}ä¸ªæ¨è")
                    return recommendations

            # å¦‚æœæ²¡æœ‰è¾“å‡ºï¼Œè¿”å›å›é€€æ¨è
            logger.warning("vLLM KV Cacheç”Ÿæˆæ— è¾“å‡ºï¼Œä½¿ç”¨å›é€€æ¨è")
            return self._generate_fallback_recommendations(num_recommendations)

        except Exception as e:
            logger.error(f"vLLM KV Cacheç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _vllm_generate_standard(
        self,
        initial_tokens: List[int],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """æ ‡å‡†vLLMç”Ÿæˆï¼ˆæ— KV Cacheï¼‰"""

        try:
            # å°†tokenè½¬æ¢ä¸ºæ–‡æœ¬
            prompt_text = self._tokens_to_text(initial_tokens)

            # è®¾ç½®ç”Ÿæˆå‚æ•°
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_new_tokens,
                seed=self.config.seed,
            )

            # æ ‡å‡†vLLMç”Ÿæˆ
            outputs = self.sync_engine.generate([prompt_text], sampling_params)

            if outputs:
                output = outputs[0]
                if hasattr(output, 'outputs') and output.outputs:
                    generated_text = output.outputs[0].text
                    return self._parse_generated_recommendations(generated_text, num_recommendations)

            return self._generate_fallback_recommendations(num_recommendations)

        except Exception as e:
            logger.error(f"æ ‡å‡†vLLMç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _tokens_to_text(self, tokens: List[int]) -> str:
        """å°†token IDè½¬æ¢ä¸ºæ–‡æœ¬"""
        try:
            if hasattr(self, 'tokenizer') and self.tokenizer:
                return self.tokenizer.decode(tokens)
            else:
                # ç®€åŒ–çš„è½¬æ¢
                return f"<tokens:{','.join(map(str, tokens))}>"
        except Exception as e:
            logger.error(f"Tokenè½¬æ–‡æœ¬å¤±è´¥: {e}")
            return "<decode_error>"

    def _tokens_to_recommendations(
        self,
        token_ids: List[int],
        text: str,
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """å°†ç”Ÿæˆçš„tokenè½¬æ¢ä¸ºæ¨èç»“æœ"""

        recommendations = []

        try:
            # æ–¹æ³•1: ç›´æ¥ä»token_idsç”Ÿæˆæ¨è
            for i, token_id in enumerate(token_ids[:num_recommendations]):
                video_id = f"video_{token_id % 10000}"  # æ˜ å°„åˆ°video IDèŒƒå›´
                score = 1.0 / (i + 1)  # ç®€å•çš„æ’åºåˆ†æ•°

                recommendations.append({
                    'video_id': video_id,
                    'score': score,
                    'rank': i + 1,
                    'reason': f'vLLMç”Ÿæˆ (token:{token_id})',
                    'token_id': token_id
                })

            # æ–¹æ³•2: å¦‚æœæœ‰æ–‡æœ¬ï¼Œå°è¯•è§£æ
            if text and len(recommendations) < num_recommendations:
                text_recommendations = self._parse_generated_recommendations(text, num_recommendations)
                # åˆå¹¶æ–‡æœ¬è§£æçš„æ¨è
                for rec in text_recommendations:
                    if len(recommendations) < num_recommendations:
                        rec['source'] = 'text_parsing'
                        recommendations.append(rec)

            # ç¡®ä¿æ¨èæ•°é‡
            while len(recommendations) < num_recommendations:
                recommendations.append({
                    'video_id': f'fallback_video_{len(recommendations)}',
                    'score': 0.1,
                    'rank': len(recommendations) + 1,
                    'reason': 'vLLMå›é€€æ¨è',
                    'source': 'fallback'
                })

            return recommendations[:num_recommendations]

        except Exception as e:
            logger.error(f"Tokenè½¬æ¨èå¤±è´¥: {e}")
            return self._generate_fallback_recommendations(num_recommendations)

    def _generate_fallback_recommendations(self, num_recommendations: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå›é€€æ¨è"""
        import random

        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'fallback_video_{random.randint(1000, 9999)}',
                'score': random.uniform(0.3, 0.8),
                'rank': i + 1,
                'reason': 'vLLMå›é€€æ¨è',
                'source': 'fallback'
            })

        return recommendations

    async def generate_recommendations_async(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        temperature: float = 0.8,
        top_p: float = 0.95,
        top_k: int = 50,
        max_tokens: int = 100,
        **kwargs
    ) -> Dict[str, Any]:
        """å¼‚æ­¥ç”Ÿæˆæ¨èç»“æœ"""
        
        if not self.vllm_available:
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )
        
        try:
            # æ„å»ºæ¨ç†æç¤º
            prompt = self._build_recommendation_prompt(user_behaviors, num_recommendations)
            
            # è®¾ç½®é‡‡æ ·å‚æ•°
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_tokens,
                seed=self.config.seed,
            )
            
            # ç”Ÿæˆè¯·æ±‚ID
            request_id = f"{user_id}_{session_id}_{self.request_counter.next()}"
            
            # å¼‚æ­¥ç”Ÿæˆ
            results_generator = self.async_engine.generate(prompt, sampling_params, request_id)
            
            # ç­‰å¾…ç”Ÿæˆå®Œæˆ
            final_output = None
            async for request_output in results_generator:
                final_output = request_output
            
            if final_output and final_output.outputs:
                generated_text = final_output.outputs[0].text
                
                # è§£æç”Ÿæˆçš„æ¨èç»“æœ
                recommendations = self._parse_generated_recommendations(
                    generated_text, num_recommendations
                )
                
                return {
                    'user_id': user_id,
                    'session_id': session_id,
                    'request_id': request_id,
                    'recommendations': recommendations,
                    'engine_type': 'vllm_async',
                    'generated_text': generated_text,
                    'finished': final_output.finished,
                    'usage_stats': {
                        'prompt_tokens': len(final_output.prompt_token_ids),
                        'completion_tokens': sum(len(o.token_ids) for o in final_output.outputs),
                        'total_tokens': len(final_output.prompt_token_ids) + sum(len(o.token_ids) for o in final_output.outputs),
                    }
                }
            else:
                raise RuntimeError("å¼‚æ­¥VLLMç”Ÿæˆå¤±è´¥")
                
        except Exception as e:
            logger.error(f"å¼‚æ­¥VLLMæ¨ç†å¤±è´¥: {e}")
            return self._fallback_generate_recommendations(
                user_id, session_id, user_behaviors, num_recommendations, **kwargs
            )
    
    def _build_recommendation_prompt(
        self, 
        user_behaviors: List[Dict[str, Any]], 
        num_recommendations: int
    ) -> str:
        """æ„å»ºæ¨èæç¤º"""
        
        # åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
        behavior_summary = self._analyze_user_behaviors(user_behaviors)
        
        # æ„å»ºç»“æ„åŒ–æç¤º
        prompt = f"""ä½œä¸ºä¸€ä¸ªæ™ºèƒ½æ¨èç³»ç»Ÿï¼Œè¯·åŸºäºç”¨æˆ·çš„å†å²è¡Œä¸ºä¸ºå…¶æ¨è{num_recommendations}ä¸ªç›¸å…³å†…å®¹ã€‚

ç”¨æˆ·è¡Œä¸ºåˆ†æï¼š
{behavior_summary}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›æ¨èç»“æœï¼š
{{
    "recommendations": [
        {{"item_id": "æ¨èå†…å®¹ID", "score": æ¨èåˆ†æ•°(0-1), "reason": "æ¨èç†ç”±"}},
        ...
    ]
}}

æ¨èç»“æœï¼š"""
        
        return prompt
    
    def _analyze_user_behaviors(self, user_behaviors: List[Dict[str, Any]]) -> str:
        """åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼"""
        if not user_behaviors:
            return "ç”¨æˆ·æ— å†å²è¡Œä¸ºè®°å½•"
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        total_behaviors = len(user_behaviors)
        avg_watch_duration = sum(b.get('watch_duration', 0) for b in user_behaviors) / total_behaviors
        like_rate = sum(1 for b in user_behaviors if b.get('is_liked', False)) / total_behaviors
        
        # è¯†åˆ«åå¥½ç±»åˆ«
        categories = {}
        for behavior in user_behaviors:
            category = behavior.get('category', 'unknown')
            categories[category] = categories.get(category, 0) + 1
        
        top_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æ—¶é—´æ¨¡å¼åˆ†æ
        recent_behaviors = user_behaviors[-5:]  # æœ€è¿‘5ä¸ªè¡Œä¸º
        
        analysis = f"""
- æ€»è¡Œä¸ºæ•°é‡: {total_behaviors}
- å¹³å‡è§‚çœ‹æ—¶é•¿: {avg_watch_duration:.1f}ç§’
- ç‚¹èµç‡: {like_rate:.2%}
- ä¸»è¦åå¥½ç±»åˆ«: {', '.join([f'{cat}({count})' for cat, count in top_categories])}
- æœ€è¿‘è¡Œä¸º: {len(recent_behaviors)}ä¸ªå†…å®¹
        """
        
        return analysis.strip()
    
    def _parse_generated_recommendations(
        self, 
        generated_text: str, 
        num_recommendations: int
    ) -> List[Dict[str, Any]]:
        """è§£æç”Ÿæˆçš„æ¨èç»“æœ"""
        
        try:
            import json
            import re
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_pattern = r'\{.*\}'
            json_match = re.search(json_pattern, generated_text, re.DOTALL)
            
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                if 'recommendations' in result:
                    recommendations = result['recommendations'][:num_recommendations]
                    
                    # æ ‡å‡†åŒ–æ ¼å¼
                    for i, rec in enumerate(recommendations):
                        rec['position'] = i + 1
                        rec['video_id'] = rec.get('item_id', f'vllm_rec_{i}')
                        if 'score' not in rec:
                            rec['score'] = max(0.1, 0.9 - i * 0.1)
                        if 'reason' not in rec:
                            rec['reason'] = 'VLLMç”Ÿæˆå¼æ¨è'
                    
                    return recommendations
            
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œç”Ÿæˆé»˜è®¤æ¨è
            logger.warning("æ— æ³•è§£æç”Ÿæˆçš„æ¨èç»“æœï¼Œä½¿ç”¨é»˜è®¤æ¨è")
            return self._generate_default_recommendations(num_recommendations, "VLLMè§£æå¤±è´¥")
            
        except Exception as e:
            logger.error(f"è§£ææ¨èç»“æœå¤±è´¥: {e}")
            return self._generate_default_recommendations(num_recommendations, "VLLMè§£æé”™è¯¯")
    
    def _generate_default_recommendations(
        self, 
        num_recommendations: int, 
        reason: str = "é»˜è®¤æ¨è"
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé»˜è®¤æ¨èç»“æœ"""
        
        recommendations = []
        for i in range(num_recommendations):
            recommendations.append({
                'video_id': f'default_vllm_{i}',
                'score': max(0.1, 0.9 - i * 0.08),
                'position': i + 1,
                'reason': reason
            })
        
        return recommendations
    
    def _fallback_generate_recommendations(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        **kwargs
    ) -> Dict[str, Any]:
        """å›é€€åˆ°HSTUæ¨¡å‹ç”Ÿæˆæ¨è"""
        
        if self.hstu_model is not None:
            try:
                recommendations = self.hstu_model.generate_recommendations(
                    user_behaviors, num_recommendations
                )
                
                return {
                    'user_id': user_id,
                    'session_id': session_id,
                    'recommendations': recommendations,
                    'engine_type': 'hstu_fallback',
                    'reason': 'VLLMä¸å¯ç”¨ï¼Œä½¿ç”¨HSTUæ¨¡å‹'
                }
                
            except Exception as e:
                logger.error(f"HSTUæ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # æœ€ç»ˆå›é€€åˆ°é»˜è®¤æ¨è
        return {
            'user_id': user_id,
            'session_id': session_id,
            'recommendations': self._generate_default_recommendations(num_recommendations, "ç³»ç»Ÿå›é€€æ¨è"),
            'engine_type': 'fallback',
            'reason': 'æ‰€æœ‰æ¨ç†å¼•æ“ä¸å¯ç”¨'
        }
    
    async def batch_generate_recommendations(
        self,
        requests: List[Dict[str, Any]],
        **kwargs
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆæ¨è"""
        
        if not self.vllm_available:
            # ä¸²è¡Œå¤„ç†å›é€€è¯·æ±‚
            results = []
            for request in requests:
                result = self._fallback_generate_recommendations(**request)
                results.append(result)
            return results
        
        # ä½¿ç”¨VLLMçš„å¼‚æ­¥æ‰¹å¤„ç†èƒ½åŠ›
        tasks = []
        for request in requests:
            task = self.generate_recommendations_async(**request)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æ¨ç†ç¬¬{i}ä¸ªè¯·æ±‚å¤±è´¥: {result}")
                processed_results.append(
                    self._fallback_generate_recommendations(**requests[i])
                )
            else:
                processed_results.append(result)
        
        return processed_results
    
    def get_engine_stats(self) -> Dict[str, Any]:
        """è·å–å¼•æ“çŠ¶æ€ç»Ÿè®¡"""
        
        if not self.vllm_available:
            return {
                'vllm_available': False,
                'fallback_mode': True
            }
        
        try:
            # è·å–VLLMå¼•æ“ç»Ÿè®¡
            stats = {
                'vllm_available': True,
                'model_config': {
                    'model_name': self.config.model_name,
                    'tensor_parallel_size': self.config.tensor_parallel_size,
                    'max_num_seqs': self.config.max_num_seqs,
                    'gpu_memory_utilization': self.config.gpu_memory_utilization,
                    'dtype': self.config.dtype,
                },
                'request_counter': self.request_counter.value,
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–å¼•æ“ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                'vllm_available': False,
                'error': str(e)
            }


def create_vllm_engine(
    model_path: Optional[str] = None,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.9,
    max_model_len: Optional[int] = None,
    dtype: str = "float16",
    hstu_model=None,
    **kwargs
) -> VLLMRecommenderEngine:
    """åˆ›å»ºVLLMæ¨èå¼•æ“"""
    
    config = VLLMConfig(
        model_path=model_path,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory_utilization,
        max_model_len=max_model_len,
        dtype=dtype,
        **kwargs
    )
    
    engine = VLLMRecommenderEngine(config, hstu_model)
    logger.info(f"âœ… VLLMæ¨èå¼•æ“åˆ›å»ºæˆåŠŸï¼Œå¯ç”¨æ€§: {engine.vllm_available}")
    
    return engine


if __name__ == "__main__":
    # æµ‹è¯•VLLMå¼•æ“
    config = VLLMConfig(
        model_name="test-recommender",
        tensor_parallel_size=1,
        gpu_memory_utilization=0.8,
        max_num_seqs=64,
    )
    
    engine = VLLMRecommenderEngine(config)
    
    # æµ‹è¯•æ¨èç”Ÿæˆ
    user_behaviors = [
        {'video_id': 'video_1', 'watch_duration': 120, 'is_liked': True, 'category': 'tech'},
        {'video_id': 'video_2', 'watch_duration': 90, 'is_liked': False, 'category': 'music'},
        {'video_id': 'video_3', 'watch_duration': 200, 'is_liked': True, 'category': 'tech'},
    ]
    
    result = engine.generate_recommendations(
        user_id="test_user",
        session_id="test_session", 
        user_behaviors=user_behaviors,
        num_recommendations=5
    )
    
    print(f"æ¨èå¼•æ“ç±»å‹: {result['engine_type']}")
    print(f"ç”Ÿæˆäº† {len(result['recommendations'])} ä¸ªæ¨è")
    for rec in result['recommendations']:
        print(f"  {rec['video_id']}: {rec['score']:.4f} - {rec['reason']}")
    
    # æµ‹è¯•å¼•æ“ç»Ÿè®¡
    stats = engine.get_engine_stats()
    print(f"å¼•æ“ç»Ÿè®¡: {stats}")