#!/usr/bin/env python3
"""
HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡

åŸºäºç°æœ‰ç»Ÿä¸€æ¨ç†ç®¡é“æ¶æ„çš„è½»é‡çº§æœåŠ¡åŒ–æ¥å£ï¼Œ
æä¾›RESTful APIè®¿é—®æ¨èç³»ç»Ÿæ¨ç†åŠŸèƒ½ã€‚
"""

import os
import sys
import logging
import asyncio
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
import traceback

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# FastAPIç›¸å…³å¯¼å…¥
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator
import uvicorn

# å¯¼å…¥ç°æœ‰çš„æ ¸å¿ƒç»„ä»¶
from integrations.framework_controller import create_integrated_controller
from examples.client_example import create_realistic_user_behaviors

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hstu_api_server.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡",
    description="åŸºäºMeta HSTUæ¨¡å‹çš„ç”Ÿæˆå¼æ¨èç³»ç»Ÿæ¨ç†ä¼˜åŒ–æœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
controller = None
server_stats = {
    "start_time": None,
    "total_requests": 0,
    "successful_requests": 0,
    "failed_requests": 0,
    "total_inference_time": 0.0,
}

# ============================= æ•°æ®æ¨¡å‹å®šä¹‰ =============================

class UserBehavior(BaseModel):
    """ç”¨æˆ·è¡Œä¸ºæ•°æ®æ¨¡å‹"""
    video_id: int = Field(..., description="è§†é¢‘ID")
    timestamp: int = Field(..., description="æ—¶é—´æˆ³")
    interaction_type: str = Field(..., description="äº¤äº’ç±»å‹: view/like/share/comment")
    duration: Optional[float] = Field(None, description="è§‚çœ‹æ—¶é•¿(ç§’)")
    device_type: Optional[str] = Field("mobile", description="è®¾å¤‡ç±»å‹")
    
    @validator('interaction_type')
    def validate_interaction_type(cls, v):
        allowed_types = ['view', 'like', 'share', 'comment', 'follow']
        if v not in allowed_types:
            raise ValueError(f'interaction_type must be one of {allowed_types}')
        return v

class InferenceRequest(BaseModel):
    """æ¨ç†è¯·æ±‚æ•°æ®æ¨¡å‹"""
    user_id: str = Field(..., description="ç”¨æˆ·ID")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")
    user_behaviors: List[UserBehavior] = Field(..., description="ç”¨æˆ·è¡Œä¸ºåˆ—è¡¨")
    num_recommendations: int = Field(10, ge=1, le=100, description="æ¨èæ•°é‡")
    strategy: str = Field("unified", description="æ¨ç†ç­–ç•¥")
    enable_cache: bool = Field(True, description="æ˜¯å¦å¯ç”¨ç¼“å­˜")
    
    @validator('strategy')
    def validate_strategy(cls, v):
        allowed_strategies = ['unified', 'tensorrt', 'vllm', 'hstu', 'fallback']
        if v not in allowed_strategies:
            raise ValueError(f'strategy must be one of {allowed_strategies}')
        return v

class RecommendationItem(BaseModel):
    """æ¨èé¡¹ç›®æ•°æ®æ¨¡å‹"""
    video_id: int = Field(..., description="æ¨èè§†é¢‘ID")
    score: float = Field(..., description="æ¨èå¾—åˆ†")
    confidence: float = Field(..., description="ç½®ä¿¡åº¦")
    reason: Optional[str] = Field(None, description="æ¨èç†ç”±")

class InferenceMetrics(BaseModel):
    """æ¨ç†æ€§èƒ½æŒ‡æ ‡"""
    total_time_ms: float = Field(..., description="æ€»æ¨ç†æ—¶é—´(æ¯«ç§’)")
    feature_processing_time_ms: float = Field(..., description="ç‰¹å¾å¤„ç†æ—¶é—´(æ¯«ç§’)")
    model_inference_time_ms: float = Field(..., description="æ¨¡å‹æ¨ç†æ—¶é—´(æ¯«ç§’)")
    strategy_used: str = Field(..., description="å®é™…ä½¿ç”¨çš„æ¨ç†ç­–ç•¥")
    cache_hit: bool = Field(..., description="æ˜¯å¦å‘½ä¸­ç¼“å­˜")
    gpu_utilization: Optional[float] = Field(None, description="GPUåˆ©ç”¨ç‡")

class InferenceResponse(BaseModel):
    """æ¨ç†å“åº”æ•°æ®æ¨¡å‹"""
    user_id: str = Field(..., description="ç”¨æˆ·ID")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")
    recommendations: List[RecommendationItem] = Field(..., description="æ¨èç»“æœåˆ—è¡¨")
    metrics: InferenceMetrics = Field(..., description="æ¨ç†æ€§èƒ½æŒ‡æ ‡")
    timestamp: str = Field(..., description="å“åº”æ—¶é—´æˆ³")
    request_id: Optional[str] = Field(None, description="è¯·æ±‚ID")

class BatchInferenceRequest(BaseModel):
    """æ‰¹é‡æ¨ç†è¯·æ±‚æ•°æ®æ¨¡å‹"""
    requests: List[InferenceRequest] = Field(..., max_items=50, description="æ¨ç†è¯·æ±‚åˆ—è¡¨")
    batch_strategy: str = Field("auto", description="æ‰¹é‡å¤„ç†ç­–ç•¥")

class BatchInferenceResponse(BaseModel):
    """æ‰¹é‡æ¨ç†å“åº”æ•°æ®æ¨¡å‹"""
    results: List[InferenceResponse] = Field(..., description="æ‰¹é‡æ¨ç†ç»“æœ")
    batch_metrics: Dict[str, Any] = Field(..., description="æ‰¹é‡å¤„ç†æŒ‡æ ‡")

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    timestamp: str = Field(..., description="æ£€æŸ¥æ—¶é—´")
    version: str = Field(..., description="æœåŠ¡ç‰ˆæœ¬")
    uptime_seconds: float = Field(..., description="è¿è¡Œæ—¶é—´(ç§’)")
    framework_status: Dict[str, bool] = Field(..., description="æ¡†æ¶å¯ç”¨æ€§çŠ¶æ€")

class StatsResponse(BaseModel):
    """ç»Ÿè®¡ä¿¡æ¯å“åº”"""
    server_stats: Dict[str, Any] = Field(..., description="æœåŠ¡å™¨ç»Ÿè®¡")
    controller_stats: Dict[str, Any] = Field(..., description="æ§åˆ¶å™¨ç»Ÿè®¡")
    system_info: Dict[str, Any] = Field(..., description="ç³»ç»Ÿä¿¡æ¯")

# ============================= ä¾èµ–æ³¨å…¥ =============================

async def get_controller():
    """è·å–æ§åˆ¶å™¨å®ä¾‹"""
    global controller
    if controller is None:
        raise HTTPException(status_code=503, detail="æ¨ç†æ§åˆ¶å™¨æœªåˆå§‹åŒ–")
    return controller

def update_server_stats(success: bool, inference_time: float = 0.0):
    """æ›´æ–°æœåŠ¡å™¨ç»Ÿè®¡"""
    global server_stats
    server_stats["total_requests"] += 1
    if success:
        server_stats["successful_requests"] += 1
        server_stats["total_inference_time"] += inference_time
    else:
        server_stats["failed_requests"] += 1

# ============================= ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ =============================

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨äº‹ä»¶"""
    global controller, server_stats
    
    logger.info("ğŸš€ å¯åŠ¨HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡...")
    
    try:
        # åˆå§‹åŒ–æ¨ç†æ§åˆ¶å™¨
        controller = create_integrated_controller()
        logger.info("âœ… æ¨ç†æ§åˆ¶å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆå§‹åŒ–æœåŠ¡å™¨ç»Ÿè®¡
        server_stats["start_time"] = datetime.now()
        
        logger.info("ğŸ‰ HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡å¯åŠ¨å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        raise e

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­äº‹ä»¶"""
    logger.info("ğŸ›‘ æ­£åœ¨å…³é—­HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡...")
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    if server_stats["start_time"]:
        uptime = (datetime.now() - server_stats["start_time"]).total_seconds()
        logger.info(f"ğŸ“Š æœåŠ¡è¿è¡Œæ—¶é—´: {uptime:.2f}ç§’")
        logger.info(f"ğŸ“Š æ€»è¯·æ±‚æ•°: {server_stats['total_requests']}")
        logger.info(f"ğŸ“Š æˆåŠŸè¯·æ±‚: {server_stats['successful_requests']}")
        logger.info(f"ğŸ“Š å¤±è´¥è¯·æ±‚: {server_stats['failed_requests']}")
        if server_stats["successful_requests"] > 0:
            avg_time = server_stats["total_inference_time"] / server_stats["successful_requests"]
            logger.info(f"ğŸ“Š å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}ms")
    
    logger.info("ğŸ‘‹ HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡å·²å…³é—­")

# ============================= APIç«¯ç‚¹ =============================

@app.get("/", response_model=dict)
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡",
        "version": "1.0.0",
        "description": "åŸºäºMeta HSTUæ¨¡å‹çš„ç”Ÿæˆå¼æ¨èç³»ç»Ÿæ¨ç†ä¼˜åŒ–æœåŠ¡",
        "docs_url": "/docs",
        "health_url": "/health",
        "stats_url": "/stats"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check(controller_instance = Depends(get_controller)):
    """å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥æ¡†æ¶å¯ç”¨æ€§
        framework_status = {}
        if hasattr(controller_instance, 'hstu_model') and controller_instance.hstu_model:
            framework_status["hstu"] = True
        else:
            framework_status["hstu"] = False
            
        if hasattr(controller_instance, 'vllm_engine') and controller_instance.vllm_engine:
            framework_status["vllm"] = True
        else:
            framework_status["vllm"] = False
            
        if hasattr(controller_instance, 'tensorrt_engine') and controller_instance.tensorrt_engine:
            framework_status["tensorrt"] = True
        else:
            framework_status["tensorrt"] = False
        
        uptime = (datetime.now() - server_stats["start_time"]).total_seconds()
        
        return HealthResponse(
            status="healthy",
            timestamp=datetime.now().isoformat(),
            version="1.0.0",
            uptime_seconds=uptime,
            framework_status=framework_status
        )
        
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=503, detail="å¥åº·æ£€æŸ¥å¤±è´¥")

@app.get("/stats", response_model=StatsResponse)
async def get_stats(controller_instance = Depends(get_controller)):
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è®¡ç®—è¿è¡Œæ—¶é—´
        uptime = (datetime.now() - server_stats["start_time"]).total_seconds()
        
        # æœåŠ¡å™¨ç»Ÿè®¡
        server_statistics = {
            "uptime_seconds": uptime,
            "total_requests": server_stats["total_requests"],
            "successful_requests": server_stats["successful_requests"],
            "failed_requests": server_stats["failed_requests"],
            "success_rate": (
                server_stats["successful_requests"] / server_stats["total_requests"] 
                if server_stats["total_requests"] > 0 else 0
            ),
            "average_inference_time_ms": (
                server_stats["total_inference_time"] / server_stats["successful_requests"]
                if server_stats["successful_requests"] > 0 else 0
            )
        }
        
        # æ§åˆ¶å™¨ç»Ÿè®¡
        controller_statistics = {}
        if hasattr(controller_instance, 'get_comprehensive_stats'):
            controller_statistics = controller_instance.get_comprehensive_stats()
        
        # ç³»ç»Ÿä¿¡æ¯
        system_information = {
            "python_version": sys.version,
            "platform": sys.platform,
            "cwd": os.getcwd()
        }
        
        return StatsResponse(
            server_stats=server_statistics,
            controller_stats=controller_statistics,
            system_info=system_information
        )
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥")

@app.post("/infer", response_model=InferenceResponse)
async def infer_recommendations(
    request: InferenceRequest,
    background_tasks: BackgroundTasks,
    controller_instance = Depends(get_controller)
):
    """å•æ¬¡æ¨ç†æ¥å£"""
    request_id = f"req_{int(time.time() * 1000)}"
    start_time = time.time()
    
    try:
        logger.info(f"[{request_id}] å¼€å§‹å¤„ç†æ¨ç†è¯·æ±‚ - ç”¨æˆ·: {request.user_id}")
        
        # è½¬æ¢ç”¨æˆ·è¡Œä¸ºæ•°æ®æ ¼å¼
        user_behaviors = []
        for behavior in request.user_behaviors:
            user_behaviors.append({
                'video_id': behavior.video_id,
                'timestamp': behavior.timestamp,
                'interaction_type': behavior.interaction_type,
                'duration': behavior.duration,
                'device_type': behavior.device_type
            })
        
        # è°ƒç”¨ç°æœ‰çš„ç»Ÿä¸€æ¨ç†ç®¡é“
        result = controller_instance.infer_with_optimal_strategy(
            user_id=request.user_id,
            session_id=request.session_id,
            user_behaviors=user_behaviors,
            num_recommendations=request.num_recommendations,
            requested_strategy=request.strategy
        )
        
        # æå–æ¨ç†ç»“æœå’ŒæŒ‡æ ‡
        recommendations = []
        if 'recommendations' in result:
            for i, rec in enumerate(result['recommendations'][:request.num_recommendations]):
                recommendations.append(RecommendationItem(
                    video_id=rec.get('video_id', i + 1),
                    score=rec.get('score', 0.5 + i * 0.1),
                    confidence=rec.get('confidence', 0.8 + i * 0.02),
                    reason=rec.get('reason', f"åŸºäº{request.strategy}ç­–ç•¥æ¨è")
                ))
        
        # æ„å»ºæ€§èƒ½æŒ‡æ ‡
        total_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        metrics = InferenceMetrics(
            total_time_ms=total_time,
            feature_processing_time_ms=result.get('feature_processing_time_ms', 0.0),
            model_inference_time_ms=result.get('model_inference_time_ms', 0.0),
            strategy_used=result.get('strategy_used', request.strategy),
            cache_hit=result.get('cache_hit', False),
            gpu_utilization=result.get('gpu_utilization')
        )
        
        # æ„å»ºå“åº”
        response = InferenceResponse(
            user_id=request.user_id,
            session_id=request.session_id,
            recommendations=recommendations,
            metrics=metrics,
            timestamp=datetime.now().isoformat(),
            request_id=request_id
        )
        
        # æ›´æ–°ç»Ÿè®¡
        update_server_stats(success=True, inference_time=total_time)
        
        logger.info(f"[{request_id}] æ¨ç†å®Œæˆ - è€—æ—¶: {total_time:.2f}ms, ç­–ç•¥: {metrics.strategy_used}")
        
        return response
        
    except Exception as e:
        total_time = (time.time() - start_time) * 1000
        update_server_stats(success=False)
        
        logger.error(f"[{request_id}] æ¨ç†å¤±è´¥ - è€—æ—¶: {total_time:.2f}ms, é”™è¯¯: {e}")
        logger.error(traceback.format_exc())
        
        raise HTTPException(
            status_code=500,
            detail={
                "error": "æ¨ç†å¤±è´¥",
                "message": str(e),
                "request_id": request_id,
                "processing_time_ms": total_time
            }
        )

@app.post("/batch_infer", response_model=BatchInferenceResponse)
async def batch_infer_recommendations(
    request: BatchInferenceRequest,
    background_tasks: BackgroundTasks,
    controller_instance = Depends(get_controller)
):
    """æ‰¹é‡æ¨ç†æ¥å£"""
    batch_id = f"batch_{int(time.time() * 1000)}"
    start_time = time.time()
    
    try:
        logger.info(f"[{batch_id}] å¼€å§‹å¤„ç†æ‰¹é‡æ¨ç†è¯·æ±‚ - æ•°é‡: {len(request.requests)}")
        
        results = []
        
        # å¤„ç†æ¯ä¸ªæ¨ç†è¯·æ±‚
        for i, inference_request in enumerate(request.requests):
            try:
                # è°ƒç”¨å•æ¬¡æ¨ç†é€»è¾‘
                response = await infer_recommendations(
                    request=inference_request,
                    background_tasks=background_tasks,
                    controller_instance=controller_instance
                )
                results.append(response)
                
            except HTTPException as e:
                logger.warning(f"[{batch_id}] æ‰¹é‡æ¨ç†ä¸­ç¬¬{i+1}ä¸ªè¯·æ±‚å¤±è´¥: {e.detail}")
                # æ‰¹é‡æ¨ç†ä¸­éƒ¨åˆ†å¤±è´¥ä¸å½±å“æ•´ä½“å¤„ç†
                continue
        
        # è®¡ç®—æ‰¹é‡æŒ‡æ ‡
        total_time = (time.time() - start_time) * 1000
        batch_metrics = {
            "total_requests": len(request.requests),
            "successful_requests": len(results),
            "failed_requests": len(request.requests) - len(results),
            "batch_processing_time_ms": total_time,
            "average_request_time_ms": (
                sum(r.metrics.total_time_ms for r in results) / len(results)
                if results else 0
            )
        }
        
        logger.info(f"[{batch_id}] æ‰¹é‡æ¨ç†å®Œæˆ - æˆåŠŸ: {len(results)}/{len(request.requests)}, è€—æ—¶: {total_time:.2f}ms")
        
        return BatchInferenceResponse(
            results=results,
            batch_metrics=batch_metrics
        )
        
    except Exception as e:
        total_time = (time.time() - start_time) * 1000
        logger.error(f"[{batch_id}] æ‰¹é‡æ¨ç†å¤±è´¥ - è€—æ—¶: {total_time:.2f}ms, é”™è¯¯: {e}")
        logger.error(traceback.format_exc())
        
        raise HTTPException(
            status_code=500,
            detail={
                "error": "æ‰¹é‡æ¨ç†å¤±è´¥",
                "message": str(e),
                "batch_id": batch_id,
                "processing_time_ms": total_time
            }
        )

@app.post("/generate_demo_data", response_model=dict)
async def generate_demo_data(
    user_id: str,
    num_behaviors: int = Field(20, ge=1, le=100)
):
    """ç”Ÿæˆæ¼”ç¤ºæ•°æ®"""
    try:
        # ä½¿ç”¨ç°æœ‰çš„æ•°æ®ç”Ÿæˆå‡½æ•°
        demo_behaviors = create_realistic_user_behaviors(
            user_id=user_id,
            num_behaviors=num_behaviors
        )
        
        # è½¬æ¢ä¸ºAPIæ ¼å¼
        api_behaviors = []
        for behavior in demo_behaviors:
            api_behaviors.append(UserBehavior(
                video_id=behavior['video_id'],
                timestamp=behavior['timestamp'],
                interaction_type=behavior['interaction_type'],
                duration=behavior.get('duration'),
                device_type=behavior.get('device_type', 'mobile')
            ))
        
        return {
            "user_id": user_id,
            "behaviors": [behavior.dict() for behavior in api_behaviors],
            "count": len(api_behaviors),
            "message": "æ¼”ç¤ºæ•°æ®ç”ŸæˆæˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¼”ç¤ºæ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ¼”ç¤ºæ•°æ®å¤±è´¥: {str(e)}")

# ============================= é”™è¯¯å¤„ç† =============================

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """HTTPå¼‚å¸¸å¤„ç†å™¨"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": "HTTPé”™è¯¯",
            "status_code": exc.status_code,
            "detail": exc.detail,
            "timestamp": datetime.now().isoformat()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """é€šç”¨å¼‚å¸¸å¤„ç†å™¨"""
    logger.error(f"æœªå¤„ç†çš„å¼‚å¸¸: {exc}")
    logger.error(traceback.format_exc())
    
    return JSONResponse(
        status_code=500,
        content={
            "error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
            "message": "æœåŠ¡é‡åˆ°äº†æœªé¢„æœŸçš„é”™è¯¯",
            "timestamp": datetime.now().isoformat()
        }
    )

# ============================= ä¸»å‡½æ•° =============================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡')
    parser.add_argument('--host', default='0.0.0.0', help='æœåŠ¡å™¨ä¸»æœºåœ°å€')
    parser.add_argument('--port', type=int, default=8000, help='æœåŠ¡å™¨ç«¯å£')
    parser.add_argument('--workers', type=int, default=1, help='å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--reload', action='store_true', help='å¼€å‘æ¨¡å¼è‡ªåŠ¨é‡è½½')
    parser.add_argument('--log-level', default='info', help='æ—¥å¿—çº§åˆ«')
    
    args = parser.parse_args()
    
    print("ğŸŒŸ" * 50)
    print("HSTUæ¨ç†ä¼˜åŒ–APIæœåŠ¡")
    print("ğŸŒŸ" * 50)
    print("åŸºäºå¼€æºæ¡†æ¶çš„ç”Ÿæˆå¼æ¨èç³»ç»Ÿæ¨ç†ä¼˜åŒ–æœåŠ¡")
    print("é›†æˆæŠ€æœ¯æ ˆ:")
    print("  ğŸ“š Meta HSTU (Hierarchical Sequential Transduction Units)")
    print("  âš¡ VLLM (PagedAttention + Continuous Batching)")
    print("  ğŸš€ TensorRT (GPU Inference Acceleration)")
    print("  ğŸ”§ Custom Triton + CUTLASS Operators")
    print("  ğŸ§  Intelligent GPU Hot Cache")
    print("  ğŸŒ FastAPI RESTful Service")
    print("ğŸŒŸ" * 50)
    print(f"ğŸš€ å¯åŠ¨æœåŠ¡: http://{args.host}:{args.port}")
    print(f"ğŸ“š APIæ–‡æ¡£: http://{args.host}:{args.port}/docs")
    print("ğŸŒŸ" * 50)
    
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        "api_server:app",
        host=args.host,
        port=args.port,
        workers=args.workers,
        reload=args.reload,
        log_level=args.log_level
    )

if __name__ == "__main__":
    main()