#!/usr/bin/env python3
"""
åŸºäºå¼€æºæ¡†æ¶çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹æ¨ç†ä¼˜åŒ–é¡¹ç›®

é›†æˆäº†çœŸæ­£çš„å¼€æºæ¡†æ¶ï¼š
- Meta HSTU (Hierarchical Sequential Transduction Units) ç”Ÿæˆå¼æ¨èæ¨¡å‹
- VLLM æ¨ç†ä¼˜åŒ–æ¡†æ¶ (PagedAttention + Continuous Batching)  
- TensorRT GPUæ¨ç†åŠ é€Ÿ
- è‡ªå®šä¹‰Tritonå’ŒCUTLASSç®—å­ä¼˜åŒ–
- æ™ºèƒ½GPUçƒ­ç¼“å­˜ç³»ç»Ÿ

ä¸å†ä½¿ç”¨æ‰‹å†™çš„æ¨¡æ‹Ÿå®ç°ï¼Œè€Œæ˜¯åŸºäºçœŸæ­£çš„å¼€æºæŠ€æœ¯æ ˆã€‚
"""

import sys
import os
import json
import logging
import argparse
import time
import asyncio
from datetime import datetime
from typing import List, Dict, Any
from collections import defaultdict
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é›†æˆæ§åˆ¶å™¨
from integrations.framework_controller import OpenSourceFrameworkController, create_integrated_controller

# å¯¼å…¥ç¤ºä¾‹æ•°æ®ç”Ÿæˆ
from examples.client_example import create_realistic_user_behaviors

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('opensoure_inference.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def create_optimized_config():
    """åˆ›å»ºä¼˜åŒ–é…ç½®"""
    
    config = {
        # Meta HSTUæ¨¡å‹é…ç½®
        'hstu': {
            'vocab_size': 50000,          # è¯æ±‡è¡¨å¤§å°
            'd_model': 1024,              # æ¨¡å‹éšè—ç»´åº¦
            'num_layers': 12,             # å±‚æ•° (çº¦3.2Bå‚æ•°)
            'num_heads': 16,              # æ³¨æ„åŠ›å¤´æ•°
            'd_ff': 4096,                 # FFNç»´åº¦
            'max_seq_len': 2048,          # æœ€å¤§åºåˆ—é•¿åº¦
            'dropout': 0.1,               # Dropoutç‡
            'hstu_expansion_factor': 4,   # HSTUæ‰©å±•å› å­
            'hstu_gate_type': 'sigmoid',  # é—¨æ§ç±»å‹
            'enable_hierarchical_attention': True,  # å¯ç”¨åˆ†å±‚æ³¨æ„åŠ›
            'similarity_dim': 256,        # ç›¸ä¼¼åº¦è®¡ç®—ç»´åº¦
            'temperature': 0.1,           # æ¸©åº¦å‚æ•°
            'pretrained_path': None,      # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„(å¦‚æœæœ‰)
        },
        
        # VLLMæ¨ç†ä¼˜åŒ–é…ç½®
        'vllm': {
            'model_name': 'hstu-generative-recommender',
            'model_path': None,           # å¦‚æœæœ‰æœ¬åœ°æ¨¡å‹è·¯å¾„
            'tensor_parallel_size': 1,    # å¼ é‡å¹¶è¡Œå¤§å°
            'pipeline_parallel_size': 1,  # æµæ°´çº¿å¹¶è¡Œå¤§å°
            'gpu_memory_utilization': 0.85,  # GPUå†…å­˜åˆ©ç”¨ç‡
            'max_model_len': 2048,        # æœ€å¤§æ¨¡å‹é•¿åº¦
            'max_num_seqs': 256,          # æœ€å¤§å¹¶å‘åºåˆ—æ•°
            'max_num_batched_tokens': None,  # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
            'block_size': 16,             # PagedAttentionå—å¤§å°
            'dtype': 'float16',           # æ•°æ®ç±»å‹
            'seed': 42,                   # éšæœºç§å­
            'quantization': None,         # é‡åŒ–æ–¹å¼ (None, 'gptq', 'awq')
            'enable_chunked_prefill': True,  # å¯ç”¨åˆ†å—é¢„å¡«å……
        },
        
        # TensorRTæ¨ç†åŠ é€Ÿé…ç½®
        'tensorrt': {
            'model_name': 'hstu-tensorrt-optimized',
            'onnx_path': None,            # ONNXæ¨¡å‹è·¯å¾„
            'engine_path': 'models/hstu_fp16.trt',  # TensorRTå¼•æ“è·¯å¾„
            'precision': 'fp16',          # ç²¾åº¦æ¨¡å¼ ('fp32', 'fp16', 'int8')
            'max_batch_size': 8,          # æœ€å¤§æ‰¹å¤„ç†å¤§å°
            'max_workspace_size': 2 << 30,  # æœ€å¤§å·¥ä½œç©ºé—´ (2GB)
            'optimization_level': 5,      # ä¼˜åŒ–ç­‰çº§ (0-5)
            'enable_dynamic_shapes': True,  # å¯ç”¨åŠ¨æ€å½¢çŠ¶
            'enable_strict_types': False, # å¯ç”¨ä¸¥æ ¼ç±»å‹
            'enable_fp16_io': True,       # å¯ç”¨FP16 I/O
            # åŠ¨æ€å½¢çŠ¶é…ç½®
            'min_shapes': {
                'input_ids': (1, 8),
                'attention_mask': (1, 8),
                'dense_features': (1, 1024),
            },
            'opt_shapes': {
                'input_ids': (4, 64),
                'attention_mask': (4, 64), 
                'dense_features': (4, 1024),
            },
            'max_shapes': {
                'input_ids': (8, 2048),
                'attention_mask': (8, 2048),
                'dense_features': (8, 1024),
            },
        },
        
        # è‡ªå®šä¹‰ç®—å­ä¼˜åŒ–é…ç½®
        'custom_operators': {
            'cache_size': 2000,           # ç®—å­ç¼“å­˜å¤§å°
            'enable_benchmarking': True,  # å¯ç”¨æ€§èƒ½åŸºå‡†æµ‹è¯•
            'triton_block_size': 64,      # Tritonå—å¤§å°
            'enable_cutlass': True,       # å¯ç”¨CUTLASSç®—å­
            'fusion_threshold': 32,       # ç®—å­èåˆé˜ˆå€¼
        },
        
        # æ™ºèƒ½GPUçƒ­ç¼“å­˜é…ç½®
        'intelligent_cache': {
            'gpu_cache_size': 8192,       # GPUç¼“å­˜å¤§å°
            'embedding_dim': 1024,        # åµŒå…¥ç»´åº¦
            'enable_prediction': True,    # å¯ç”¨çƒ­ç‚¹é¢„æµ‹
            'dtype': 'float32',           # æ•°æ®ç±»å‹
            'prediction_window': 1000,    # é¢„æµ‹çª—å£å¤§å°
            'decay_factor': 0.95,         # è¡°å‡å› å­
        },
        
        # æ¨ç†ç­–ç•¥é…ç½®
        'inference_strategy': {
            'auto_selection': True,       # è‡ªåŠ¨ç­–ç•¥é€‰æ‹©
            'vllm_sequence_threshold': 100,    # VLLMåºåˆ—é•¿åº¦é˜ˆå€¼
            'tensorrt_sequence_threshold': 50, # TensorRTåºåˆ—é•¿åº¦é˜ˆå€¼
            'fallback_strategy': 'hstu',  # å›é€€ç­–ç•¥
            'enable_batching': True,      # å¯ç”¨æ‰¹å¤„ç†
            'batch_timeout_ms': 50,       # æ‰¹å¤„ç†è¶…æ—¶
        },
        
        # æ€§èƒ½ç›‘æ§é…ç½®
        'monitoring': {
            'enable_detailed_logging': True,  # å¯ç”¨è¯¦ç»†æ—¥å¿—
            'log_inference_time': True,   # è®°å½•æ¨ç†æ—¶é—´
            'log_memory_usage': True,     # è®°å½•å†…å­˜ä½¿ç”¨
            'benchmark_interval': 100,    # åŸºå‡†æµ‹è¯•é—´éš”
            'save_performance_metrics': True,  # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
        }
    }
    
    return config

class OpenSourceRecommenderSystem:
    """åŸºäºå¼€æºæ¡†æ¶çš„æ¨èç³»ç»Ÿ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.performance_metrics = defaultdict(list)
        
        # åˆ›å»ºé›†æˆæ§åˆ¶å™¨
        logger.info("åˆå§‹åŒ–åŸºäºå¼€æºæ¡†æ¶çš„æ¨èç³»ç»Ÿ...")
        self.controller = create_integrated_controller(config)
        
        # æ£€æŸ¥æ¡†æ¶å¯ç”¨æ€§
        self._check_framework_availability()
        
        logger.info("âœ… å¼€æºæ¨èç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _check_framework_availability(self):
        """æ£€æŸ¥æ¡†æ¶å¯ç”¨æ€§"""
        availability = self.controller.framework_availability
        
        logger.info("å¼€æºæ¡†æ¶å¯ç”¨æ€§æ£€æŸ¥:")
        logger.info(f"  Meta HSTUæ¨¡å‹: {'âœ…' if availability['hstu'] else 'âŒ'}")
        logger.info(f"  VLLMæ¨ç†å¼•æ“: {'âœ…' if availability['vllm'] else 'âŒ'}")
        logger.info(f"  TensorRTåŠ é€Ÿ: {'âœ…' if availability['tensorrt'] else 'âŒ'}")
        logger.info(f"  Tritonç®—å­: {'âœ…' if availability['triton_ops'] else 'âŒ'}")
        logger.info(f"  æ™ºèƒ½ç¼“å­˜: {'âœ…' if availability['cache'] else 'âŒ'}")
        
        available_count = sum(availability.values())
        total_count = len(availability)
        logger.info(f"æ€»ä½“å¯ç”¨æ€§: {available_count}/{total_count} ({available_count/total_count:.1%})")
    
    def generate_recommendations(
        self,
        user_id: str,
        session_id: str,
        user_behaviors: List[Dict[str, Any]],
        num_recommendations: int = 10,
        strategy: str = "auto",
        **kwargs
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨èç»“æœ"""
        
        logger.info(f"ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆ {num_recommendations} ä¸ªæ¨è (ç­–ç•¥: {strategy})")
        
        # ä½¿ç”¨é›†æˆæ§åˆ¶å™¨è¿›è¡Œæ¨ç†
        result = self.controller.infer_with_optimal_strategy(
            user_id=user_id,
            session_id=session_id,
            user_behaviors=user_behaviors,
            num_recommendations=num_recommendations,
            strategy=strategy,
            **kwargs
        )
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        if 'inference_time_ms' in result:
            self.performance_metrics['inference_times'].append(result['inference_time_ms'])
            self.performance_metrics['strategies'].append(result.get('inference_strategy', 'unknown'))
        
        return result
    
    async def batch_generate_recommendations(
        self,
        requests: List[Dict[str, Any]],
        **kwargs
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆæ¨è"""
        
        logger.info(f"æ‰¹é‡ç”Ÿæˆæ¨èï¼Œè¯·æ±‚æ•°é‡: {len(requests)}")
        
        start_time = time.time()
        results = await self.controller.batch_infer(requests, **kwargs)
        batch_time = time.time() - start_time
        
        logger.info(f"æ‰¹é‡æ¨ç†å®Œæˆï¼Œæ€»è€—æ—¶: {batch_time:.2f}sï¼Œå¹³å‡æ¯è¯·æ±‚: {batch_time/len(requests)*1000:.2f}ms")
        
        return results
    
    def benchmark_performance(
        self,
        test_cases: List[Dict[str, Any]],
        strategies: List[str] = ['auto', 'hstu', 'vllm', 'tensorrt'],
        iterations: int = 50
    ) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        
        logger.info(f"å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œç­–ç•¥: {strategies}ï¼Œè¿­ä»£æ¬¡æ•°: {iterations}")
        
        benchmark_results = {}
        
        for strategy in strategies:
            logger.info(f"æµ‹è¯•ç­–ç•¥: {strategy}")
            times = []
            success_count = 0
            
            for test_case in test_cases:
                for _ in range(iterations):
                    start_time = time.time()
                    
                    result = self.generate_recommendations(
                        user_id=f"bench_user_{success_count}",
                        session_id=f"bench_session_{success_count}",
                        user_behaviors=test_case.get('user_behaviors', []),
                        num_recommendations=test_case.get('num_recommendations', 10),
                        strategy=strategy
                    )
                    
                    end_time = time.time()
                    
                    if 'error' not in result:
                        times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                        success_count += 1
            
            if times:
                benchmark_results[strategy] = {
                    'avg_latency_ms': np.mean(times),
                    'min_latency_ms': np.min(times),
                    'max_latency_ms': np.max(times),
                    'p50_latency_ms': np.percentile(times, 50),
                    'p95_latency_ms': np.percentile(times, 95),
                    'p99_latency_ms': np.percentile(times, 99),
                    'std_latency_ms': np.std(times),
                    'throughput_rps': 1000 / np.mean(times) if times else 0,
                    'success_rate': success_count / (len(test_cases) * iterations),
                    'total_tests': len(test_cases) * iterations,
                    'successful_tests': success_count,
                }
                
                logger.info(f"ç­–ç•¥ {strategy} ç»“æœ:")
                logger.info(f"  å¹³å‡å»¶è¿Ÿ: {benchmark_results[strategy]['avg_latency_ms']:.2f}ms")
                logger.info(f"  P95å»¶è¿Ÿ: {benchmark_results[strategy]['p95_latency_ms']:.2f}ms") 
                logger.info(f"  ååé‡: {benchmark_results[strategy]['throughput_rps']:.2f} RPS")
                logger.info(f"  æˆåŠŸç‡: {benchmark_results[strategy]['success_rate']:.2%}")
        
        return benchmark_results
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        
        # è·å–æ§åˆ¶å™¨ç»Ÿè®¡
        controller_stats = self.controller.get_comprehensive_stats()
        
        # æ·»åŠ ç³»ç»Ÿçº§ç»Ÿè®¡
        system_stats = {
            'system_info': {
                'python_version': sys.version,
                'cuda_available': self._check_cuda_availability(),
                'gpu_count': self._get_gpu_count(),
                'memory_usage': self._get_memory_usage(),
            },
            'performance_metrics': dict(self.performance_metrics),
            'uptime': time.time(),  # å¯ä»¥æ·»åŠ å¯åŠ¨æ—¶é—´è·Ÿè¸ª
        }
        
        return {
            'controller_stats': controller_stats,
            'system_stats': system_stats,
            'timestamp': datetime.now().isoformat(),
        }
    
    def _check_cuda_availability(self) -> bool:
        """æ£€æŸ¥CUDAå¯ç”¨æ€§"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def _get_gpu_count(self) -> int:
        """è·å–GPUæ•°é‡"""
        try:
            import torch
            return torch.cuda.device_count() if torch.cuda.is_available() else 0
        except ImportError:
            return 0
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            import torch
            
            # ç³»ç»Ÿå†…å­˜
            system_memory = psutil.virtual_memory()
            
            memory_info = {
                'system_memory_total_gb': system_memory.total / (1024**3),
                'system_memory_used_gb': system_memory.used / (1024**3),
                'system_memory_percent': system_memory.percent,
            }
            
            # GPUå†…å­˜
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory
                    gpu_allocated = torch.cuda.memory_allocated(i)
                    gpu_reserved = torch.cuda.memory_reserved(i)
                    
                    memory_info[f'gpu_{i}_total_gb'] = gpu_memory / (1024**3)
                    memory_info[f'gpu_{i}_allocated_gb'] = gpu_allocated / (1024**3)
                    memory_info[f'gpu_{i}_reserved_gb'] = gpu_reserved / (1024**3)
                    memory_info[f'gpu_{i}_utilization'] = gpu_allocated / gpu_memory
            
            return memory_info
            
        except ImportError:
            return {'memory_info': 'psutil not available'}

def run_single_inference_demo():
    """è¿è¡Œå•æ¬¡æ¨ç†æ¼”ç¤º"""
    logger.info("ğŸš€ å¼€å§‹å•æ¬¡æ¨ç†æ¼”ç¤º...")
    
    # åˆ›å»ºé…ç½®
    config = create_optimized_config()
    
    # åˆ›å»ºæ¨èç³»ç»Ÿ
    recommender = OpenSourceRecommenderSystem(config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    user_behaviors = create_realistic_user_behaviors("demo_user", 15)
    
    # ç”Ÿæˆæ¨è
    result = recommender.generate_recommendations(
        user_id="demo_user_001",
        session_id="demo_session_001", 
        user_behaviors=user_behaviors,
        num_recommendations=10,
        strategy="unified"
    )
    
    print("\n" + "="*80)
    print("å•æ¬¡æ¨ç†æ¼”ç¤ºç»“æœ")
    print("="*80)
    print(f"ç”¨æˆ·ID: {result.get('user_id', 'unknown')}")
    print(f"æ¨ç†ç­–ç•¥: {result.get('inference_strategy', 'unknown')}")
    print(f"æ¨ç†æ—¶é—´: {result.get('inference_time_ms', 0):.2f}ms")
    print(f"å¼•æ“ç±»å‹: {result.get('engine_type', 'unknown')}")
    
    if 'recommendations' in result:
        print(f"\nğŸ“ ç”Ÿæˆäº† {len(result['recommendations'])} ä¸ªæ¨è:")
        for i, rec in enumerate(result['recommendations'][:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  {i+1}. {rec.get('video_id', 'unknown')} "
                  f"(åˆ†æ•°: {rec.get('score', 0):.4f}) - {rec.get('reason', '')}")
    
    if 'error' in result:
        print(f"\nâŒ æ¨ç†å‡ºé”™: {result['error']}")
    
    return result

def run_batch_inference_demo():
    """è¿è¡Œæ‰¹é‡æ¨ç†æ¼”ç¤º"""
    logger.info("ğŸš€ å¼€å§‹æ‰¹é‡æ¨ç†æ¼”ç¤º...")
    
    config = create_optimized_config()
    recommender = OpenSourceRecommenderSystem(config)
    
    # åˆ›å»ºæ‰¹é‡è¯·æ±‚
    batch_requests = []
    strategies = ['unified', 'hstu', 'vllm', 'tensorrt']  # ä¸»è¦ä½¿ç”¨unifiedæµç¨‹
    
    for i in range(8):  # 8ä¸ªå¹¶å‘è¯·æ±‚
        user_behaviors = create_realistic_user_behaviors(f"batch_user_{i}", 10)
        strategy = strategies[i % len(strategies)]
        
        batch_requests.append({
            'user_id': f'batch_user_{i}',
            'session_id': f'batch_session_{i}',
            'user_behaviors': user_behaviors,
            'num_recommendations': 5,
            'strategy': strategy
        })
    
    # æ‰§è¡Œæ‰¹é‡æ¨ç†
    async def run_batch():
        return await recommender.batch_generate_recommendations(batch_requests)
    
    batch_results = asyncio.run(run_batch())
    
    print("\n" + "="*80)
    print("æ‰¹é‡æ¨ç†æ¼”ç¤ºç»“æœ")
    print("="*80)
    
    strategy_stats = defaultdict(list)
    
    for i, result in enumerate(batch_results):
        strategy = result.get('inference_strategy', 'unknown')
        inference_time = result.get('inference_time_ms', 0)
        num_recs = len(result.get('recommendations', []))
        
        strategy_stats[strategy].append(inference_time)
        
        print(f"è¯·æ±‚ {i+1}: ç­–ç•¥={strategy}, æ—¶é—´={inference_time:.2f}ms, æ¨èæ•°={num_recs}")
    
    print("\nğŸ“Š ç­–ç•¥æ€§èƒ½ç»Ÿè®¡:")
    for strategy, times in strategy_stats.items():
        if times:
            print(f"  {strategy}: å¹³å‡ {np.mean(times):.2f}ms, "
                  f"æœ€å° {np.min(times):.2f}ms, æœ€å¤§ {np.max(times):.2f}ms")
    
    return batch_results

def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    config = create_optimized_config()
    recommender = OpenSourceRecommenderSystem(config)
    
    # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            'user_behaviors': create_realistic_user_behaviors("bench_user_short", 5),
            'num_recommendations': 5,
            'name': 'short_sequence'
        },
        {
            'user_behaviors': create_realistic_user_behaviors("bench_user_medium", 25),
            'num_recommendations': 10,
            'name': 'medium_sequence'
        },
        {
            'user_behaviors': create_realistic_user_behaviors("bench_user_long", 100),
            'num_recommendations': 15,
            'name': 'long_sequence'
        },
    ]
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_results = recommender.benchmark_performance(
        test_cases=test_cases,
        strategies=['unified', 'hstu', 'vllm', 'tensorrt'],
        iterations=20
    )
    
    print("\n" + "="*80)
    print("æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ")
    print("="*80)
    
    # è¾“å‡ºè¯¦ç»†ç»“æœ
    for strategy, metrics in benchmark_results.items():
        print(f"\nğŸ“ˆ ç­–ç•¥: {strategy.upper()}")
        print(f"  å¹³å‡å»¶è¿Ÿ: {metrics['avg_latency_ms']:.2f}ms")
        print(f"  P95å»¶è¿Ÿ: {metrics['p95_latency_ms']:.2f}ms")
        print(f"  P99å»¶è¿Ÿ: {metrics['p99_latency_ms']:.2f}ms")
        print(f"  ååé‡: {metrics['throughput_rps']:.2f} RPS")
        print(f"  æˆåŠŸç‡: {metrics['success_rate']:.2%}")
        print(f"  æ ‡å‡†å·®: {metrics['std_latency_ms']:.2f}ms")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"benchmark_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return benchmark_results

def run_comprehensive_demo():
    """è¿è¡Œç»¼åˆæ¼”ç¤º"""
    logger.info("ğŸš€ å¼€å§‹ç»¼åˆæ¼”ç¤º...")
    
    config = create_optimized_config()
    recommender = OpenSourceRecommenderSystem(config)
    
    print("\n" + "="*80)
    print("å¼€æºæ¡†æ¶é›†æˆæ¨èç³»ç»Ÿ - ç»¼åˆæ¼”ç¤º")
    print("="*80)
    
    # 1. å•æ¬¡æ¨ç†æ¼”ç¤º
    print("\nğŸ”¸ 1. å•æ¬¡æ¨ç†æ¼”ç¤º")
    single_result = run_single_inference_demo()
    
    # 2. æ‰¹é‡æ¨ç†æ¼”ç¤º
    print("\nğŸ”¸ 2. æ‰¹é‡æ¨ç†æ¼”ç¤º")
    batch_results = run_batch_inference_demo()
    
    # 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\nğŸ”¸ 3. æ€§èƒ½åŸºå‡†æµ‹è¯•")
    benchmark_results = run_performance_benchmark()
    
    # 4. ç»¼åˆç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ”¸ 4. ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
    stats = recommender.get_comprehensive_stats()
    
    print("\nğŸ“Š æ¡†æ¶å¯ç”¨æ€§:")
    framework_availability = stats['controller_stats']['framework_availability']
    for framework, available in framework_availability.items():
        print(f"  {framework}: {'âœ… å¯ç”¨' if available else 'âŒ ä¸å¯ç”¨'}")
    
    print(f"\nğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    system_info = stats['system_stats']['system_info']
    print(f"  CUDAå¯ç”¨: {'âœ…' if system_info['cuda_available'] else 'âŒ'}")
    print(f"  GPUæ•°é‡: {system_info['gpu_count']}")
    
    if 'system_memory_total_gb' in system_info['memory_usage']:
        memory_info = system_info['memory_usage']
        print(f"  ç³»ç»Ÿå†…å­˜: {memory_info['system_memory_used_gb']:.1f}GB / {memory_info['system_memory_total_gb']:.1f}GB")
    
    print(f"\nğŸ† æ€»æ¨ç†æ¬¡æ•°: {stats['controller_stats']['total_inferences']}")
    
    return {
        'single_result': single_result,
        'batch_results': batch_results,
        'benchmark_results': benchmark_results,
        'system_stats': stats,
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŸºäºå¼€æºæ¡†æ¶çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹æ¨ç†ä¼˜åŒ–é¡¹ç›®')
    parser.add_argument('--mode', 
                       choices=['single', 'batch', 'benchmark', 'comprehensive'], 
                       default='comprehensive',
                       help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log-level', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', 
                       help='æ—¥å¿—çº§åˆ«')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    print("ğŸŒŸ" * 40)
    print("åŸºäºå¼€æºæ¡†æ¶çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹æ¨ç†ä¼˜åŒ–é¡¹ç›®")
    print("ğŸŒŸ" * 40)
    print("é›†æˆæŠ€æœ¯æ ˆ:")
    print("  ğŸ“š Meta HSTU (Hierarchical Sequential Transduction Units)")
    print("  âš¡ VLLM (PagedAttention + Continuous Batching)")
    print("  ğŸš€ TensorRT (GPU Inference Acceleration)")
    print("  ğŸ”§ Custom Triton + CUTLASS Operators")
    print("  ğŸ§  Intelligent GPU Hot Cache")
    print("ğŸŒŸ" * 40)
    
    try:
        # è¿è¡ŒæŒ‡å®šæ¨¡å¼
        if args.mode == 'single':
            result = run_single_inference_demo()
        elif args.mode == 'batch':
            result = run_batch_inference_demo()
        elif args.mode == 'benchmark':
            result = run_performance_benchmark()
        else:  # comprehensive
            result = run_comprehensive_demo()
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“‹ ç›‘æ§å’Œæ—¥å¿—:")
        print("  - æ¨ç†æ—¥å¿—: tail -f opensoure_inference.log")
        print("  - åŸºå‡†æµ‹è¯•ç»“æœ: benchmark_results_*.json")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
        return 1
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())