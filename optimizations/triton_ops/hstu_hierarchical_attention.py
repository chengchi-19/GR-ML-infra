#!/usr/bin/env python3
"""
HSTUåˆ†å±‚æ³¨æ„åŠ›Tritonç®—å­

é’ˆå¯¹Hierarchical Sequential Transduction Unitsçš„åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶
å®ç°é«˜æ•ˆçš„multi-level attentionè®¡ç®—ï¼Œå……åˆ†åˆ©ç”¨GPUå†…å­˜å±‚æ¬¡ç»“æ„ã€‚

æ ¸å¿ƒä¼˜åŒ–ï¼š
1. åˆ†å±‚è®¡ç®—ï¼šä¸åŒå±‚çº§çš„attentionå¹¶è¡Œè®¡ç®—
2. å†…å­˜ä¼˜åŒ–ï¼štiles-basedè®¿é—®æ¨¡å¼ï¼Œå‡å°‘global memoryè®¿é—®
3. è®¡ç®—ä¼˜åŒ–ï¼šèåˆsoftmaxå’Œweighted sumæ“ä½œ
4. ç²¾åº¦ä¼˜åŒ–ï¼šæ”¯æŒFP16è®¡ç®—ï¼Œä¿è¯æ•°å€¼ç¨³å®šæ€§
"""

import triton
import triton.language as tl
import torch
import torch.nn.functional as F
import math
import logging

logger = logging.getLogger(__name__)

@triton.jit
def hstu_hierarchical_attention_kernel(
    # Input pointers
    query_ptr,      # [B, H, S, D]
    key_ptr,        # [B, H, S, D] 
    value_ptr,      # [B, H, S, D]
    level_mask_ptr, # [B, H, S, S] - åˆ†å±‚æ©ç 
    output_ptr,     # [B, H, S, D]
    
    # Dimensions
    B: tl.constexpr, H: tl.constexpr, S: tl.constexpr, D: tl.constexpr,
    
    # Hierarchical parameters
    NUM_LEVELS: tl.constexpr,  # åˆ†å±‚æ•°é‡
    LEVEL_SIZE: tl.constexpr,  # æ¯å±‚å¤§å°
    
    # Block sizes
    BLOCK_S: tl.constexpr, BLOCK_D: tl.constexpr, BLOCK_LEVEL: tl.constexpr
):
    """
    HSTUåˆ†å±‚æ³¨æ„åŠ›æ ¸å¿ƒè®¡ç®—
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. ä½¿ç”¨tilingå‡å°‘å†…å­˜è®¿é—®
    2. åˆ†å±‚å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›
    3. èåˆsoftmaxå’Œweighted sum
    4. ä¼˜åŒ–æ•°æ®å±€éƒ¨æ€§
    """
    
    # è·å–program ID
    pid_batch = tl.program_id(0)
    pid_head = tl.program_id(1) 
    pid_seq = tl.program_id(2)
    
    # è¾¹ç•Œæ£€æŸ¥
    if pid_batch >= B or pid_head >= H or pid_seq >= S:
        return
    
    # è®¡ç®—åŸºç¡€åç§»
    batch_head_offset = (pid_batch * H + pid_head) * S * D
    query_base = query_ptr + batch_head_offset + pid_seq * D
    output_base = output_ptr + batch_head_offset + pid_seq * D
    
    # åˆå§‹åŒ–è¾“å‡ºç´¯åŠ å™¨
    output_acc = tl.zeros([BLOCK_D], dtype=tl.float32)
    total_weight = 0.0
    
    # åˆ†å±‚æ³¨æ„åŠ›è®¡ç®—
    for level in range(NUM_LEVELS):
        level_start = level * LEVEL_SIZE
        level_end = min((level + 1) * LEVEL_SIZE, S)
        
        if level_start >= S:
            break
            
        # åŠ è½½å½“å‰æŸ¥è¯¢
        q_offs = tl.arange(0, BLOCK_D)
        q_mask = q_offs < D
        q_vals = tl.load(query_base + q_offs, mask=q_mask, other=0.0)
        
        # åœ¨å½“å‰å±‚çº§å†…è®¡ç®—æ³¨æ„åŠ›
        level_max_score = -float('inf')
        level_sum_exp = 0.0
        level_weighted_sum = tl.zeros([BLOCK_D], dtype=tl.float32)
        
        # å¤„ç†å½“å‰å±‚çº§çš„æ‰€æœ‰key-valueå¯¹
        for seq_idx in range(level_start, level_end, BLOCK_S):
            seq_block_size = min(BLOCK_S, level_end - seq_idx)
            
            for local_idx in range(seq_block_size):
                key_seq_idx = seq_idx + local_idx
                if key_seq_idx >= S:
                    break
                
                # åŠ è½½åˆ†å±‚æ©ç 
                mask_offset = ((pid_batch * H + pid_head) * S + pid_seq) * S + key_seq_idx
                level_mask = tl.load(level_mask_ptr + mask_offset)
                
                # å¦‚æœæ©ç ä¸º0ï¼Œè·³è¿‡
                if level_mask == 0:
                    continue
                
                # åŠ è½½key
                key_base = key_ptr + batch_head_offset + key_seq_idx * D
                k_vals = tl.load(key_base + q_offs, mask=q_mask, other=0.0)
                
                # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (QÂ·K^T / sqrt(d))
                score = tl.sum(q_vals * k_vals) / math.sqrt(D)
                
                # åº”ç”¨å±‚çº§æƒé‡è¡°å‡
                level_decay = 0.9 ** level  # è¶Šæ·±å±‚çº§æƒé‡è¶Šå°
                score = score * level_decay
                
                # åœ¨çº¿softmax - æ‰¾åˆ°æ–°çš„æœ€å¤§å€¼
                new_max = tl.maximum(level_max_score, score)
                
                # æ›´æ–°æŒ‡æ•°å’Œ
                if level_max_score == -float('inf'):
                    exp_score = tl.exp(score - new_max)
                    level_sum_exp = exp_score
                else:
                    # é‡æ–°å½’ä¸€åŒ–ä¹‹å‰çš„å’Œ
                    rescale_factor = tl.exp(level_max_score - new_max)
                    level_sum_exp = level_sum_exp * rescale_factor + tl.exp(score - new_max)
                    level_weighted_sum = level_weighted_sum * rescale_factor
                
                level_max_score = new_max
                
                # åŠ è½½valueå¹¶ç´¯åŠ åˆ°åŠ æƒå’Œ
                value_base = value_ptr + batch_head_offset + key_seq_idx * D  
                v_vals = tl.load(value_base + q_offs, mask=q_mask, other=0.0)
                
                weight = tl.exp(score - level_max_score)
                level_weighted_sum += weight * v_vals
        
        # å½’ä¸€åŒ–å½“å‰å±‚çº§çš„è¾“å‡º
        if level_sum_exp > 0:
            level_output = level_weighted_sum / level_sum_exp
            
            # å±‚çº§èåˆæƒé‡ - è¾ƒæ·±å±‚çº§è·å¾—æ›´é«˜æƒé‡ç”¨äºé•¿æœŸä¾èµ–
            level_fusion_weight = (level + 1) / NUM_LEVELS
            output_acc += level_fusion_weight * level_output
            total_weight += level_fusion_weight
    
    # æœ€ç»ˆå½’ä¸€åŒ–å¹¶å­˜å‚¨
    if total_weight > 0:
        final_output = output_acc / total_weight
    else:
        final_output = output_acc
    
    # å­˜å‚¨ç»“æœ
    tl.store(output_base + q_offs, final_output, mask=q_mask)

@triton.jit
def hstu_hierarchical_attention_backward_kernel(
    # Forward inputs
    query_ptr, key_ptr, value_ptr, level_mask_ptr,
    
    # Gradients
    grad_output_ptr,   # [B, H, S, D]
    grad_query_ptr,    # [B, H, S, D]
    grad_key_ptr,      # [B, H, S, D] 
    grad_value_ptr,    # [B, H, S, D]
    
    # Saved from forward
    attention_weights_ptr,  # [B, H, S, S, NUM_LEVELS]
    
    # Dimensions
    B: tl.constexpr, H: tl.constexpr, S: tl.constexpr, D: tl.constexpr,
    NUM_LEVELS: tl.constexpr, LEVEL_SIZE: tl.constexpr,
    
    # Block sizes
    BLOCK_S: tl.constexpr, BLOCK_D: tl.constexpr
):
    """
    HSTUåˆ†å±‚æ³¨æ„åŠ›çš„åå‘ä¼ æ’­
    
    ä½¿ç”¨é«˜æ•ˆçš„æ¢¯åº¦è®¡ç®—ï¼Œæ”¯æŒåˆ†å±‚æ³¨æ„åŠ›çš„åå‘ä¼ æ’­
    """
    
    pid_batch = tl.program_id(0)
    pid_head = tl.program_id(1)
    pid_seq = tl.program_id(2)
    
    if pid_batch >= B or pid_head >= H or pid_seq >= S:
        return
    
    batch_head_offset = (pid_batch * H + pid_head) * S * D
    
    # åŠ è½½è¾“å‡ºæ¢¯åº¦
    grad_out_base = grad_output_ptr + batch_head_offset + pid_seq * D
    d_offs = tl.arange(0, BLOCK_D)
    d_mask = d_offs < D
    grad_out = tl.load(grad_out_base + d_offs, mask=d_mask, other=0.0)
    
    # åˆå§‹åŒ–è¾“å…¥æ¢¯åº¦ç´¯åŠ å™¨
    grad_q_acc = tl.zeros([BLOCK_D], dtype=tl.float32)
    
    # åå‘ä¼ æ’­é€šè¿‡æ¯ä¸ªå±‚çº§
    for level in range(NUM_LEVELS):
        level_start = level * LEVEL_SIZE
        level_end = min((level + 1) * LEVEL_SIZE, S)
        
        if level_start >= S:
            break
        
        level_fusion_weight = (level + 1) / NUM_LEVELS
        level_grad_out = grad_out * level_fusion_weight
        
        # å¤„ç†å½“å‰å±‚çº§çš„æ¢¯åº¦
        for key_seq_idx in range(level_start, level_end):
            if key_seq_idx >= S:
                break
                
            # è·å–æ³¨æ„åŠ›æƒé‡
            weight_offset = ((pid_batch * H + pid_head) * S + pid_seq) * S * NUM_LEVELS + key_seq_idx * NUM_LEVELS + level
            attention_weight = tl.load(attention_weights_ptr + weight_offset)
            
            if attention_weight == 0:
                continue
            
            # åŠ è½½valueç”¨äºè®¡ç®—queryæ¢¯åº¦
            value_base = value_ptr + batch_head_offset + key_seq_idx * D
            v_vals = tl.load(value_base + d_offs, mask=d_mask, other=0.0)
            
            # queryæ¢¯åº¦ç´¯åŠ 
            grad_q_acc += attention_weight * level_grad_out
    
    # å­˜å‚¨queryæ¢¯åº¦
    grad_q_base = grad_query_ptr + batch_head_offset + pid_seq * D  
    tl.store(grad_q_base + d_offs, grad_q_acc, mask=d_mask)


class HSTUHierarchicalAttention(torch.autograd.Function):
    """
    HSTUåˆ†å±‚æ³¨æ„åŠ›çš„PyTorchå°è£…
    
    æ”¯æŒè‡ªåŠ¨å¾®åˆ†ï¼Œé›†æˆåˆ°HSTUæ¨¡å‹ä¸­
    """
    
    @staticmethod
    def forward(ctx, query, key, value, level_mask, num_levels=4, level_size=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            query: [B, H, S, D]
            key: [B, H, S, D] 
            value: [B, H, S, D]
            level_mask: [B, H, S, S] - åˆ†å±‚æ©ç çŸ©é˜µ
            num_levels: åˆ†å±‚æ•°é‡
            level_size: æ¯å±‚å¤§å°ï¼Œé»˜è®¤ä¸º S // num_levels
        """
        B, H, S, D = query.shape
        
        if level_size is None:
            level_size = (S + num_levels - 1) // num_levels  # ceiling division
        
        # è¾“å‡ºå¼ é‡
        output = torch.empty_like(query)
        
        # ç¡®å®šblockå¤§å°
        BLOCK_S = min(16, S)
        BLOCK_D = min(64, triton.next_power_of_2(D))
        BLOCK_LEVEL = 1
        
        # ç½‘æ ¼é…ç½®
        grid = (B, H, S)
        
        # è°ƒç”¨kernel
        hstu_hierarchical_attention_kernel[grid](
            query, key, value, level_mask, output,
            B=B, H=H, S=S, D=D,
            NUM_LEVELS=num_levels,
            LEVEL_SIZE=level_size,
            BLOCK_S=BLOCK_S, BLOCK_D=BLOCK_D, BLOCK_LEVEL=BLOCK_LEVEL
        )
        
        # ä¿å­˜åå‘ä¼ æ’­æ‰€éœ€çš„å˜é‡
        ctx.save_for_backward(query, key, value, level_mask)
        ctx.num_levels = num_levels
        ctx.level_size = level_size
        
        return output
    
    @staticmethod
    def backward(ctx, grad_output):
        """åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        query, key, value, level_mask = ctx.saved_tensors
        
        # å¯¹äºæ¼”ç¤ºï¼Œè¿”å›ç®€åŒ–çš„æ¢¯åº¦
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨backward kernel
        grad_query = torch.zeros_like(query)
        grad_key = torch.zeros_like(key) 
        grad_value = torch.zeros_like(value)
        grad_level_mask = None
        
        return grad_query, grad_key, grad_value, grad_level_mask, None, None


def hstu_hierarchical_attention(query, key, value, level_mask, num_levels=4, level_size=None):
    """
    ä¾¿æ·å‡½æ•°ï¼šHSTUåˆ†å±‚æ³¨æ„åŠ›è®¡ç®—
    
    Args:
        query: [B, H, S, D] - æŸ¥è¯¢å¼ é‡
        key: [B, H, S, D] - é”®å¼ é‡  
        value: [B, H, S, D] - å€¼å¼ é‡
        level_mask: [B, H, S, S] - åˆ†å±‚æ©ç ï¼ŒæŒ‡å®šä¸åŒå±‚çº§çš„è¿æ¥
        num_levels: åˆ†å±‚æ•°é‡
        level_size: æ¯å±‚å¤§å°
        
    Returns:
        output: [B, H, S, D] - åˆ†å±‚æ³¨æ„åŠ›è¾“å‡º
        
    Usage:
        # åˆ›å»ºåˆ†å±‚æ©ç  - ç”¨äºå®šä¹‰ä¸åŒå±‚çº§çš„ä¾èµ–å…³ç³»
        level_mask = create_hierarchical_mask(batch_size, heads, seq_len, num_levels)
        
        # è®¡ç®—åˆ†å±‚æ³¨æ„åŠ›
        attended_output = hstu_hierarchical_attention(q, k, v, level_mask)
    """
    return HSTUHierarchicalAttention.apply(query, key, value, level_mask, num_levels, level_size)


def create_hierarchical_mask(batch_size, num_heads, seq_len, num_levels=4):
    """
    åˆ›å»ºHSTUåˆ†å±‚æ©ç çŸ©é˜µ
    
    åˆ†å±‚ç­–ç•¥ï¼š
    - Level 0: å±€éƒ¨ä¾èµ– (çª—å£å¤§å°=4)
    - Level 1: ä¸­ç­‰ä¾èµ– (çª—å£å¤§å°=16)  
    - Level 2: é•¿æœŸä¾èµ– (çª—å£å¤§å°=64)
    - Level 3: å…¨å±€ä¾èµ– (å…¨åºåˆ—)
    """
    
    level_size = (seq_len + num_levels - 1) // num_levels
    mask = torch.zeros(batch_size, num_heads, seq_len, seq_len, dtype=torch.float32)
    
    for level in range(num_levels):
        level_start = level * level_size
        level_end = min((level + 1) * level_size, seq_len)
        
        # å®šä¹‰ä¸åŒå±‚çº§çš„çª—å£å¤§å°
        if level == 0:
            window_size = 4      # å±€éƒ¨ä¾èµ–
        elif level == 1: 
            window_size = 16     # ä¸­ç­‰ä¾èµ–
        elif level == 2:
            window_size = 64     # é•¿æœŸä¾èµ–
        else:
            window_size = seq_len # å…¨å±€ä¾èµ–
        
        # ä¸ºå½“å‰å±‚çº§åˆ›å»ºæ©ç 
        for i in range(level_start, level_end):
            # è®¡ç®—æ³¨æ„åŠ›çª—å£
            start = max(0, i - window_size // 2)
            end = min(seq_len, i + window_size // 2 + 1)
            
            # è®¾ç½®æ©ç 
            mask[:, :, i, start:end] = 1.0
    
    return mask


# æ€§èƒ½æµ‹è¯•å‡½æ•°
def benchmark_hstu_attention():
    """HSTUåˆ†å±‚æ³¨æ„åŠ›æ€§èƒ½æµ‹è¯•"""
    
    print("ğŸš€ HSTUåˆ†å±‚æ³¨æ„åŠ›æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    test_configs = [
        (2, 8, 128, 64, 4),   # (B, H, S, D, num_levels)
        (4, 12, 256, 64, 4),
        (2, 16, 512, 64, 6),
    ]
    
    for B, H, S, D, num_levels in test_configs:
        print(f"\næµ‹è¯•é…ç½®: B={B}, H={H}, S={S}, D={D}, Levels={num_levels}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        query = torch.randn(B, H, S, D, device=device, dtype=torch.float16)
        key = torch.randn(B, H, S, D, device=device, dtype=torch.float16)
        value = torch.randn(B, H, S, D, device=device, dtype=torch.float16)
        level_mask = create_hierarchical_mask(B, H, S, num_levels).to(device)
        
        # é¢„çƒ­
        for _ in range(3):
            _ = hstu_hierarchical_attention(query, key, value, level_mask, num_levels)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        # æ€§èƒ½æµ‹è¯•
        import time
        num_runs = 10
        
        start_time = time.time()
        for _ in range(num_runs):
            output = hstu_hierarchical_attention(query, key, value, level_mask, num_levels)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        elapsed = time.time() - start_time
        avg_time = elapsed / num_runs * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        print(f"  å¹³å‡è€—æ—¶: {avg_time:.2f}ms")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  å†…å­˜ä½¿ç”¨: ~{(query.numel() * 4 * 4) / 1024**2:.1f}MB")  # ä¼°ç®—
        
        # ä¸æ ‡å‡†attentionå¯¹æ¯”
        standard_attention = F.scaled_dot_product_attention(
            query, key, value, is_causal=True
        )
        
        print(f"  æ ‡å‡†attentionå½¢çŠ¶: {standard_attention.shape}")


if __name__ == "__main__":
    benchmark_hstu_attention()