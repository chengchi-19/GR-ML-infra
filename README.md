# GR æ¨ç†ä¼˜åŒ–æ¡†æ¶

ä¸€ä¸ªä¸“ä¸ºç”Ÿæˆå¼æ¨èï¼ˆGenerative Recommendation, GRï¼‰æ¨¡å‹è®¾è®¡çš„ç»¼åˆæ¨ç†ä¼˜åŒ–æ¡†æ¶ï¼Œé’ˆå¯¹å•NVIDIA A100 GPUç¯å¢ƒä¼˜åŒ–ã€‚

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

- **å®Œæ•´æµæ°´çº¿**: PyTorch â†’ ONNX â†’ TensorRT â†’ Triton (é›†æˆæ¨ç†)
- **è‡ªå®šä¹‰Tritonå†…æ ¸**: åŸºäºTriton DSLçš„é«˜æ€§èƒ½æˆå¯¹äº¤äº’æ“ä½œ
- **GPUçƒ­ç¼“å­˜**: æ™ºèƒ½åµŒå…¥ç¼“å­˜ä¸GPUå†…å­˜ä¼˜åŒ–
- **CUTLASSé›†æˆ**: å¯é€‰çš„CUTLASSåŠ é€ŸGEMMè¿ç®—
- **TensorRTæ’ä»¶**: ä¸“ç”¨æ“ä½œçš„è‡ªå®šä¹‰TensorRTæ’ä»¶
- **ç”Ÿäº§å°±ç»ª**: å®Œæ•´çš„CI/CDæµæ°´çº¿å’ŒDockeræ”¯æŒ
- **ä¼ä¸šçº§ç”¨æˆ·è¡Œä¸º**: æ‰©å±•çš„ç”¨æˆ·è¡Œä¸ºåºåˆ—å­—æ®µæ”¯æŒ
- **é«˜å¹¶å‘å¤„ç†**: åŸºäºTritonçš„å¤šçº¿ç¨‹å¹¶å‘æ¨ç†

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- **ç¡¬ä»¶**: NVIDIA A100 GPU (æˆ–å…¼å®¹å‹å·)
- **è½¯ä»¶ç¯å¢ƒ**: 
  - CUDA 11.8+
  - Python 3.8+
  - TensorRT 8.6+
  - Triton Inference Server 23.11+

## ğŸ› ï¸ å®‰è£…æŒ‡å—

### å¿«é€Ÿå¼€å§‹

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-username/gr-inference-opt.git
cd gr-inference-opt

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æ„å»ºTensorRTæ’ä»¶
cd kernels/trt_plugin_skeleton
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)
cd ../../..
```

### Dockerå®‰è£…

```bash
# æ„å»ºå¼€å‘é•œåƒ
docker build -t gr-inference-opt:dev --target development .

# æ„å»ºç”Ÿäº§é•œåƒ
docker build -t gr-inference-opt:prod --target production .

# ä½¿ç”¨GPUæ”¯æŒè¿è¡Œ
docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 gr-inference-opt:prod
```

## ğŸš€ ä½¿ç”¨æŒ‡å—

### 1. å¯¼å‡ºONNXæ¨¡å‹

```bash
# å¯¼å‡ºprefillå’Œdecodeæ¨¡å‹
python src/export_onnx.py \
    --vocab_size 10000 \
    --embedding_dim 128 \
    --num_features 32 \
    --prefill prefill.onnx \
    --decode decode.onnx
```

### 2. æ„å»ºTensorRTå¼•æ“

```bash
# ä½¿ç”¨trtexecæ„å»º
python src/build_engine.py \
    --onnx prefill.onnx \
    --engine prefill.engine \
    --mode trtexec \
    --fp16 \
    --workspace 8192

# ä½¿ç”¨TensorRT Python APIæ„å»º
python src/build_engine.py \
    --onnx prefill.onnx \
    --engine prefill.engine \
    --mode api \
    --precision fp16 \
    --validate
```

### 3. å¯åŠ¨TritonæœåŠ¡å™¨

```bash
# ä½¿ç”¨Docker
docker run --gpus all -v $(pwd)/triton_model_repo:/models \
    nvcr.io/nvidia/tritonserver:23.11 \
    tritonserver --model-repository=/models --strict-model-config=false

# ä½¿ç”¨æœ¬åœ°å®‰è£…
tritonserver --model-repository=./triton_model_repo --strict-model-config=false
```

### 4. è¿è¡Œæ€§èƒ½æµ‹è¯•

```bash
# è¿è¡ŒTritonæ€§èƒ½åˆ†æå™¨
bash bench/run_triton_perf.sh gr_pipeline localhost:8000

# è¿è¡Œäº¤äº’å†…æ ¸è‡ªåŠ¨è°ƒä¼˜
python kernels/triton_ops/autotune_interaction.py \
    --B 8 --F 16 --D 64 \
    --blocks 32,64,128,256 \
    --iters 100 \
    --out autotune_results.json
```

### 5. ä½¿ç”¨åµŒå…¥æœåŠ¡

```python
from src.embedding_service import EmbeddingService

# åˆ›å»ºåµŒå…¥æœåŠ¡
service = EmbeddingService(
    num_items=50000,
    emb_dim=128,
    gpu_cache_size=4096,
    host_cache_size=20000,
    enable_persistence=True
)

# æ‰¹é‡æŸ¥æ‰¾åµŒå…¥
embeddings = service.lookup_batch([1, 5, 10, 15, 20])

# è·å–ç¼“å­˜ç»Ÿè®¡
stats = service.get_cache_stats()
print(f"GPUå‘½ä¸­ç‡: {stats['gpu_hit_rate']:.2%}")
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
gr-inference-opt/
â”œâ”€â”€ src/                          # æ ¸å¿ƒæºä»£ç 
â”‚   â”œâ”€â”€ export_onnx.py           # ONNXæ¨¡å‹å¯¼å‡º
â”‚   â”œâ”€â”€ build_engine.py          # TensorRTå¼•æ“æ„å»º
â”‚   â””â”€â”€ embedding_service.py     # GPUçƒ­ç¼“å­˜æœåŠ¡
â”œâ”€â”€ kernels/                      # è‡ªå®šä¹‰å†…æ ¸
â”‚   â”œâ”€â”€ triton_ops/              # Triton DSLå†…æ ¸
â”‚   â”‚   â”œâ”€â”€ interaction_triton_fast.py
â”‚   â”‚   â”œâ”€â”€ interaction_wrapper.py
â”‚   â”‚   â””â”€â”€ autotune_interaction.py
â”‚   â”œâ”€â”€ cutlass_prototype/       # CUTLASSé›†æˆ
â”‚   â””â”€â”€ trt_plugin_skeleton/     # TensorRTæ’ä»¶
â”œâ”€â”€ triton_model_repo/           # Tritonæ¨¡å‹ä»“åº“
â”‚   â”œâ”€â”€ ensemble_model/          # é›†æˆæ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ gr_trt/                  # TensorRTæ¨¡å‹
â”‚   â””â”€â”€ interaction_python/      # Pythonåç«¯æ¨¡å‹
â”œâ”€â”€ bench/                       # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”œâ”€â”€ tests/                       # æµ‹è¯•å¥—ä»¶
â”œâ”€â”€ docs/                        # æ–‡æ¡£
â””â”€â”€ scripts/                     # å·¥å…·è„šæœ¬
```

## ğŸ”§ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

æ¡†æ¶æ”¯æŒå¤šç§æ¨¡å‹é…ç½®ï¼š

- **è¯æ±‡è¡¨å¤§å°**: 1K - 100K tokens
- **åµŒå…¥ç»´åº¦**: 32 - 512
- **ç‰¹å¾æ•°é‡**: 8 - 64
- **Transformerå±‚æ•°**: 1 - 12
- **åºåˆ—é•¿åº¦**: 8 - 512

### æ€§èƒ½è°ƒä¼˜

```bash
# ä¼˜åŒ–äº¤äº’å†…æ ¸å—å¤§å°
python kernels/triton_ops/autotune_interaction.py \
    --B 8 --F 16 --D 64 \
    --blocks 16,32,64,128,256 \
    --iters 1000

# åŸºå‡†æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
bash bench/run_triton_perf.sh gr_pipeline localhost:8000 \
    --concurrency-range 1:32:4 \
    --batch-size 1,4,8,16
```

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### åŸºå‡†æµ‹è¯•ç»“æœ

| ç»„ä»¶ | ååé‡ | å»¶è¿Ÿ | å†…å­˜ä½¿ç”¨ |
|------|--------|------|----------|
| äº¤äº’å†…æ ¸ | 1000 ops/sec | 1ms | 2GB |
| åµŒå…¥æœåŠ¡ | 5000 lookups/sec | 0.2ms | 4GB |
| å®Œæ•´æµæ°´çº¿ | 100 requests/sec | 10ms | 8GB |

### ä¼˜åŒ–æ•ˆæœ

- **æ¨ç†é€Ÿåº¦æå‡3å€** ç›¸æ¯”åŸºçº¿PyTorch
- **GPUå†…å­˜ä½¿ç”¨å‡å°‘50%**
- **åµŒå…¥æœåŠ¡ç¼“å­˜å‘½ä¸­ç‡90%+**

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»åˆ«
pytest tests/test_interaction.py -v
pytest tests/test_embedding_service.py -v

# è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
pytest tests/ --cov=src --cov=kernels --cov-report=html
```

## ğŸš€ éƒ¨ç½²

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```bash
# æ„å»ºç”Ÿäº§Dockeré•œåƒ
docker build -t gr-inference-opt:prod --target production .

# ä½¿ç”¨Kuberneteséƒ¨ç½²
kubectl apply -f k8s/deployment.yaml

# ä½¿ç”¨Prometheus/Grafanaç›‘æ§
kubectl apply -f k8s/monitoring.yaml
```

### æ‰©å±•ç­–ç•¥

- **æ°´å¹³æ‰©å±•**: å¤šä¸ªTritonå®ä¾‹é…åˆè´Ÿè½½å‡è¡¡å™¨
- **å‚ç›´æ‰©å±•**: å¤šGPUæ”¯æŒä¸æ¨¡å‹å¹¶è¡Œ
- **ç¼“å­˜æ‰©å±•**: åŸºäºRedisçš„åˆ†å¸ƒå¼åµŒå…¥ç¼“å­˜

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'æ·»åŠ æ–°åŠŸèƒ½'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»ºPull Request

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# è®¾ç½®pre-commité’©å­
pre-commit install

# è¿è¡Œä»£ç æ ¼å¼åŒ–
black src/ kernels/ tests/
isort src/ kernels/ tests/

# è¿è¡Œä»£ç æ£€æŸ¥
flake8 src/ kernels/ tests/
mypy src/
```

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- NVIDIA Triton Inference Serverå›¢é˜Ÿ
- CUTLASSåº“è´¡çŒ®è€…
- PyTorchå’ŒTensorRTç¤¾åŒº

## ğŸ“ æ”¯æŒ

- **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/your-username/gr-inference-opt/issues)
- **è®¨è®ºäº¤æµ**: [GitHub Discussions](https://github.com/your-username/gr-inference-opt/discussions)
- **æ–‡æ¡£**: [Wiki](https://github.com/your-username/gr-inference-opt/wiki)

## ğŸ”„ æ›´æ–°æ—¥å¿—

### v1.0.0 (2024-01-01)
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- å®Œæ•´æ¨ç†ä¼˜åŒ–æµæ°´çº¿
- Triton DSLäº¤äº’å†…æ ¸
- GPUçƒ­ç¼“å­˜å®ç°
- TensorRTæ’ä»¶æ¡†æ¶
-  comprehensiveæµ‹è¯•å¥—ä»¶
- CI/CDæµæ°´çº¿
- Dockeræ”¯æŒ
