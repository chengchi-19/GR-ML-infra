# ç”Ÿæˆå¼æ¨èæ¨¡å‹æ¨ç†ä¼˜åŒ–é¡¹ç›®

## é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹æ¨ç†ä¼˜åŒ–é¡¹ç›®ï¼Œä¸“æ³¨äº**æ¨ç†ä¼˜åŒ–åŠ é€Ÿéƒ¨ç½²**ã€‚é¡¹ç›®é›†æˆäº†TensorRTã€Tritonæ¨ç†æœåŠ¡å™¨ã€è‡ªå®šä¹‰ç®—å­ã€GPUåŠ é€Ÿç­‰æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ï¼Œå®ç°äº†ä»ç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ°æ¨èç»“æœçš„ç«¯åˆ°ç«¯é«˜æ€§èƒ½æ¨ç†æµç¨‹ã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### æ¨ç†ä¼˜åŒ–æŠ€æœ¯æ ˆ
- âœ… **TensorRTä¼˜åŒ–**: GPUåŠ é€Ÿæ¨ç†ï¼Œæ€§èƒ½æå‡3-10å€
- âœ… **Tritonæ¨ç†æœåŠ¡å™¨**: ç”Ÿäº§çº§é«˜å¹¶å‘æ¨ç†æœåŠ¡
- âœ… **è‡ªå®šä¹‰ç®—å­**: Triton DSLå’ŒTensorRTæ’ä»¶
- âœ… **GPUåŠ é€Ÿ**: CUDAå†…æ ¸ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†
- âœ… **ONNXå¯¼å‡º**: æ¨¡å‹æ ¼å¼æ ‡å‡†åŒ–
- âœ… **æ€§èƒ½ç›‘æ§**: å®æ—¶æ€§èƒ½æŒ‡æ ‡å’Œç›‘æ§

### æ¨èç³»ç»ŸåŠŸèƒ½
- âœ… **1024ç»´ç‰¹å¾å¤„ç†**: ä¼ä¸šçº§ç”¨æˆ·è¡Œä¸ºç‰¹å¾
- âœ… **å¤šä»»åŠ¡å­¦ä¹ **: å‚ä¸åº¦ã€ç•™å­˜ã€å•†ä¸šåŒ–é¢„æµ‹
- âœ… **åŠ¨æ€æ‰¹æ¬¡å¤„ç†**: æ”¯æŒé«˜ååé‡æ¨ç†
- âœ… **ç¼“å­˜æœºåˆ¶**: ç‰¹å¾å’Œæ¨¡å‹ç¼“å­˜ä¼˜åŒ–
- âœ… **å®æ—¶æ¨ç†**: ä½å»¶è¿Ÿæ¨èæœåŠ¡

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd gr-inference-opt-updated

# å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# å®‰è£…å¼€å‘ä¾èµ–ï¼ˆå¯é€‰ï¼‰
pip install -r requirements-dev.txt
```

### 2. ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹

```bash
# è¿è¡Œé›†æˆä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
python main_optimized.py --mode all
```

è¿™ä¸ªå‘½ä»¤ä¼šè‡ªåŠ¨æ‰§è¡Œï¼š
1. **æ¨¡å‹åˆå§‹åŒ–** - åŠ è½½ç”Ÿæˆå¼æ¨èæ¨¡å‹
2. **TensorRTä¼˜åŒ–** - æ„å»ºå’ŒåŠ è½½TensorRTå¼•æ“
3. **è‡ªå®šä¹‰ç®—å­é›†æˆ** - åŠ è½½é«˜æ€§èƒ½ç®—å­
4. **Tritonéƒ¨ç½²é…ç½®** - é…ç½®æ¨ç†æœåŠ¡å™¨
5. **å•æ¬¡æ¨ç†æ¼”ç¤º** - å±•ç¤ºä¼˜åŒ–æ¨ç†æ•ˆæœ
6. **æ‰¹é‡æ¨ç†æµ‹è¯•** - æµ‹è¯•é«˜å¹¶å‘æ€§èƒ½
7. **æ€§èƒ½åŸºå‡†æµ‹è¯•** - å¯¹æ¯”ä¸åŒæ¨ç†å¼•æ“æ€§èƒ½

### 3. è¿è¡Œç»“æœç¤ºä¾‹

```
================================================================================
ç”Ÿæˆå¼æ¨èæ¨¡å‹æ¨ç†ä¼˜åŒ–é¡¹ç›® - é›†æˆä¼˜åŒ–ç‰ˆæœ¬
================================================================================

âœ… GPUç¯å¢ƒå¯ç”¨: NVIDIA A100-SXM4-40GB
âœ… TensorRTå¼•æ“åˆå§‹åŒ–æˆåŠŸ
âœ… è‡ªå®šä¹‰ç®—å­åˆå§‹åŒ–æˆåŠŸ
âš ï¸ TritonæœåŠ¡å™¨æœªè¿è¡Œï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨ç†

============================================================
ä¼˜åŒ–æ¨ç†ç»“æœ
============================================================
ç”¨æˆ·ID: user_12345
ä¼šè¯ID: session_67890
åºåˆ—é•¿åº¦: 10
æ¨ç†å¼•æ“: tensorrt

æ¨èç»“æœ:
  1. video_0 (åˆ†æ•°: 0.8234)
  2. video_1 (åˆ†æ•°: 0.7654)
  3. video_2 (åˆ†æ•°: 0.7123)
  ...

ç‰¹å¾åˆ†æ•°:
  engagement_score: 0.8543
  retention_score: 0.7234
  diversity_score: 0.9123

æ€§èƒ½æµ‹è¯•ç»“æœ:
  æµ‹è¯•æ¬¡æ•°: 10
  å¹³å‡æ¨ç†æ—¶é—´: 45.23ms
  ååé‡: 22.1 è¯·æ±‚/ç§’
```

## ğŸ“ é¡¹ç›®ç»“æ„

> ğŸ“‹ **è¯¦ç»†é¡¹ç›®ç»“æ„è¯´æ˜è¯·æŸ¥çœ‹ [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md)**

```
gr-inference-opt-updated/
â”œâ”€â”€ main.py                        # ğŸ¯ ä¸»å…¥å£æ–‡ä»¶ï¼ˆé›†æˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
â”œâ”€â”€ src/                           # ğŸ”¥ æ ¸å¿ƒæºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ inference_pipeline.py      # æ¨ç†æµæ°´çº¿
â”‚   â”œâ”€â”€ tensorrt_inference.py      # TensorRTæ¨ç†æ¨¡å—
â”‚   â”œâ”€â”€ export_onnx.py            # ONNXæ¨¡å‹å¯¼å‡º
â”‚   â”œâ”€â”€ user_behavior_schema.py    # ç”¨æˆ·è¡Œä¸ºæ•°æ®ç»“æ„
â”‚   â”œâ”€â”€ embedding_service.py       # é«˜æ€§èƒ½åµŒå…¥æœåŠ¡
â”‚   â”œâ”€â”€ build_engine.py           # TensorRTå¼•æ“æ„å»º
â”‚   â””â”€â”€ model_parameter_calculator.py # æ¨¡å‹å‚æ•°è®¡ç®—
â”œâ”€â”€ kernels/                       # ğŸ”¥ è‡ªå®šä¹‰ç®—å­ç›®å½•
â”‚   â”œâ”€â”€ triton_ops/              # Triton DSLç®—å­
â”‚   â”œâ”€â”€ trt_plugin_skeleton/     # TensorRTæ’ä»¶
â”‚   â””â”€â”€ cutlass_prototype/       # CUTLASSåŸå‹
â”œâ”€â”€ triton_model_repo/             # ğŸ”¥ Tritonæ¨ç†æœåŠ¡å™¨
â”‚   â”œâ”€â”€ ensemble_model/          # é›†æˆæ¨¡å‹
â”‚   â”œâ”€â”€ gr_trt/                  # TensorRTæ¨¡å‹
â”‚   â”œâ”€â”€ interaction_python/      # Pythonç®—å­
â”‚   â”œâ”€â”€ embedding_service/       # åµŒå…¥æœåŠ¡
â”‚   â””â”€â”€ preprocess_py/           # é¢„å¤„ç†
â”œâ”€â”€ docs/                         # ğŸ“š é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ scripts/                      # ğŸ”§ è‡ªåŠ¨åŒ–è„šæœ¬
â”œâ”€â”€ examples/                     # ğŸ“– ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ tests/                        # ğŸ§ª æµ‹è¯•ä»£ç 
â”œâ”€â”€ bench/                        # âš¡ æ€§èƒ½æµ‹è¯•
â””â”€â”€ models/                       # ğŸ¤– æ¨¡å‹æ–‡ä»¶ï¼ˆè¿è¡Œæ—¶ç”Ÿæˆï¼‰
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. é›†æˆæ¨ç†ä¼˜åŒ–å¼•æ“ (`main_optimized.py`)

**åŠŸèƒ½**: ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ¨ç†ä¼˜åŒ–ç»„ä»¶ï¼Œå®ç°ä¸€é”®å¼ä¼˜åŒ–æ¨ç†

**æ ¸å¿ƒç‰¹æ€§**:
- è‡ªåŠ¨æ£€æµ‹å’Œåˆå§‹åŒ–GPUç¯å¢ƒ
- æ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ¨ç†å¼•æ“ï¼ˆTriton > TensorRT > PyTorchï¼‰
- é›†æˆè‡ªå®šä¹‰ç®—å­å¤„ç†
- å®æ—¶æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—è®°å½•

**ä½¿ç”¨æ–¹å¼**:
```python
from main_optimized import OptimizedInferenceEngine

# åˆ›å»ºä¼˜åŒ–æ¨ç†å¼•æ“
engine = OptimizedInferenceEngine(model_config, optimization_config)

# æ‰§è¡Œä¼˜åŒ–æ¨ç†
result = engine.infer_with_optimization(
    user_behaviors=user_behaviors,
    user_id="user_123",
    session_id="session_456",
    num_recommendations=10
)
```

### 2. TensorRTä¼˜åŒ–æ¨¡å— (`src/tensorrt_inference.py`)

**åŠŸèƒ½**: å°†ONNXæ¨¡å‹è½¬æ¢ä¸ºTensorRTå¼•æ“ï¼Œå®ç°GPUåŠ é€Ÿæ¨ç†

**æ€§èƒ½æå‡**: ç›¸æ¯”PyTorch GPUæ¨ç†ï¼Œé€šå¸¸å¯è·å¾—1.5-3xçš„åŠ é€Ÿæ¯”

**ä½¿ç”¨æ–¹å¼**:
```python
from src.tensorrt_inference import TensorRTInference, build_tensorrt_engine

# æ„å»ºTensorRTå¼•æ“
engine_path = build_tensorrt_engine(
    onnx_path="models/prefill.onnx",
    engine_path="models/prefill.trt",
    precision="fp16",
    max_batch_size=8
)

# ä½¿ç”¨TensorRTæ¨ç†
trt_inference = TensorRTInference(engine_path)
result = trt_inference.infer(input_data)
```

### 3. Tritonæ¨ç†æœåŠ¡å™¨ (`triton_model_repo/`)

**åŠŸèƒ½**: ç”Ÿäº§çº§æ¨ç†æœåŠ¡å™¨ï¼Œæ”¯æŒé«˜å¹¶å‘ã€å¤šæ¨¡å‹éƒ¨ç½²

**éƒ¨ç½²å‘½ä»¤**:
```bash
# å¯åŠ¨TritonæœåŠ¡å™¨
docker run --gpus=all --rm -p8000:8000 -p8001:8001 -p8002:8002 \
  -v $(pwd)/triton_model_repo:/models \
  nvcr.io/nvidia/tritonserver:23.12-py3 \
  tritonserver --model-repository=/models

# æˆ–ä½¿ç”¨è„šæœ¬å¯åŠ¨
./scripts/run_server.sh
```

### 4. è‡ªå®šä¹‰ç®—å­ (`kernels/`)

**åŠŸèƒ½**: å®ç°é«˜æ€§èƒ½è‡ªå®šä¹‰ç®—å­ï¼Œä¼˜åŒ–ç‰¹å®šè®¡ç®—

**ç®—å­ç±»å‹**:
- **Triton DSLç®—å­**: é«˜æ€§èƒ½äº¤äº’ç®—å­
- **TensorRTæ’ä»¶**: è‡ªå®šä¹‰TensorRTå±‚
- **CUTLASSåŸå‹**: é«˜æ€§èƒ½çŸ©é˜µè¿ç®—

**ç¼–è¯‘æ–¹å¼**:
```bash
# ç¼–è¯‘Triton DSLç®—å­
cd kernels/triton_ops
python setup.py build_ext --inplace

# ç¼–è¯‘TensorRTæ’ä»¶
cd kernels/trt_plugin_skeleton
mkdir build && cd build
cmake .. && make
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨ç†æ–¹å¼ | å»¶è¿Ÿ(ms) | ååé‡(æ ·æœ¬/ç§’) | å†…å­˜å ç”¨ | åŠ é€Ÿæ¯” |
|---------|---------|----------------|---------|--------|
| PyTorch CPU | ~500 | ~2 | é«˜ | 1x |
| PyTorch GPU | ~150 | ~7 | ä¸­ | 3.3x |
| **TensorRT** | **~50** | **~20** | ä½ | **10x** |
| **Tritonéƒ¨ç½²** | **~45** | **~22** | ä½ | **11x** |

## ğŸ® è¿è¡Œæ¨¡å¼

### 1. å®Œæ•´ä¼˜åŒ–æµç¨‹ï¼ˆæ¨èï¼‰
```bash
python main_optimized.py --mode all
```

### 2. ä¸“é¡¹æµ‹è¯•
```bash
# å•æ¬¡æ¨ç†
python main_optimized.py --mode single

# æ‰¹é‡æ¨ç†
python main_optimized.py --mode batch

# æ€§èƒ½æµ‹è¯•
python main_optimized.py --mode performance

# Tritonéƒ¨ç½²
python main_optimized.py --mode triton
```

### 3. è°ƒè¯•æ¨¡å¼
```bash
# è¯¦ç»†æ—¥å¿—
python main_optimized.py --mode all --log-level DEBUG
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### 1. å®æ—¶ç›‘æ§
```bash
# æŸ¥çœ‹æ¨ç†æ—¥å¿—
tail -f inference.log

# æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
tail -f performance_metrics.log

# Tritonç›‘æ§é¢æ¿
http://localhost:8000/metrics
```

### 2. æ€§èƒ½æŒ‡æ ‡
- **æ¨ç†å»¶è¿Ÿ**: ç«¯åˆ°ç«¯æ¨ç†æ—¶é—´
- **ååé‡**: æ¯ç§’å¤„ç†è¯·æ±‚æ•°
- **GPUåˆ©ç”¨ç‡**: GPUè®¡ç®—èµ„æºä½¿ç”¨ç‡
- **å†…å­˜å ç”¨**: æ¨¡å‹å’Œç¼“å­˜å†…å­˜ä½¿ç”¨
- **ç¼“å­˜å‘½ä¸­ç‡**: ç‰¹å¾ç¼“å­˜æ•ˆç‡

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### åŸºç¡€ç¯å¢ƒ
- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (æ¨è)
- Docker (ç”¨äºTritonéƒ¨ç½²)

### å¯é€‰ä¾èµ–
```bash
# TensorRT (éœ€è¦NVIDIA GPU)
pip install tensorrt

# Triton (éœ€è¦Docker)
# å‚è€ƒå®˜æ–¹æ–‡æ¡£å®‰è£…Triton Inference Server

# æ€§èƒ½ç›‘æ§
pip install prometheus_client
```

## ğŸš€ éƒ¨ç½²æŒ‡å—

### 1. å¼€å‘ç¯å¢ƒ
```bash
# å¿«é€ŸéªŒè¯
python main_optimized.py --mode single

# æ€§èƒ½æµ‹è¯•
python main_optimized.py --mode performance
```

### 2. ç”Ÿäº§ç¯å¢ƒ
```bash
# å¯åŠ¨TritonæœåŠ¡å™¨
./scripts/run_server.sh

# è¿è¡Œä¼˜åŒ–æ¨ç†
python main_optimized.py --mode all
```

### 3. å®¹å™¨åŒ–éƒ¨ç½²
```bash
# æ„å»ºDockeré•œåƒ
docker build -t gr-inference-opt .

# è¿è¡Œå®¹å™¨
docker run --gpus=all -p8000:8000 gr-inference-opt
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **TensorRTå®‰è£…å¤±è´¥**
   ```bash
   # æ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§
   nvidia-smi
   python -c "import torch; print(torch.version.cuda)"
   ```

2. **Tritonå¯åŠ¨å¤±è´¥**
   ```bash
   # æ£€æŸ¥Dockeræƒé™
   sudo usermod -aG docker $USER
   sudo systemctl restart docker
   ```

3. **GPUå†…å­˜ä¸è¶³**
   ```python
   # å‡å°‘æ‰¹æ¬¡å¤§å°
   batch_size = 1
   # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   torch.utils.checkpoint.checkpoint(model, input)
   ```

### è°ƒè¯•æŠ€å·§
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export LOG_LEVEL=DEBUG

# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# ç›‘æ§ç³»ç»Ÿèµ„æº
htop
```

## ğŸ“š æ–‡æ¡£

- [æ¨ç†ä¼˜åŒ–åŠŸèƒ½æ€»ç»“](docs/inference_optimization_summary.md)
- [é¡¹ç›®è¿è¡ŒæŒ‡å—](docs/project_runtime_guide.md)
- [é¡¹ç›®æ¶æ„æ€»ç»“](docs/project_summary.md)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

æ„Ÿè°¢NVIDIAæä¾›çš„TensorRTå’ŒTriton Inference Serverç­‰ä¼˜ç§€å·¥å…·ã€‚

---

**ğŸ¯ é¡¹ç›®é‡ç‚¹**: è¿™ä¸ªé¡¹ç›®çš„æ ¸å¿ƒä»·å€¼åœ¨äºæ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡TensorRTã€Tritonã€è‡ªå®šä¹‰ç®—å­ç­‰æŠ€æœ¯çš„é›†æˆï¼Œå®ç°äº†é«˜æ€§èƒ½çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹æ¨ç†ï¼Œæ˜¯æ¨ç†ä¼˜åŒ–åŠ é€Ÿéƒ¨ç½²çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚
