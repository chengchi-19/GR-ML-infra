#!/usr/bin/env python3
"""
Tritonç®—å­é›†æˆæµ‹è¯•

æµ‹è¯•æ‰€æœ‰Tritonè‡ªå®šä¹‰ç®—å­æ˜¯å¦æ­£ç¡®é›†æˆåˆ°æ¨ç†æ¡†æ¶ä¸­
"""

import sys
import os
import torch
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_triton_operator_availability():
    """æµ‹è¯•Tritonç®—å­å¯ç”¨æ€§"""
    print("ğŸ”§ æµ‹è¯•Tritonç®—å­å¯ç”¨æ€§...")
    
    try:
        from optimizations.triton_ops.trriton_operator_manager import create_triton_operator_manager
        from main import create_optimized_config
        
        # åˆ›å»ºé…ç½®
        config = create_optimized_config()
        
        # åˆ›å»ºTritonç®—å­ç®¡ç†å™¨
        manager = create_triton_operator_manager(config)
        
        # è·å–å¯ç”¨æ€§
        availability = manager.get_operator_availability()
        
        print("Tritonç®—å­å¯ç”¨æ€§:")
        for name, available in availability.items():
            status = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
            print(f"  {name}: {status}")
        
        available_count = sum(availability.values())
        total_count = len(availability)
        
        print(f"\nğŸ“Š æ€»ä½“å¯ç”¨æ€§: {available_count}/{total_count} ({available_count/total_count:.1%})")
        
        return availability
        
    except Exception as e:
        print(f"âŒ Tritonç®—å­æµ‹è¯•å¤±è´¥: {e}")
        return {}

def test_triton_operator_integration():
    """æµ‹è¯•Tritonç®—å­åœ¨æ¡†æ¶ä¸­çš„é›†æˆ"""
    print("\nğŸ”„ æµ‹è¯•Tritonç®—å­åœ¨æ¡†æ¶ä¸­çš„é›†æˆ...")
    
    try:
        from integrations.framework_controller import create_integrated_controller
        from main import create_optimized_config
        
        # åˆ›å»ºé›†æˆæ§åˆ¶å™¨
        config = create_optimized_config()
        controller = create_integrated_controller(config)
        
        # æ£€æŸ¥Tritoné›†æˆçŠ¶æ€
        framework_availability = controller.framework_availability
        
        print("æ¡†æ¶é›†æˆçŠ¶æ€:")
        print(f"  Tritonç®—å­: {'âœ… å·²é›†æˆ' if framework_availability.get('triton_ops', False) else 'âŒ æœªé›†æˆ'}")
        
        # è·å–è¯¦ç»†ç»Ÿè®¡
        if hasattr(controller, 'triton_manager') and controller.triton_manager:
            stats = controller.triton_manager.get_operator_stats()
            print(f"  Tritonç®—å­ç»Ÿè®¡: {stats}")
        
        return framework_availability
        
    except Exception as e:
        print(f"âŒ Tritonç®—å­é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return {}

def test_triton_operator_functionality():
    """æµ‹è¯•Tritonç®—å­åŠŸèƒ½"""
    print("\nâš¡ æµ‹è¯•Tritonç®—å­åŠŸèƒ½...")
    
    try:
        from optimizations.triton_ops.trriton_operator_manager import create_triton_operator_manager
        from main import create_optimized_config
        
        # åˆ›å»ºé…ç½®å’Œç®¡ç†å™¨
        config = create_optimized_config()
        manager = create_triton_operator_manager(config)
        
        # æµ‹è¯•æ•°æ®
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        test_results = {}
        
        # æµ‹è¯•èåˆæ³¨æ„åŠ›+LayerNormç®—å­
        if manager.availability.get('fused_attention_layernorm', False):
            print("  æµ‹è¯•èåˆæ³¨æ„åŠ›+LayerNormç®—å­...")
            hidden_states = torch.randn(2, 32, 1024, device=device)
            output = manager.apply_fused_attention_layernorm(hidden_states)
            test_results['fused_attention_layernorm'] = {
                'input_shape': list(hidden_states.shape),
                'output_shape': list(output.shape),
                'success': True
            }
            print(f"    âœ… è¾“å…¥: {hidden_states.shape} -> è¾“å‡º: {output.shape}")
        
        # æµ‹è¯•åˆ†å±‚åºåˆ—èåˆç®—å­
        if manager.availability.get('hierarchical_sequence_fusion', False):
            print("  æµ‹è¯•åˆ†å±‚åºåˆ—èåˆç®—å­...")
            sequence_features = torch.randn(2, 64, 1024, device=device)
            output = manager.apply_hierarchical_sequence_fusion(sequence_features)
            test_results['hierarchical_sequence_fusion'] = {
                'input_shape': list(sequence_features.shape),
                'output_shape': list(output.shape),
                'success': True
            }
            print(f"    âœ… è¾“å…¥: {sequence_features.shape} -> è¾“å‡º: {output.shape}")
        
        # æµ‹è¯•äº¤äº’ç®—å­
        if manager.availability.get('interaction_operator', False):
            print("  æµ‹è¯•äº¤äº’ç®—å­...")
            features = torch.randn(100, 1024, device=device)
            output = manager.apply_interaction_operator(features)
            test_results['interaction_operator'] = {
                'input_shape': list(features.shape),
                'output_shape': list(output.shape),
                'success': True
            }
            print(f"    âœ… è¾“å…¥: {features.shape} -> è¾“å‡º: {output.shape}")
        
        # æµ‹è¯•åºåˆ—æ¨èäº¤äº’ç®—å­
        if manager.availability.get('sequence_recommendation_interaction', False):
            print("  æµ‹è¯•åºåˆ—æ¨èäº¤äº’ç®—å­...")
            user_sequences = [[
                {'video_id': 'video_1', 'watch_duration': 120, 'is_liked': True},
                {'video_id': 'video_2', 'watch_duration': 90, 'is_liked': False},
            ]]
            output = manager.apply_sequence_recommendation_interaction(user_sequences)
            test_results['sequence_recommendation_interaction'] = {
                'input_sequences': len(user_sequences),
                'output_shape': list(output.shape),
                'success': True
            }
            print(f"    âœ… è¾“å…¥åºåˆ—: {len(user_sequences)} -> è¾“å‡º: {output.shape}")
        
        # æµ‹è¯•HSTUåˆ†å±‚æ³¨æ„åŠ›ç®—å­
        if manager.availability.get('hstu_hierarchical_attention', False):
            print("  æµ‹è¯•HSTUåˆ†å±‚æ³¨æ„åŠ›ç®—å­...")
            query = torch.randn(2, 32, 1024, device=device)
            key = torch.randn(2, 32, 1024, device=device)
            value = torch.randn(2, 32, 1024, device=device)
            output = manager.apply_hstu_hierarchical_attention(query, key, value)
            test_results['hstu_hierarchical_attention'] = {
                'query_shape': list(query.shape),
                'output_shape': list(output.shape),
                'success': True
            }
            print(f"    âœ… æŸ¥è¯¢: {query.shape} -> è¾“å‡º: {output.shape}")
        
        return test_results
        
    except Exception as e:
        print(f"âŒ Tritonç®—å­åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {}

def test_end_to_end_integration():
    """æµ‹è¯•ç«¯åˆ°ç«¯é›†æˆ"""
    print("\nğŸ¯ æµ‹è¯•ç«¯åˆ°ç«¯é›†æˆ...")
    
    try:
        from integrations.framework_controller import create_integrated_controller
        from main import create_optimized_config
        
        # åˆ›å»ºé›†æˆæ§åˆ¶å™¨
        config = create_optimized_config()
        controller = create_integrated_controller(config)
        
        # æµ‹è¯•æ•°æ®
        test_behaviors = [
            {'video_id': 'video_1', 'watch_duration': 120, 'is_liked': True, 'category': 'tech'},
            {'video_id': 'video_2', 'watch_duration': 90, 'is_liked': False, 'category': 'music'},
            {'video_id': 'video_3', 'watch_duration': 200, 'is_liked': True, 'category': 'tech'},
            {'video_id': 'video_4', 'watch_duration': 45, 'is_liked': True, 'category': 'sports'},
            {'video_id': 'video_5', 'watch_duration': 180, 'is_liked': False, 'category': 'education'},
        ]
        
        # æ‰§è¡Œæ¨ç†
        result = controller.infer_with_optimal_strategy(
            user_id="test_user",
            session_id="test_session",
            user_behaviors=test_behaviors,
            num_recommendations=5
        )
        
        # æ£€æŸ¥Tritonä¼˜åŒ–æ˜¯å¦è¢«åº”ç”¨
        optimizations = result.get('optimizations_applied', [])
        triton_optimized = result.get('triton_optimized', False)
        
        print(f"  æ¨ç†ç­–ç•¥: {result.get('inference_strategy', 'unknown')}")
        print(f"  Tritonä¼˜åŒ–: {'âœ… å·²åº”ç”¨' if triton_optimized else 'âŒ æœªåº”ç”¨'}")
        print(f"  ä¼˜åŒ–ç®—å­: {optimizations}")
        print(f"  æ¨èæ•°é‡: {len(result.get('recommendations', []))}")
        
        return {
            'triton_optimized': triton_optimized,
            'optimizations_applied': optimizations,
            'recommendations_count': len(result.get('recommendations', [])),
            'inference_time_ms': result.get('inference_time_ms', 0),
            'success': True
        }
        
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def test_triton_benchmark():
    """æµ‹è¯•Tritonç®—å­æ€§èƒ½"""
    print("\nğŸ“Š æµ‹è¯•Tritonç®—å­æ€§èƒ½...")
    
    try:
        from optimizations.triton_ops.trriton_operator_manager import create_triton_operator_manager
        from main import create_optimized_config
        
        # åˆ›å»ºé…ç½®å’Œç®¡ç†å™¨
        config = create_optimized_config()
        manager = create_triton_operator_manager(config)
        
        # æµ‹è¯•æ•°æ®
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        test_data = {
            'hidden_states': torch.randn(4, 64, 1024, device=device),
            'sequence_features': torch.randn(4, 128, 1024, device=device),
            'features': torch.randn(200, 1024, device=device),
            'query': torch.randn(4, 64, 1024, device=device),
            'key': torch.randn(4, 64, 1024, device=device),
            'value': torch.randn(4, 64, 1024, device=device),
            'user_sequences': [[
                {'video_id': f'video_{i}', 'watch_duration': 100 + i*10, 'is_liked': i % 2 == 0}
                for i in range(10)
            ] for _ in range(4)]
        }
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_results = manager.benchmark_all_operators(
            test_data=test_data,
            num_iterations=5
        )
        
        print("Tritonç®—å­æ€§èƒ½åŸºå‡†:")
        for name, metrics in benchmark_results['benchmark_results'].items():
            print(f"  {name}: {metrics['avg_latency_ms']:.2f}ms")
        
        return benchmark_results
        
    except Exception as e:
        print(f"âŒ Tritonç®—å­æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {}

def run_all_triton_tests():
    """è¿è¡Œæ‰€æœ‰Tritoné›†æˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹Tritonç®—å­é›†æˆæµ‹è¯•")
    print("="*80)
    
    test_results = {}
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_results['operator_availability'] = test_triton_operator_availability()
    test_results['operator_integration'] = test_triton_operator_integration()
    test_results['operator_functionality'] = test_triton_operator_functionality()
    test_results['end_to_end_integration'] = test_end_to_end_integration()
    test_results['triton_benchmark'] = test_triton_benchmark()
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "="*80)
    print("Tritonç®—å­é›†æˆæµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*80)
    
    for test_name, result in test_results.items():
        if isinstance(result, dict):
            if 'success' in result:
                status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
            else:
                status = "âœ… å®Œæˆ"
        else:
            status = "âœ… å®Œæˆ"
        
        print(f"  {test_name}: {status}")
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'test_results': test_results,
        'summary': {
            'triton_operators_tested': len(test_results),
            'integration_status': 'completed',
        }
    }
    
    with open('triton_integration_test_report.json', 'w', encoding='utf-8') as f:
        import json
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ Tritoné›†æˆæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: triton_integration_test_report.json")
    
    return test_results

if __name__ == "__main__":
    run_all_triton_tests()