#!/usr/bin/env python3
"""
TensorRTè‡ªå®šä¹‰æ’ä»¶æµ‹è¯•éªŒè¯è„šæœ¬

æµ‹è¯•TensorRTè‡ªå®šä¹‰æ’ä»¶çš„åŠŸèƒ½å’Œæ€§èƒ½
"""

import os
import sys
import time
import logging
import numpy as np
from typing import Dict, List, Any, Optional
import torch

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from integrations.hstu.hstu_model import HSTUGenerativeRecommender, HSTUModelConfig
    from integrations.hstu.enhanced_onnx_exporter import export_hstu_model_with_custom_ops
    from integrations.tensorrt.tensorrt_engine import create_tensorrt_engine, TensorRTConfig
    from integrations.tensorrt.plugins.python.tensorrt_plugins import (
        initialize_plugins, is_plugins_available, get_num_registered_plugins
    )
    IMPORTS_AVAILABLE = True
except ImportError as e:
    logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    IMPORTS_AVAILABLE = False


class TensorRTPluginTester:
    """TensorRTæ’ä»¶æµ‹è¯•å™¨"""

    def __init__(self, test_dir: str = "./tensorrt_plugin_tests"):
        self.test_dir = test_dir
        os.makedirs(test_dir, exist_ok=True)

        self.model = None
        self.config = None
        self.tensorrt_engine = None

        logger.info(f"TensorRTæ’ä»¶æµ‹è¯•å™¨åˆå§‹åŒ–ï¼Œæµ‹è¯•ç›®å½•: {test_dir}")

    def test_plugin_loading(self) -> bool:
        """æµ‹è¯•æ’ä»¶åŠ è½½"""
        logger.info("ğŸ§ª æµ‹è¯•æ’ä»¶åŠ è½½...")

        try:
            # æµ‹è¯•æ’ä»¶åˆå§‹åŒ–
            success = initialize_plugins()
            if not success:
                logger.error("âŒ æ’ä»¶åˆå§‹åŒ–å¤±è´¥")
                return False

            # æ£€æŸ¥æ’ä»¶å¯ç”¨æ€§
            available = is_plugins_available()
            if not available:
                logger.error("âŒ æ’ä»¶ä¸å¯ç”¨")
                return False

            # è·å–æ’ä»¶æ•°é‡
            num_plugins = get_num_registered_plugins()
            logger.info(f"âœ… æ’ä»¶åŠ è½½æˆåŠŸï¼Œæ³¨å†Œäº† {num_plugins} ä¸ªæ’ä»¶")

            return True

        except Exception as e:
            logger.error(f"âŒ æ’ä»¶åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
            return False

    def create_test_model(self) -> bool:
        """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
        logger.info("ğŸ§ª åˆ›å»ºæµ‹è¯•æ¨¡å‹...")

        try:
            # åˆ›å»ºå°å‹æµ‹è¯•é…ç½®
            self.config = HSTUModelConfig(
                vocab_size=5000,
                d_model=256,
                num_layers=2,
                num_heads=4,
                max_seq_len=128,
                dropout=0.1
            )

            # åˆ›å»ºæ¨¡å‹
            self.model = HSTUGenerativeRecommender(self.config)
            self.model.eval()

            logger.info("âœ… æµ‹è¯•æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return False

    def test_onnx_export_with_custom_ops(self) -> bool:
        """æµ‹è¯•åŒ…å«è‡ªå®šä¹‰ç®—å­çš„ONNXå¯¼å‡º"""
        logger.info("ğŸ§ª æµ‹è¯•ONNXå¯¼å‡ºä¸è‡ªå®šä¹‰ç®—å­...")

        if self.model is None or self.config is None:
            logger.error("æ¨¡å‹æœªåˆ›å»º")
            return False

        try:
            # å¯¼å‡ºåŒ…å«è‡ªå®šä¹‰ç®—å­çš„ONNXæ¨¡å‹
            export_result = export_hstu_model_with_custom_ops(
                model=self.model,
                model_config=self.config,
                export_dir=os.path.join(self.test_dir, "onnx_models"),
                batch_sizes=[1, 2],
                sequence_lengths=[32, 64],
                enable_custom_ops=True,
                optimize=True
            )

            if export_result["success"]:
                logger.info("âœ… ONNXå¯¼å‡ºæˆåŠŸ")
                logger.info(f"å¯¼å‡ºæ–‡ä»¶: {export_result['export_paths']}")

                # æ£€æŸ¥æ˜¯å¦åŒ…å«è‡ªå®šä¹‰ç®—å­
                if "custom_ops_onnx" in export_result["export_paths"]:
                    logger.info("âœ… è‡ªå®šä¹‰ç®—å­ONNXæ¨¡å‹ç”ŸæˆæˆåŠŸ")
                    return True
                else:
                    logger.warning("âš ï¸ æœªç”Ÿæˆè‡ªå®šä¹‰ç®—å­ONNXæ¨¡å‹")
                    return True  # æ ‡å‡†æ¨¡å‹å¯¼å‡ºæˆåŠŸä¹Ÿç®—é€šè¿‡

            else:
                logger.error(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {export_result.get('error')}")
                return False

        except Exception as e:
            logger.error(f"âŒ ONNXå¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_tensorrt_engine_with_plugins(self) -> bool:
        """æµ‹è¯•åŒ…å«æ’ä»¶çš„TensorRTå¼•æ“"""
        logger.info("ğŸ§ª æµ‹è¯•TensorRTå¼•æ“ä¸è‡ªå®šä¹‰æ’ä»¶...")

        try:
            # æŸ¥æ‰¾ONNXæ¨¡å‹
            onnx_dir = os.path.join(self.test_dir, "onnx_models")
            onnx_files = [f for f in os.listdir(onnx_dir) if f.endswith('.onnx')] if os.path.exists(onnx_dir) else []

            if not onnx_files:
                logger.error("æœªæ‰¾åˆ°ONNXæ¨¡å‹æ–‡ä»¶")
                return False

            # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰ç®—å­æ¨¡å‹
            onnx_path = None
            for f in onnx_files:
                if "custom_ops" in f:
                    onnx_path = os.path.join(onnx_dir, f)
                    break

            if onnx_path is None:
                onnx_path = os.path.join(onnx_dir, onnx_files[0])

            logger.info(f"ä½¿ç”¨ONNXæ¨¡å‹: {onnx_path}")

            # åˆ›å»ºTensorRTé…ç½®
            tensorrt_config = TensorRTConfig(
                model_name="hstu_plugin_test",
                onnx_path=onnx_path,
                engine_path=os.path.join(self.test_dir, "engines", "hstu_test.trt"),
                precision="fp16",
                max_batch_size=2,
                enable_custom_plugins=True,
                enable_dynamic_shapes=True
            )

            # åˆ›å»ºTensorRTå¼•æ“
            self.tensorrt_engine = create_tensorrt_engine(
                onnx_path=tensorrt_config.onnx_path,
                engine_path=tensorrt_config.engine_path,
                precision=tensorrt_config.precision,
                max_batch_size=tensorrt_config.max_batch_size,
                enable_custom_plugins=tensorrt_config.enable_custom_plugins
            )

            if self.tensorrt_engine.tensorrt_available:
                logger.info("âœ… TensorRTå¼•æ“åˆ›å»ºæˆåŠŸ")

                # è·å–å¼•æ“ä¿¡æ¯
                engine_info = self.tensorrt_engine.get_engine_info()
                logger.info(f"å¼•æ“ä¿¡æ¯: {engine_info}")

                return True
            else:
                logger.error("âŒ TensorRTå¼•æ“åˆ›å»ºå¤±è´¥")
                return False

        except Exception as e:
            logger.error(f"âŒ TensorRTå¼•æ“æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_inference_performance(self) -> bool:
        """æµ‹è¯•æ¨ç†æ€§èƒ½"""
        logger.info("ğŸ§ª æµ‹è¯•æ¨ç†æ€§èƒ½...")

        if self.tensorrt_engine is None or not self.tensorrt_engine.tensorrt_available:
            logger.error("TensorRTå¼•æ“ä¸å¯ç”¨")
            return False

        try:
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            batch_size = 2
            seq_len = 64

            test_inputs = {
                'input_ids': torch.randint(0, self.config.vocab_size, (batch_size, seq_len), dtype=torch.long),
                'attention_mask': torch.ones(batch_size, seq_len, dtype=torch.long),
                'dense_features': torch.randn(batch_size, 1024, dtype=torch.float32),
                'position_ids': torch.arange(seq_len, dtype=torch.long).unsqueeze(0).expand(batch_size, -1),
            }

            # é¢„çƒ­
            logger.info("é¢„çƒ­æ¨ç†...")
            for _ in range(5):
                _ = self.tensorrt_engine.infer(test_inputs)

            # æ€§èƒ½æµ‹è¯•
            logger.info("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
            num_iterations = 50
            start_time = time.time()

            for _ in range(num_iterations):
                outputs = self.tensorrt_engine.infer(test_inputs)

            end_time = time.time()

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_time = end_time - start_time
            avg_latency = (total_time / num_iterations) * 1000  # æ¯«ç§’
            throughput = (batch_size * num_iterations) / total_time  # samples/sec

            logger.info(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ:")
            logger.info(f"  - æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}s")
            logger.info(f"  - å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
            logger.info(f"  - ååé‡: {throughput:.2f} samples/sec")

            # éªŒè¯è¾“å‡º
            if outputs and 'logits' in outputs:
                logits_shape = outputs['logits'].shape
                logger.info(f"  - è¾“å‡ºlogitså½¢çŠ¶: {logits_shape}")

                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åˆç†
                if logits_shape == (batch_size, seq_len, self.config.vocab_size):
                    logger.info("âœ… è¾“å‡ºå½¢çŠ¶éªŒè¯é€šè¿‡")
                    return True
                else:
                    logger.error(f"âŒ è¾“å‡ºå½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸ: {logits_shape}")
                    return False
            else:
                logger.error("âŒ æœªè·å¾—æœ‰æ•ˆè¾“å‡º")
                return False

        except Exception as e:
            logger.error(f"âŒ æ¨ç†æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_benchmark_comparison(self) -> bool:
        """æµ‹è¯•æ€§èƒ½å¯¹æ¯”åŸºå‡†"""
        logger.info("ğŸ§ª æµ‹è¯•æ€§èƒ½å¯¹æ¯”åŸºå‡†...")

        if self.model is None or self.config is None:
            logger.error("æ¨¡å‹æœªåˆ›å»º")
            return False

        try:
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            batch_size = 2
            seq_len = 64

            test_inputs = {
                'input_ids': torch.randint(0, self.config.vocab_size, (batch_size, seq_len), dtype=torch.long),
                'attention_mask': torch.ones(batch_size, seq_len, dtype=torch.long),
                'dense_features': torch.randn(batch_size, 1024, dtype=torch.float32),
                'position_ids': torch.arange(seq_len, dtype=torch.long).unsqueeze(0).expand(batch_size, -1),
            }

            # PyTorchåŸºå‡†æµ‹è¯•
            logger.info("PyTorchåŸºå‡†æµ‹è¯•...")
            self.model.eval()
            with torch.no_grad():
                # é¢„çƒ­
                for _ in range(5):
                    _ = self.model(**test_inputs)

                # æ€§èƒ½æµ‹è¯•
                num_iterations = 20
                start_time = time.time()

                for _ in range(num_iterations):
                    outputs = self.model(**test_inputs)

                pytorch_time = time.time() - start_time
                pytorch_latency = (pytorch_time / num_iterations) * 1000

            logger.info(f"PyTorchå¹³å‡å»¶è¿Ÿ: {pytorch_latency:.2f}ms")

            # TensorRTæ€§èƒ½æµ‹è¯•
            if self.tensorrt_engine and self.tensorrt_engine.tensorrt_available:
                logger.info("TensorRTæ€§èƒ½æµ‹è¯•...")

                # é¢„çƒ­
                for _ in range(5):
                    _ = self.tensorrt_engine.infer(test_inputs)

                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()

                for _ in range(num_iterations):
                    outputs = self.tensorrt_engine.infer(test_inputs)

                tensorrt_time = time.time() - start_time
                tensorrt_latency = (tensorrt_time / num_iterations) * 1000

                logger.info(f"TensorRTå¹³å‡å»¶è¿Ÿ: {tensorrt_latency:.2f}ms")

                # è®¡ç®—åŠ é€Ÿæ¯”
                speedup = pytorch_latency / tensorrt_latency if tensorrt_latency > 0 else 0
                logger.info(f"âœ… TensorRTåŠ é€Ÿæ¯”: {speedup:.2f}x")

                return True
            else:
                logger.warning("âš ï¸ TensorRTå¼•æ“ä¸å¯ç”¨ï¼Œè·³è¿‡å¯¹æ¯”æµ‹è¯•")
                return True

        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
            return False

    def run_all_tests(self) -> Dict[str, bool]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹TensorRTæ’ä»¶å®Œæ•´æµ‹è¯•æµç¨‹...")

        test_results = {}

        # æ£€æŸ¥å¯¼å…¥
        if not IMPORTS_AVAILABLE:
            logger.error("âŒ å¿…è¦æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
            return {"imports": False}

        # 1. æµ‹è¯•æ’ä»¶åŠ è½½
        test_results["plugin_loading"] = self.test_plugin_loading()

        # 2. åˆ›å»ºæµ‹è¯•æ¨¡å‹
        test_results["model_creation"] = self.create_test_model()

        # 3. æµ‹è¯•ONNXå¯¼å‡º
        if test_results["model_creation"]:
            test_results["onnx_export"] = self.test_onnx_export_with_custom_ops()
        else:
            test_results["onnx_export"] = False

        # 4. æµ‹è¯•TensorRTå¼•æ“
        if test_results["onnx_export"]:
            test_results["tensorrt_engine"] = self.test_tensorrt_engine_with_plugins()
        else:
            test_results["tensorrt_engine"] = False

        # 5. æµ‹è¯•æ¨ç†æ€§èƒ½
        if test_results["tensorrt_engine"]:
            test_results["inference_performance"] = self.test_inference_performance()
        else:
            test_results["inference_performance"] = False

        # 6. æµ‹è¯•æ€§èƒ½å¯¹æ¯”
        test_results["benchmark_comparison"] = self.test_benchmark_comparison()

        # æ€»ç»“æµ‹è¯•ç»“æœ
        logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
        passed_tests = 0
        total_tests = len(test_results)

        for test_name, result in test_results.items():
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            logger.info(f"  - {test_name}: {status}")
            if result:
                passed_tests += 1

        success_rate = (passed_tests / total_tests) * 100
        logger.info(f"ğŸ“ˆ æµ‹è¯•é€šè¿‡ç‡: {passed_tests}/{total_tests} ({success_rate:.1f}%)")

        if success_rate >= 80:
            logger.info("ğŸ‰ TensorRTæ’ä»¶æµ‹è¯•æ•´ä½“æˆåŠŸ!")
        else:
            logger.warning("âš ï¸ TensorRTæ’ä»¶æµ‹è¯•å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

        return test_results


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ”§ TensorRTè‡ªå®šä¹‰æ’ä»¶æµ‹è¯•ç¨‹åº")

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = TensorRTPluginTester()

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = tester.run_all_tests()

    # è¿”å›ç»“æœ
    return results


if __name__ == "__main__":
    results = main()

    # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
    if all(results.values()):
        exit(0)  # å…¨éƒ¨é€šè¿‡
    else:
        exit(1)  # å­˜åœ¨å¤±è´¥