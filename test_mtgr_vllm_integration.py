#!/usr/bin/env python3
"""
MTGRæ¨¡å‹å’ŒVLLMæ¨ç†å¼•æ“é›†æˆæµ‹è¯•è„šæœ¬
éªŒè¯ä¸¤ä¸ªç»„ä»¶çš„åŠŸèƒ½å’Œæ€§èƒ½
"""

import asyncio
import time
import json
import logging
from typing import Dict, Any, List
import torch

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_mtgr_model():
    """æµ‹è¯•MTGRæ¨¡å‹åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•MTGRæ¨¡å‹åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from src.mtgr_model import create_mtgr_model
        
        # åˆ›å»ºæ¨¡å‹
        model_config = {
            'vocab_size': 50000,
            'd_model': 1024,
            'nhead': 16,
            'num_layers': 24,
            'd_ff': 4096,
            'max_seq_len': 2048,
            'num_features': 1024,
            'user_profile_dim': 256,
            'item_feature_dim': 512,
            'dropout': 0.1
        }
        
        model = create_mtgr_model(model_config)
        model.eval()
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ… MTGRæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   æ€»å‚æ•°é‡: {total_params:,} (çº¦{total_params/1e9:.1f}B)")
        print(f"   æ¨¡å‹é…ç½®: {model_config}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 2
        seq_len = 100
        num_features = 1024
        
        input_ids = torch.randint(0, 50000, (batch_size, seq_len))
        dense_features = torch.randn(batch_size, num_features)
        user_profile = torch.randn(batch_size, 256)
        item_features = torch.randn(batch_size, 512)
        
        with torch.no_grad():
            # æµ‹è¯•Prefillé˜¶æ®µ
            start_time = time.time()
            outputs = model.forward_prefill(
                input_ids, dense_features, user_profile, item_features
            )
            prefill_time = time.time() - start_time
            
            print(f"âœ… Prefillé˜¶æ®µæµ‹è¯•é€šè¿‡")
            print(f"   è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {[out.shape for out in outputs]}")
            print(f"   æ¨ç†æ—¶é—´: {prefill_time*1000:.2f}ms")
            
            # æµ‹è¯•Decodeé˜¶æ®µ
            last_token = input_ids[:, -1:]
            hidden_states = outputs[-1]
            
            start_time = time.time()
            decode_outputs = model.forward_decode(
                last_token, hidden_states, dense_features, user_profile, item_features
            )
            decode_time = time.time() - start_time
            
            print(f"âœ… Decodeé˜¶æ®µæµ‹è¯•é€šè¿‡")
            print(f"   è§£ç æ—¶é—´: {decode_time*1000:.2f}ms")
            
        return True
        
    except Exception as e:
        print(f"âŒ MTGRæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_vllm_engine():
    """æµ‹è¯•VLLMæ¨ç†å¼•æ“åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•VLLMæ¨ç†å¼•æ“åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from src.vllm_engine import create_vllm_engine
        
        # åˆ›å»ºVLLMå¼•æ“
        engine = create_vllm_engine(
            model_path="mtgr_model",
            tensor_parallel_size=1,
            gpu_memory_utilization=0.9,
            max_model_len=2048
        )
        
        print(f"âœ… VLLMå¼•æ“åˆ›å»ºæˆåŠŸ")
        print(f"   åˆå§‹åŒ–çŠ¶æ€: {engine.is_initialized}")
        print(f"   å¼•æ“é…ç½®: {engine.config}")
        
        # æµ‹è¯•æ¨èç”Ÿæˆ
        test_behaviors = [
            {
                'video_id': 'video_001',
                'watch_duration': 25,
                'watch_percentage': 0.83,
                'is_liked': True,
                'is_favorited': False,
                'is_shared': True,
                'device_type': 'mobile',
                'network_type': 'wifi'
            },
            {
                'video_id': 'video_015',
                'watch_duration': 30,
                'watch_percentage': 1.0,
                'is_liked': True,
                'is_favorited': True,
                'is_shared': False,
                'device_type': 'mobile',
                'network_type': 'wifi'
            }
        ]
        
        # æµ‹è¯•åŒæ­¥æ¨èç”Ÿæˆ
        result = engine._fallback_generation(
            user_id="test_user",
            session_id="test_session",
            user_behaviors=test_behaviors,
            num_recommendations=5
        )
        
        print(f"âœ… æ¨èç”Ÿæˆæµ‹è¯•é€šè¿‡")
        print(f"   æ¨èæ•°é‡: {len(result['recommendations'])}")
        print(f"   å¼•æ“ç±»å‹: {result['engine']}")
        print(f"   å»¶è¿Ÿ: {result['latency_ms']:.2f}ms")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = engine.get_stats()
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ")
        print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"   å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']*1000:.2f}ms")
        print(f"   ååé‡: {stats['throughput']:.2f} req/s")
        
        # å…³é—­å¼•æ“
        engine.shutdown()
        
        return True
        
    except Exception as e:
        print(f"âŒ VLLMå¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_async_inference():
    """æµ‹è¯•å¼‚æ­¥æ¨ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å¼‚æ­¥æ¨ç†åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from src.inference_pipeline import UserBehaviorInferencePipeline
        
        # åˆ›å»ºæ¨ç†æµæ°´çº¿
        pipeline = UserBehaviorInferencePipeline()
        
        print(f"âœ… æ¨ç†æµæ°´çº¿åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹ç±»å‹: MTGR")
        print(f"   VLLMå¼•æ“: {'å·²å¯ç”¨' if hasattr(pipeline, 'vllm_engine') else 'æœªå¯ç”¨'}")
        
        # æµ‹è¯•æ•°æ®
        test_behaviors = [
            {
                'video_id': 'video_001',
                'timestamp': '2024-01-15T14:30:00Z',
                'watch_duration': 25,
                'watch_percentage': 0.83,
                'is_liked': True,
                'is_favorited': False,
                'is_shared': True,
                'device_type': 'mobile',
                'network_type': 'wifi'
            }
        ]
        
        # æµ‹è¯•åŒæ­¥æ¨ç†
        print("\nğŸ” æµ‹è¯•åŒæ­¥æ¨ç†...")
        start_time = time.time()
        result = pipeline.infer_recommendations(
            user_id="test_user",
            session_id="test_session",
            behaviors=test_behaviors,
            num_recommendations=5,
            use_vllm=False  # ä½¿ç”¨MTGRæ¨ç†
        )
        sync_time = time.time() - start_time
        
        print(f"âœ… åŒæ­¥æ¨ç†æµ‹è¯•é€šè¿‡")
        print(f"   æ¨ç†æ—¶é—´: {sync_time*1000:.2f}ms")
        print(f"   æ¨èæ•°é‡: {len(result['recommendations'])}")
        print(f"   å¼•æ“ç±»å‹: {result.get('engine_type', 'unknown')}")
        
        # æµ‹è¯•å¼‚æ­¥æ¨ç†
        print("\nğŸ” æµ‹è¯•å¼‚æ­¥æ¨ç†...")
        start_time = time.time()
        async_result = await pipeline.infer_recommendations_async(
            user_id="test_user",
            session_id="test_session",
            behaviors=test_behaviors,
            num_recommendations=5,
            use_vllm=True  # å°è¯•ä½¿ç”¨VLLM
        )
        async_time = time.time() - start_time
        
        print(f"âœ… å¼‚æ­¥æ¨ç†æµ‹è¯•é€šè¿‡")
        print(f"   æ¨ç†æ—¶é—´: {async_time*1000:.2f}ms")
        print(f"   æ¨èæ•°é‡: {len(async_result['recommendations'])}")
        print(f"   å¼•æ“ç±»å‹: {async_result.get('engine_type', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¼‚æ­¥æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_export():
    """æµ‹è¯•æ¨¡å‹å¯¼å‡ºåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ¨¡å‹å¯¼å‡ºåŠŸèƒ½")
    print("=" * 60)
    
    try:
        from src.export_mtgr_onnx import MTGRONNXExporter
        
        # åˆ›å»ºå¯¼å‡ºå™¨
        model_config = {
            'vocab_size': 50000,
            'd_model': 1024,
            'nhead': 16,
            'num_layers': 24,
            'd_ff': 4096,
            'max_seq_len': 2048,
            'num_features': 1024,
            'user_profile_dim': 256,
            'item_feature_dim': 512,
            'dropout': 0.1
        }
        
        exporter = MTGRONNXExporter(model_config)
        
        print(f"âœ… å¯¼å‡ºå™¨åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹é…ç½®: {model_config}")
        
        # æµ‹è¯•è™šæ‹Ÿæ•°æ®åˆ›å»º
        dummy_data = exporter.create_dummy_data(batch_size=2, seq_len=50)
        print(f"âœ… è™šæ‹Ÿæ•°æ®åˆ›å»ºæˆåŠŸ")
        print(f"   æ•°æ®å½¢çŠ¶: {[data.shape for data in dummy_data]}")
        
        # æµ‹è¯•ONNXå¯¼å‡ºï¼ˆå¯é€‰ï¼‰
        try:
            import onnx
            print(f"âœ… ONNXåº“å¯ç”¨")
        except ImportError:
            print(f"âš ï¸  ONNXåº“æœªå®‰è£…ï¼Œè·³è¿‡å¯¼å‡ºæµ‹è¯•")
            return True
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹å¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    try:
        from src.mtgr_model import create_mtgr_model
        
        # åˆ›å»ºæ¨¡å‹
        model_config = {
            'vocab_size': 50000,
            'd_model': 1024,
            'nhead': 16,
            'num_layers': 24,
            'd_ff': 4096,
            'max_seq_len': 2048,
            'num_features': 1024,
            'user_profile_dim': 256,
            'item_feature_dim': 512,
            'dropout': 0.1
        }
        
        model = create_mtgr_model(model_config)
        model.eval()
        
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦çš„æ€§èƒ½
        test_configs = [
            (1, 100),   # å°æ‰¹æ¬¡ï¼ŒçŸ­åºåˆ—
            (4, 200),   # ä¸­ç­‰æ‰¹æ¬¡ï¼Œä¸­ç­‰åºåˆ—
            (8, 500),   # å¤§æ‰¹æ¬¡ï¼Œé•¿åºåˆ—
        ]
        
        print("æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"{'æ‰¹æ¬¡å¤§å°':<8} {'åºåˆ—é•¿åº¦':<8} {'æ¨ç†æ—¶é—´(ms)':<12} {'å†…å­˜ä½¿ç”¨(MB)':<12}")
        print("-" * 50)
        
        for batch_size, seq_len in test_configs:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            input_ids = torch.randint(0, 50000, (batch_size, seq_len))
            dense_features = torch.randn(batch_size, 1024)
            user_profile = torch.randn(batch_size, 256)
            item_features = torch.randn(batch_size, 512)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(3):
                    _ = model.forward_prefill(
                        input_ids, dense_features, user_profile, item_features
                    )
            
            # æ€§èƒ½æµ‹è¯•
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(10):
                    _ = model.forward_prefill(
                        input_ids, dense_features, user_profile, item_features
                    )
            
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            
            # è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´
            avg_time = (end_time - start_time) / 10 * 1000
            
            # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            estimated_memory = (batch_size * seq_len * 1024 * 4) / (1024 * 1024)  # MB
            
            print(f"{batch_size:<8} {seq_len:<8} {avg_time:<12.2f} {estimated_memory:<12.1f}")
        
        print("\nâœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹MTGRæ¨¡å‹å’ŒVLLMæ¨ç†å¼•æ“é›†æˆæµ‹è¯•")
    print("=" * 80)
    
    test_results = []
    
    # 1. æµ‹è¯•MTGRæ¨¡å‹
    test_results.append(("MTGRæ¨¡å‹åŠŸèƒ½", test_mtgr_model()))
    
    # 2. æµ‹è¯•VLLMå¼•æ“
    test_results.append(("VLLMæ¨ç†å¼•æ“", test_vllm_engine()))
    
    # 3. æµ‹è¯•å¼‚æ­¥æ¨ç†
    test_results.append(("å¼‚æ­¥æ¨ç†åŠŸèƒ½", await test_async_inference()))
    
    # 4. æµ‹è¯•æ¨¡å‹å¯¼å‡º
    test_results.append(("æ¨¡å‹å¯¼å‡ºåŠŸèƒ½", test_model_export()))
    
    # 5. è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    test_results.append(("æ€§èƒ½åŸºå‡†æµ‹è¯•", run_performance_benchmark()))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print("\n" + "=" * 80)
    print("æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 80)
    
    passed_tests = 0
    total_tests = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:<20} {status}")
        if result:
            passed_tests += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MTGRæ¨¡å‹å’ŒVLLMæ¨ç†å¼•æ“é›†æˆæˆåŠŸï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "failed_tests": total_tests - passed_tests,
        "test_results": [
            {"name": name, "status": "PASS" if result else "FAIL"}
            for name, result in test_results
        ]
    }
    
    with open("test_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: test_report.json")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())
